{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About me I am a computer science undergraduate at IIITH , currently (2024, fifth-year of my dual degree program) working on adiabatic quantum computing advised by Prof Shantanav Chakraborty and Prof Indranil Chakrabarty . Also, I often go by my nickname Arjo and can be found playing valorant when bored. Main quests I am a pretty random person and have tried my hands at a lot of things including football and japanese calligraphy (I suck at it). Some things you are obliged to do to survive. Some things are fun but do not stick. Some things are panache. And some provide a sense of being alive, awareness beyond routine life. They are the main quests. Research and Engineering : To me an endeavour to understand and create is just as much bound in romance and passion as in reason and rationale. Regarding fields I am interested in, I tend to sway between physics ( now: quantum computing, post-quantum gravity ), computation ( now: cryptography, learning ) and political economy. Writing : If there's anything the deserves to be called divine, then it's for sure stories which have shaped, steered and instilled meaning in us. Side quests Professional NLH Poker : I play mostly cash games since good tournament games are hard to come by. Wrestling : Preparing to start the belt route in jiu-jutsu. Competitive Programming : 2020 - 2022, restarted 2024 - now. Links Email address: alapan.chaudhuri@research.iiit.ac.in My Github: banrovegrie Google Scholar: arjo Zeeshan's blog: Devil in Details","title":"Home"},{"location":"#about-me","text":"I am a computer science undergraduate at IIITH , currently (2024, fifth-year of my dual degree program) working on adiabatic quantum computing advised by Prof Shantanav Chakraborty and Prof Indranil Chakrabarty . Also, I often go by my nickname Arjo and can be found playing valorant when bored.","title":"About me"},{"location":"#main-quests","text":"I am a pretty random person and have tried my hands at a lot of things including football and japanese calligraphy (I suck at it). Some things you are obliged to do to survive. Some things are fun but do not stick. Some things are panache. And some provide a sense of being alive, awareness beyond routine life. They are the main quests. Research and Engineering : To me an endeavour to understand and create is just as much bound in romance and passion as in reason and rationale. Regarding fields I am interested in, I tend to sway between physics ( now: quantum computing, post-quantum gravity ), computation ( now: cryptography, learning ) and political economy. Writing : If there's anything the deserves to be called divine, then it's for sure stories which have shaped, steered and instilled meaning in us.","title":"Main quests"},{"location":"#side-quests","text":"Professional NLH Poker : I play mostly cash games since good tournament games are hard to come by. Wrestling : Preparing to start the belt route in jiu-jutsu. Competitive Programming : 2020 - 2022, restarted 2024 - now.","title":"Side quests"},{"location":"#links","text":"Email address: alapan.chaudhuri@research.iiit.ac.in My Github: banrovegrie Google Scholar: arjo Zeeshan's blog: Devil in Details","title":"Links"},{"location":"reading-list/","text":"I thought that I would maintain a list of books and movies or shows I have meant to watch or read in 2024. Will be rating them, once completed. Reading List [x] Vagabond by Inoue: Greatest manga ever. I wish he completes it. 5/5. [x] The Three Body Problem by Cixin Liu [ ] The Dark Forest by Cixin Liu [ ] Death's End by Cixin Liu [ ] Berserk by Miura [x] The Remains of the Day by Ishiguro: Read and gifted it. This is the first book I read of Ishiguro. [ ] The Buried Giant by Ishiguro [ ] Capital by Thomas Picketty [ ] Capital and Ideology by Thomas Picketty [x] The Gospel according to Jesus Christ by Jose Saramago: I re-read it. 5/5. The book to me reads like a reminiscence of a deep dream. Watching List I tend to watch a lot of random shows and movies. So best only if I add the biggest and/or the best. This is the \"watching\" list and not \"watching because I am bored\" list. [x] Shogun: Best show I have watched. 5/5. [x] 3 Body Problem (Season 1): Some of the dialogues were badly written especially in the beginning. Wish it had more science, as in the book(s). But the show is good overall. Wish it had more nuanced character developments given the added character-focus in the show. 4/5. [x] Spirited Away: I love Hayao Miyazaki. [x] The Boy and the Heron: 5/5. [x] Dune 2: Absolutely amazing. This is going to be bigger than Star Wars. 5/5.","title":"Reading List"},{"location":"reading-list/#reading-list","text":"[x] Vagabond by Inoue: Greatest manga ever. I wish he completes it. 5/5. [x] The Three Body Problem by Cixin Liu [ ] The Dark Forest by Cixin Liu [ ] Death's End by Cixin Liu [ ] Berserk by Miura [x] The Remains of the Day by Ishiguro: Read and gifted it. This is the first book I read of Ishiguro. [ ] The Buried Giant by Ishiguro [ ] Capital by Thomas Picketty [ ] Capital and Ideology by Thomas Picketty [x] The Gospel according to Jesus Christ by Jose Saramago: I re-read it. 5/5. The book to me reads like a reminiscence of a deep dream.","title":"Reading List"},{"location":"reading-list/#watching-list","text":"I tend to watch a lot of random shows and movies. So best only if I add the biggest and/or the best. This is the \"watching\" list and not \"watching because I am bored\" list. [x] Shogun: Best show I have watched. 5/5. [x] 3 Body Problem (Season 1): Some of the dialogues were badly written especially in the beginning. Wish it had more science, as in the book(s). But the show is good overall. Wish it had more nuanced character developments given the added character-focus in the show. 4/5. [x] Spirited Away: I love Hayao Miyazaki. [x] The Boy and the Heron: 5/5. [x] Dune 2: Absolutely amazing. This is going to be bigger than Star Wars. 5/5.","title":"Watching List"},{"location":"weblog/","text":"This is my weblog . Yes, it means logs meant to be on the web. Yes, the word blog comes from weblog . Yes, I had no freaking clue that this was the case. And yes, I am writing all these just to fill up the space below which the logs can begin. Don't criticize what you can't understand Don't write what you do not \u2014 to at least a certain admissible level \u2014 understand. This reminds me of the line in Bob Dylan's song \"don't criticize what you can't understand\". Ab initio cum recta abstractio It is important to think from the ground up but you must be at the right ground. Hence, ab initio cum recta abstractio . From the beginning with the correct abstraction. Cryptography and Physics The philosophical exploration of cryptography from a physical perspective delves into fundamental questions about the nature of information, its robustness under physical laws, and the principles governing its secure communication and control (black hole information paradox, non-locality and nature of quantum information). Need for? Almost always we find works of science established on grounds of mathematical understanding (language formalisation?) and physical importance (pragmatic realism?). However, philosophy (understanding perspective?) is often left out. This often comes around to bite back. The main reason I believe this happens is because of lack of perspective leading to further lack of ideological basis or motivational push. \u2018If\u2019 from Rewards and Fairies If you can keep your head when all about you Are losing theirs and blaming it on you, If you can trust yourself when all men doubt you, But make allowance for their doubting too; If you can wait and not be tired by waiting, Or being lied about, don\u2019t deal in lies, Or being hated, don\u2019t give way to hating, And yet don\u2019t look too good, nor talk too wise: If you can dream\u2014and not make dreams your master; If you can think\u2014and not make thoughts your aim; If you can meet with Triumph and Disaster And treat those two impostors just the same; If you can bear to hear the truth you\u2019ve spoken Twisted by knaves to make a trap for fools, Or watch the things you gave your life to, broken, And stoop and build \u2019em up with worn-out tools: If you can make one heap of all your winnings And risk it on one turn of pitch-and-toss, And lose, and start again at your beginnings And never breathe a word about your loss; If you can force your heart and nerve and sinew To serve your turn long after they are gone, And so hold on when there is nothing in you Except the Will which says to them: \u2018Hold on!\u2019 If you can talk with crowds and keep your virtue, Or walk with Kings\u2014nor lose the common touch, If neither foes nor loving friends can hurt you, If all men count with you, but none too much; If you can fill the unforgiving minute With sixty seconds\u2019 worth of distance run, Yours is the Earth and everything that\u2019s in it, And\u2014which is more\u2014you\u2019ll be a Man, my son! The poem nicely encapsulates my notion of virtue ethics. To be stable when none are, to be trustworthy when none are, to be hated but not giveaway to hating, to not be too proud or be grandoise, to be stoic about results yet be passionate about actions, to build things back up no matter how many times they are broken, to wager away everything on that what you feel righteous and to hold on to all these when despair glooms all. The only line which feels ill to me is \u201cIf neither foes nor loving friends can hurt you, If all men count with you, but none too much\u201c. Might it rather be: If neither foes nor loving friends can make you, Feel that life\u2019s worth is futile much; Yours is the Earth and everything that\u2019s in it, And\u2014which is more\u2014you\u2019ll be a great, my son! \u2018Gettysberg Address\u2019 by Abraham Lincoln \" Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this. But, in a larger sense, we can not dedicate \u2014 we can not consecrate \u2014 we can not hallow \u2014 this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before us\u2014that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion \u2014 that we here highly resolve that these dead shall not have died in vain \u2014 that this nation, under God, shall have a new birth of freedom \u2014 and that government of the people, by the people, for the people, shall not perish from the earth. (Abraham Lincoln)\" My favourite speech given of all times. Found randomly \" The insects have chosen a different line: they have sought first the material welfare and security of the hive, and presumably they have their reward. Men are different. They propound mathematical theorems in beleaguered cities, conduct metaphysical arguments in condemned cells, make jokes on scaffolds, discuss the last new poem while advancing to the walls of Quebec, and comb their hair at Thermopylae. This is not panache; it is our nature. (C.S. Lewis)\" Diplomacy and Perception Identities matter to us more than actions. By turning actions into identities we can make people much more likely to engage in those desired actions. Speak with a great deal of confidence. Remove fillers. Be prophetic. Don\u2019t ask me wtf that means. Code Capture, Organize, Distill, and Express. A nice mantra to remember when you think of approaching any system design job be it in the manufacturing industry or be it in good old software engineering. Now, on the subject of actual \u201ccode\u201d or \u201cprograms or software\u201d, code is used and read more than it is written. Zeeshan sent me a blog post regarding this. biz > user > ops > maintainer > author Whims and Selling Always selling and marketing based on the whims of the public is a fast track to disaster for the public and often for the institution doing it too. However, certain individuals can get wealth and influence, a lot of it, by doing this. Trust and Values Without trust society cannot stand. It is neither just truth nor lies that a civilization is built upon. A civilization stands on trust be it fictional, fake or emperical truth. To imagine an intermingling of trust and fiction is strange. Maybe fiction is essential for our minds to comprehend reality and work with it. As to values and personal values, I trust in being virtuous. And my virtuosity assumes openness and honesty in all things important. Rosch and Whorf's Hypothesis: an excerpt Rosch's early studies were on color. She learned of the Berlin-Kay color research midway through her own research and found that their results meshed with her own work on Dani, a New Guinea language that has only two basic color categories: mili (dark-cool, including black, green, and blue) and mola (light-warm, including white, red, yellow). Berlin and Kay had shown that focal colors had a special status within color categoraies-that of the best example of the category. Rosch found that Dani speakers, when asked for the best exampies of their two color categories, chose focal colors, for example, white, red, or yellow for mola with different speakers making different choices. In a remarkable set of experiments, Rosch set out to show that primary color categories were psychologically real for speakers of Dani, even though they were not named. She set out to challenge one of Whorf's hypotheses, namely, that language determines one's conceptual system. If Whorf were right on this matter, the Dani's two words for colors would determine two and only two conceptual categories of colors. Rosch reasoned that if it was language alone that determined color categorization, then the Dani should have equal difficulty learning new words for colors, no matter whether the color ranges had a primary color at the center or a nonprimary color. She then went about studying how Dani speakers would learn new, made-up color terms. One group was taught arbitrary names for eight focal colors, and another group, arbitrary names for eight nonfocal colors (Rosch 1973). The names for focal colors were learned more easily. Dani speakers were also found (like English speakers) to be able to remember focal colors better than nonfocal colors (Heider 1972). In an experiment in which speakers judged color similarity, the Dani were shown to represent colors in memory the same way English speakers do (Heider and Olivier 1972). Story and its elements A story is built with the following: Narration \u2192 moves the story \u2192 active voice, fuck adverbs and adjectives Description \u2192 show the perceptions \u2192 don\u2019t tell Dialogue \u2192 brings characters to life \u2192 don\u2019t overstate Where I differ from Nietzsche? On Vagabond The Three-body Problem \"Should philosophy guide experiments or expeiment guide philosophy?\" (Cixin Liu, \u4e09\u4f53 or Three-body)","title":"Weblog"},{"location":"weblog/#dont-criticize-what-you-cant-understand","text":"Don't write what you do not \u2014 to at least a certain admissible level \u2014 understand. This reminds me of the line in Bob Dylan's song \"don't criticize what you can't understand\".","title":"Don't criticize what you can't understand"},{"location":"weblog/#ab-initio-cum-recta-abstractio","text":"It is important to think from the ground up but you must be at the right ground. Hence, ab initio cum recta abstractio . From the beginning with the correct abstraction.","title":"Ab initio cum recta abstractio"},{"location":"weblog/#cryptography-and-physics","text":"The philosophical exploration of cryptography from a physical perspective delves into fundamental questions about the nature of information, its robustness under physical laws, and the principles governing its secure communication and control (black hole information paradox, non-locality and nature of quantum information).","title":"Cryptography and Physics"},{"location":"weblog/#need-for","text":"Almost always we find works of science established on grounds of mathematical understanding (language formalisation?) and physical importance (pragmatic realism?). However, philosophy (understanding perspective?) is often left out. This often comes around to bite back. The main reason I believe this happens is because of lack of perspective leading to further lack of ideological basis or motivational push.","title":"Need for?"},{"location":"weblog/#if-from-rewards-and-fairies","text":"If you can keep your head when all about you Are losing theirs and blaming it on you, If you can trust yourself when all men doubt you, But make allowance for their doubting too; If you can wait and not be tired by waiting, Or being lied about, don\u2019t deal in lies, Or being hated, don\u2019t give way to hating, And yet don\u2019t look too good, nor talk too wise: If you can dream\u2014and not make dreams your master; If you can think\u2014and not make thoughts your aim; If you can meet with Triumph and Disaster And treat those two impostors just the same; If you can bear to hear the truth you\u2019ve spoken Twisted by knaves to make a trap for fools, Or watch the things you gave your life to, broken, And stoop and build \u2019em up with worn-out tools: If you can make one heap of all your winnings And risk it on one turn of pitch-and-toss, And lose, and start again at your beginnings And never breathe a word about your loss; If you can force your heart and nerve and sinew To serve your turn long after they are gone, And so hold on when there is nothing in you Except the Will which says to them: \u2018Hold on!\u2019 If you can talk with crowds and keep your virtue, Or walk with Kings\u2014nor lose the common touch, If neither foes nor loving friends can hurt you, If all men count with you, but none too much; If you can fill the unforgiving minute With sixty seconds\u2019 worth of distance run, Yours is the Earth and everything that\u2019s in it, And\u2014which is more\u2014you\u2019ll be a Man, my son! The poem nicely encapsulates my notion of virtue ethics. To be stable when none are, to be trustworthy when none are, to be hated but not giveaway to hating, to not be too proud or be grandoise, to be stoic about results yet be passionate about actions, to build things back up no matter how many times they are broken, to wager away everything on that what you feel righteous and to hold on to all these when despair glooms all. The only line which feels ill to me is \u201cIf neither foes nor loving friends can hurt you, If all men count with you, but none too much\u201c. Might it rather be: If neither foes nor loving friends can make you, Feel that life\u2019s worth is futile much; Yours is the Earth and everything that\u2019s in it, And\u2014which is more\u2014you\u2019ll be a great, my son!","title":"\u2018If\u2019 from Rewards and Fairies"},{"location":"weblog/#gettysberg-address-by-abraham-lincoln","text":"\" Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this. But, in a larger sense, we can not dedicate \u2014 we can not consecrate \u2014 we can not hallow \u2014 this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before us\u2014that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion \u2014 that we here highly resolve that these dead shall not have died in vain \u2014 that this nation, under God, shall have a new birth of freedom \u2014 and that government of the people, by the people, for the people, shall not perish from the earth. (Abraham Lincoln)\" My favourite speech given of all times.","title":"\u2018Gettysberg Address\u2019 by Abraham Lincoln"},{"location":"weblog/#found-randomly","text":"\" The insects have chosen a different line: they have sought first the material welfare and security of the hive, and presumably they have their reward. Men are different. They propound mathematical theorems in beleaguered cities, conduct metaphysical arguments in condemned cells, make jokes on scaffolds, discuss the last new poem while advancing to the walls of Quebec, and comb their hair at Thermopylae. This is not panache; it is our nature. (C.S. Lewis)\"","title":"Found randomly"},{"location":"weblog/#diplomacy-and-perception","text":"Identities matter to us more than actions. By turning actions into identities we can make people much more likely to engage in those desired actions. Speak with a great deal of confidence. Remove fillers. Be prophetic. Don\u2019t ask me wtf that means.","title":"Diplomacy and Perception"},{"location":"weblog/#code","text":"Capture, Organize, Distill, and Express. A nice mantra to remember when you think of approaching any system design job be it in the manufacturing industry or be it in good old software engineering. Now, on the subject of actual \u201ccode\u201d or \u201cprograms or software\u201d, code is used and read more than it is written. Zeeshan sent me a blog post regarding this. biz > user > ops > maintainer > author","title":"Code"},{"location":"weblog/#whims-and-selling","text":"Always selling and marketing based on the whims of the public is a fast track to disaster for the public and often for the institution doing it too. However, certain individuals can get wealth and influence, a lot of it, by doing this.","title":"Whims and Selling"},{"location":"weblog/#trust-and-values","text":"Without trust society cannot stand. It is neither just truth nor lies that a civilization is built upon. A civilization stands on trust be it fictional, fake or emperical truth. To imagine an intermingling of trust and fiction is strange. Maybe fiction is essential for our minds to comprehend reality and work with it. As to values and personal values, I trust in being virtuous. And my virtuosity assumes openness and honesty in all things important.","title":"Trust and Values"},{"location":"weblog/#rosch-and-whorfs-hypothesis-an-excerpt","text":"Rosch's early studies were on color. She learned of the Berlin-Kay color research midway through her own research and found that their results meshed with her own work on Dani, a New Guinea language that has only two basic color categories: mili (dark-cool, including black, green, and blue) and mola (light-warm, including white, red, yellow). Berlin and Kay had shown that focal colors had a special status within color categoraies-that of the best example of the category. Rosch found that Dani speakers, when asked for the best exampies of their two color categories, chose focal colors, for example, white, red, or yellow for mola with different speakers making different choices. In a remarkable set of experiments, Rosch set out to show that primary color categories were psychologically real for speakers of Dani, even though they were not named. She set out to challenge one of Whorf's hypotheses, namely, that language determines one's conceptual system. If Whorf were right on this matter, the Dani's two words for colors would determine two and only two conceptual categories of colors. Rosch reasoned that if it was language alone that determined color categorization, then the Dani should have equal difficulty learning new words for colors, no matter whether the color ranges had a primary color at the center or a nonprimary color. She then went about studying how Dani speakers would learn new, made-up color terms. One group was taught arbitrary names for eight focal colors, and another group, arbitrary names for eight nonfocal colors (Rosch 1973). The names for focal colors were learned more easily. Dani speakers were also found (like English speakers) to be able to remember focal colors better than nonfocal colors (Heider 1972). In an experiment in which speakers judged color similarity, the Dani were shown to represent colors in memory the same way English speakers do (Heider and Olivier 1972).","title":"Rosch and Whorf's Hypothesis: an  excerpt"},{"location":"weblog/#story-and-its-elements","text":"A story is built with the following: Narration \u2192 moves the story \u2192 active voice, fuck adverbs and adjectives Description \u2192 show the perceptions \u2192 don\u2019t tell Dialogue \u2192 brings characters to life \u2192 don\u2019t overstate","title":"Story and its elements"},{"location":"weblog/#where-i-differ-from-nietzsche","text":"","title":"Where I differ from Nietzsche?"},{"location":"weblog/#on-vagabond","text":"","title":"On Vagabond"},{"location":"weblog/#the-three-body-problem","text":"\"Should philosophy guide experiments or expeiment guide philosophy?\" (Cixin Liu, \u4e09\u4f53 or Three-body)","title":"The Three-body Problem"},{"location":"research/ads-cft/","text":"https://arxiv.org/pdf/1802.01040.pdf (TASI Lectures, involved) https://www.henryyuen.net/spring2022/projects/adscft.pdf (Basic story) Lectures by David Tong: http://www.damtp.cam.ac.uk/user/tong/gr/gr.pdf (GR) https://www.damtp.cam.ac.uk/user/tong/qft/qft.pdf (QFT) http://www.damtp.cam.ac.uk/user/tong/string/string.pdf (String Theory)","title":"Ads-CFT Correspondance"},{"location":"research/data-analysis/","text":"Data Analysis Learning = Representation + Evaluation + Optimization. Generalisation is necessary. Data alone ain\u2019t enough. Overfitting will attack you from unpredictable places. Intuition fails in high dimensions. Theoretical guarantees are wobbly. Feature engineering for the win. More data beats better algos. Learn many models. Simplicity doesn\u2019t imply accuracy (always). Use Occam\u2019s razor with care. Representable doesn\u2019t mean learnable (always). Correlation doesn\u2019t imply causation. Universal Approximation Perceptrons are the simplest form of neural networks. MLPs or multi-layer perceptrons are called universal approximators. The Curse of Dimensionality The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience. In case of machine learning, the number of learning samples increases exponentially with increasing dimensions. Graphs Graph functions and node functions are permutation independent. Graph Theorists argue that GNNs are just special cases of graph isomorphism test(s) like WL test.","title":"Data Analysis"},{"location":"research/data-analysis/#data-analysis","text":"Learning = Representation + Evaluation + Optimization. Generalisation is necessary. Data alone ain\u2019t enough. Overfitting will attack you from unpredictable places. Intuition fails in high dimensions. Theoretical guarantees are wobbly. Feature engineering for the win. More data beats better algos. Learn many models. Simplicity doesn\u2019t imply accuracy (always). Use Occam\u2019s razor with care. Representable doesn\u2019t mean learnable (always). Correlation doesn\u2019t imply causation.","title":"Data Analysis"},{"location":"research/data-analysis/#universal-approximation","text":"Perceptrons are the simplest form of neural networks. MLPs or multi-layer perceptrons are called universal approximators.","title":"Universal Approximation"},{"location":"research/data-analysis/#the-curse-of-dimensionality","text":"The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience. In case of machine learning, the number of learning samples increases exponentially with increasing dimensions.","title":"The Curse of Dimensionality"},{"location":"research/data-analysis/#graphs","text":"Graph functions and node functions are permutation independent. Graph Theorists argue that GNNs are just special cases of graph isomorphism test(s) like WL test.","title":"Graphs"},{"location":"research/haskell/","text":"The Fuck is Haskell? A pure , lazy , functional programming language. Basically, it is something that happens when like-minded cool people come together. Meaning of the above bullshit words? Well here we go. Functional All hail functions. Children of the idea behind lambda calculus. Use functions just like any other sort of values. Evaluate not execute. Pure Immutability is the key. Fuck all side-effects. Deterministic as fuck. Benefits: equational reasoning, parallelism, happiness Lazy Infinity? ez. Compositional programming \u2014 you feel like Mozart. Disadvantage: Wot is time? Wot is space? Haskell, the new cool guy in campus Well here we go. Types Statically typed: run time errors \\(\\rightarrow\\) compile-time errors Expressive: the code is the documentation Expressive: brings clarity into coding Abstraction Eat, Code, Sleep, ~~Repeat~~: Haskell has polymorphism, higher-order functions and type classes \u2014 so fuck repetition. Think about the big picture and don't cry about some stupid exception. What can I do with it? Program Correctness (QuickCheck) Fail safe programming (Cardano) High-load concurrent programming (web back-end) Haskell: A Functional Programming Langauge What does it mean for Haskell to be a functional programming language? Well, it means that Haskell follows the principle of functional programming \u2014 a programming paradigm where functions are the basic building blocks of computation. Def: A function is a mapping that takes one or more arguments and produces a single result. Properties of Haskell Consice programs Powerful type system List comprehension Recursive functions Higher-order functions Effectful functions Generic functions Lazy evaluation Equational reasoning First Steps Haskell comes with a large number of built-in functions, which are defined in a library file called the standard prelude. head <list> tail <list> take <num> <list> drop <num> <list> sum <list> reverse <list> product <list> Function Format Just as in lambda calculus haskell follows a fixed pattern for functions. Here are some examples to elaborate: \\(f(x)\\) as f x \\(f(x, y)\\) as f x y \\(f(g(x))\\) as f (g x) \\(f(x, g(y))\\) as f x (g y) \\(f(x)g(y)\\) as f x * g y Function application has the highest priority than all other operators. x f y is a syntactic sugar for f x y Examples -- Program 1 main = print (a ++ b ++ c) a = [((2**3) * 4)] b = [(2*3) + (4*5)] c = [2 + (3 * (4**5))] -- Program 2 main = print (n) n = (a `div` (length xs)) where a = 10 xs = [1 .. 5] -- Program 3 main = print (a [1 .. 5]) a xs = sum (drop (length xs - 1) (take (length xs) xs)) main = putStrLn \"hello, world\" Types and class Types They are a collection of related values. f :: A -> B and e :: A then, f e :: B Bool \u2013 logical values Char \u2013 single characters String \u2013 strings of characters Int \u2013 fixed-precision integers Integer \u2013 arbitrary-precision integers Float \u2013 single-precision floating-point numbers Double \u2013 double-precision floating-point numbers [[\u2019a\u2019,\u2019b\u2019],[\u2019c\u2019,\u2019d\u2019,\u2019e\u2019]] :: [[Char]] List types \u2014 [\"One\",\"Two\",\"Three\"] :: [String] Tuple types \u2014 (\"Yes\",True,\u2019a\u2019) :: (String,Bool,Char) Function types \u2014 not :: Bool -> Bool and add :: (Int,Int) -> Int Polymorphic types \u2014 length :: [a] -> Int and zip :: [a] -> [b] -> [(a, b)] Classes They are collections of types that support certain overloaded operations called methods. Eq \u2014 (==) :: Eq a => a -> a -> Bool Ord \u2014 <, >, min, max with (<) :: Ord a => a -> a -> Bool Show \u2014 show :: a -> String Read \u2014 read :: String -> a Num \u2014 e.g., (+), (-), negate, abs, signum with (+) :: Num a => a -> a -> a Integral \u2014 div :: Int a => a -> a -> a and mod :: Int a => a -> a -> a , also Int and Integer types are instances of this class. Fractional \u2014 Float and Double are instances of this class. We also have methods such as / and recip . Functions Let us now introduce some really cool implementation techniques in haskell with respect to defining functions. Conditionals -- Let's see an example even :: a -> Bool even n = (n `mod` 2 == 0) -- Conditional using \"if else\" signum n = if n > 0 then 1 else if n < 0 then -1 else 0 -- Conditional using \"such that\" signum n | n > 0 = 1 | n < 0 = -1 | otherwise = 0 Pattern Matching -- Define functions using '_' len [] = 0 len (_:xs) = 1 + len xs initials :: String -> String -> String initials firstname lastname = [f] ++ \". \" ++ [l] ++ \".\" where (f:_) = firstname (l:_) = lastname -- Defining using '_' fundamentally test :: Int -> Int test 0 = 1 test 1 = 2 test _ = 0 Lambda Functions \\x -> x + 1 -- For example, add :: Int -> Int -> Int add x y = x + y -- and odds n = map f [0 .. n - 1] where f x = x * 2 + 1 -- can be written as add :: Int -> (Int -> Int) add = \\x -> (\\y -> x + y) -- and odds n = map (\\x -> x * 2 + 1) [0 .. n - 1] Currying Functions Let \\(x = f(a, b, c)\\) then, we shall have the following. \\[ f :: (\\text{Type}(a),\\ \\text{Type}(b),\\ \\text{Type}(c)) \\to \\text{Type}(x) \\] Upon currying, this gets translated to the below expression. \\[ f :: \\text{Type}(a) \\to (\\text{Type}(b) \\to (\\text{Type}(c) \\to (\\text{Type}(x)))) \\] \\[ \\text{or, } f :: \\text{Type}(a) \\to \\text{Type}(b) \\to \\text{Type}(c) \\to \\text{Type}(x) \\] This is because, \\(x = f(a, b, c)\\) becomes \\(h = g(a), i = h(b), x = i(c)\\) or if called in sequence \\(x = g(a)(b)(c)\\) . fst :: (a, b) -> a zip :: [a] -> [b] -> [(a, b)] id :: a -> a take :: Int -> [a] -> [a] head :: [a] -> a Operator Sections -- Operator Sections w = 1 + 2 -- is same as x = (+) 1 2 -- is same as y = (1+) 2 -- or z = (+2) 1 This is very useful to construct certain functions. List Comprehension The idea is based on set construction from other sets. xy = [(x, y) | x <- [1..3], y <- [x..3]] -- The above is an example of dependent generators. -- Definitions of x and y serve as generators. -- Check out the usability of .. operator. concat :: [[a]] -> [a] concat xss = [x | xs <- xss, x <- xs] -- Guards are possible for example as bellow. evens = [x | x <- [1..10], even x] Recursion Here we go. How to think recursively? Name the function Define its type Enumerate the cases Define the base cases List the \"ingredients\" Reason about the parameters Define the transition (non-trivial cases) Think about the result Recursion -- Example of a recursion zip :: [a] -> [b] -> [(a, b)] zip [] _ = [] zip _ [] = [] zip (x:xs) (y:ys) = (x, y) : zip xs ys -- Append Definition (++) :: [a] -> [a] -> [a] [] ++ ys = ys (x:xs) ++ ys = x : (xs ++ ys) Memoization The following memoize function takes a function of type Int -> a and returns a memoized version of the same function. The trick is to turn a function into a value because, in Haskell, functions are not memoized but values are. import Data.Function (fix) memoize :: (Int -> a) -> (Int -> a) memoize f = (map f [0 ..] !!) fib :: (Int -> Integer) -> Int -> Integer fib f 0 = 0 fib f 1 = 1 fib f n = f (n - 1) + f (n - 2) fibMemo :: Int -> Integer fibMemo = fix (memoize . fib) Higher Order Function A function is called a higher order if it takes a function as an argument or returns a function as a result. twice :: (a -> a) -> a -> a twice f x = f (f x) Common programming idioms can be encoded as functions within the language itself. Domain specific languages can be defined as collections of higher order functions. Algebraic properties of higher-order functions can be used to reason about numbers. -- Example map :: (a -> b) -> [a] -> [b] map f xs = [f x | x <- xs] -- Alternate map :: (a -> b) -> [a] -> [b] map f [] = [] map f (x:xs) = f x : map f xs -- Example filter :: (a -> Bool) -> [a] -> [a] filter f xs = [x | x <- xs, f x] -- Alternatively filter f [] = [] filter f (x:xs) | f x = x : filter f xs | otherwise = filter f xs Foldr A number of functions on lists can be defined using the following simple pattern of recursion. f [] = v f (x:xs) = x \u2295 f xs Here \\(f\\) maps the empty list to some value \\(v\\) and any non-empty list to some function \\(\u2295\\) applied to its head and \\(f\\) of its tail. The higher-order library function foldr (fold-right) encapsulates this simple pattern of recursion with the function \\(\u2295\\) and the value \\(v\\) as arguments. foldr :: (a -> b -> b) -> b -> [a] -> b foldr f v [] = v foldr f v (x:xs) = f x (foldr f v xs) -- Generic Examples sum = foldr (+) 0 product = foldr (*) 1 or = foldr (||) False and = foldr (&&) True -- Other examples length = foldr (\\ _ n -> 1 + n) 0 reverse = foldr (\\ x xs -> xs ++ [x]) [] (++ ys) = foldr (:) ys Composition (.) :: (b -> c) -> (a -> b) -> (a -> c) f . g = \\x -> f (g x) odd :: Int -> Bool odd = not . even Other nice library functions all :: (a -> Bool) -> [a] -> Bool all f xs = and [f x | x <- xs] any :: (a -> Bool) -> [a] -> Bool any f xs = or [f x | x <- xs] takeWhile :: (a -> Bool) -> [a] -> [a] takeWhile f [] = [] takeWhile f (x:xs) | f x = x : takeWhile f xs | otherwise = [] dropWhile :: (a -> Bool) -> [a] -> [a] dropWhile f [] = [] dropWhile f (x:xs) | f x = dropWhile f xs | otherwise = x:xs Moreover, curried functions are higher-order functions that returns a function. Where and Let Clauses -- Let f = let x = 1; y = 2 in (x + y) -- Where f = x + y where x = 1; y = 1 Type and Class Declaration type Pos = (Int, Int) type Trans = Pos -> Pos type Tree = (Int, [Tree]) A completely new type can be defined by specifying its values in context-free formulation. data Bool = False | True data Answer = Yes | No | Unknown answers :: [Answer] answers = [Yes, No, Unknown] flip :: Answer -> Answer flip Yes = No flip No = Yes flip Unknown = Unknown Circle :: Float -> Shape Rect :: Float -> Float -> Shape data Shape = Circle Float | Rect Float Float square :: Float -> Shape square n = Rect n n area :: Shape -> Float area (Circle r) = pi * r^2 area (Rect x y) = x * y Maybe, Nothing, Just data Maybe a = Nothing | Just a safediv :: Int -> Int -> Maybe Int safediv _ 0 = Nothing safediv m n = Just (m `div` n) safehead :: [a] -> Maybe a safehead [] = Nothing safehead xs = Just (head xs) This is basically a cool syntactic encapsulation to prevent crashing. This is used to often deal with exceptions and create failsafe programs. Recursive Types data Nat = Zero | Succ Nat -- we have Zero :: Nat -- and Succ :: Nat -> Nat data Expr = Val Int | Add Expr Expr | Mul Expr Expr eval :: Expr -> Int eval (Val n) = n eval (Add x y) = eval x + eval y eval (Mul x y) = eval x * eval y Interactive Programming This is a problem because Haskell is designed to have no side effects and thus only to create batch programs. Interactive programs, on the other hand, necessarily require side effects. Solution We will use types to describe impure actions involving side effects ( IO a ). IO Char is the type of actions that return a character. IO () is the type of purely side effecting actions that return no result value. The standard library provides a number of actions including the following three primitives. -- reads character from the keyboard -- and echoes it o the screen -- returning the character as the result value getChar :: IO Char -- takes a character as a input and writes it to the screen -- and returns no result value putChar :: Char -> IO () -- simply return value without performing any interaction -- this is basically a pure to impure conversion return :: a -> IO a Sequencing act :: IO (Char, Char) act = do x <- getChar getChar y <- getChar return (x, y) Derived Primitives getLine :: IO String getLine = do x <- getChar if x == '\\n' then return [] else do xs <- getLine return (x:xs) putStr :: String -> IO () putStr [] = return () putStr (x:xs) = do putChar x putStr xs -- Just mentioned here so that -- one can try out problems with input import Control.Arrow ((>>>)) main :: IO () main = interact $ lines >>> head >>> read >>> solve >>> (++ \"\\n\") solve :: Int -> String -- Type 2 import Control.Arrow ((>>>)) main :: IO () main = interact $ words >>> map read >>> solve >>> show >>> (++ \"\\n\") solve :: [Integer] -> Integer Simple I/O Operations These are in-built. putChar :: Char -> IO() putStr :: String -> IO () putStrLn :: String -> IO () print :: Show a => a -> IO () getChar :: IO Char getLine :: IO String getContents :: IO String interact :: (String -> String) -> IO () show :: Show a => a -> String read :: Read a => String -> a main = do putStrLn \"enter value for x: \" input1 <- getLine putStrLn \"enter value for y: \" input2 <- getLine let x = (read input1 :: Int) let y = (read input2 :: Int) print (x + y) Lazy Evaluation \\[ \\text{lazy evaluation} = \\text{outermost evaluation}\\ +\\ \\text{shared arguments} \\] Why is it important? No unnecessary evaluation Ensures termination when possible Supports programming with infinite structures Allows programs to be modular (separates control from data) Notes on Functional Programming Haskell is very expressive language. It makes you think abstractly and forces to model and define the problem mathematically. Focus on what to compute (definitions) rather than how Power of abstraction and modularity Equational reasoning How powerful types are The main drawback is that it is hard to reason about efficiency.","title":"Haskell in Harsh Words"},{"location":"research/haskell/#the-fuck-is-haskell","text":"A pure , lazy , functional programming language. Basically, it is something that happens when like-minded cool people come together.","title":"The Fuck is Haskell?"},{"location":"research/haskell/#meaning-of-the-above-bullshit-words","text":"Well here we go.","title":"Meaning of the above bullshit words?"},{"location":"research/haskell/#functional","text":"All hail functions. Children of the idea behind lambda calculus. Use functions just like any other sort of values. Evaluate not execute.","title":"Functional"},{"location":"research/haskell/#pure","text":"Immutability is the key. Fuck all side-effects. Deterministic as fuck. Benefits: equational reasoning, parallelism, happiness","title":"Pure"},{"location":"research/haskell/#lazy","text":"Infinity? ez. Compositional programming \u2014 you feel like Mozart. Disadvantage: Wot is time? Wot is space?","title":"Lazy"},{"location":"research/haskell/#haskell-the-new-cool-guy-in-campus","text":"Well here we go.","title":"Haskell, the new cool guy in campus"},{"location":"research/haskell/#types","text":"Statically typed: run time errors \\(\\rightarrow\\) compile-time errors Expressive: the code is the documentation Expressive: brings clarity into coding","title":"Types"},{"location":"research/haskell/#abstraction","text":"Eat, Code, Sleep, ~~Repeat~~: Haskell has polymorphism, higher-order functions and type classes \u2014 so fuck repetition. Think about the big picture and don't cry about some stupid exception.","title":"Abstraction"},{"location":"research/haskell/#what-can-i-do-with-it","text":"Program Correctness (QuickCheck) Fail safe programming (Cardano) High-load concurrent programming (web back-end)","title":"What can I do with it?"},{"location":"research/haskell/#haskell-a-functional-programming-langauge","text":"What does it mean for Haskell to be a functional programming language? Well, it means that Haskell follows the principle of functional programming \u2014 a programming paradigm where functions are the basic building blocks of computation. Def: A function is a mapping that takes one or more arguments and produces a single result.","title":"Haskell: A Functional Programming Langauge"},{"location":"research/haskell/#properties-of-haskell","text":"Consice programs Powerful type system List comprehension Recursive functions Higher-order functions Effectful functions Generic functions Lazy evaluation Equational reasoning","title":"Properties of Haskell"},{"location":"research/haskell/#first-steps","text":"Haskell comes with a large number of built-in functions, which are defined in a library file called the standard prelude. head <list> tail <list> take <num> <list> drop <num> <list> sum <list> reverse <list> product <list>","title":"First Steps"},{"location":"research/haskell/#function-format","text":"Just as in lambda calculus haskell follows a fixed pattern for functions. Here are some examples to elaborate: \\(f(x)\\) as f x \\(f(x, y)\\) as f x y \\(f(g(x))\\) as f (g x) \\(f(x, g(y))\\) as f x (g y) \\(f(x)g(y)\\) as f x * g y Function application has the highest priority than all other operators. x f y is a syntactic sugar for f x y","title":"Function Format"},{"location":"research/haskell/#examples","text":"-- Program 1 main = print (a ++ b ++ c) a = [((2**3) * 4)] b = [(2*3) + (4*5)] c = [2 + (3 * (4**5))] -- Program 2 main = print (n) n = (a `div` (length xs)) where a = 10 xs = [1 .. 5] -- Program 3 main = print (a [1 .. 5]) a xs = sum (drop (length xs - 1) (take (length xs) xs)) main = putStrLn \"hello, world\"","title":"Examples"},{"location":"research/haskell/#types-and-class","text":"","title":"Types and class"},{"location":"research/haskell/#types_1","text":"They are a collection of related values. f :: A -> B and e :: A then, f e :: B Bool \u2013 logical values Char \u2013 single characters String \u2013 strings of characters Int \u2013 fixed-precision integers Integer \u2013 arbitrary-precision integers Float \u2013 single-precision floating-point numbers Double \u2013 double-precision floating-point numbers [[\u2019a\u2019,\u2019b\u2019],[\u2019c\u2019,\u2019d\u2019,\u2019e\u2019]] :: [[Char]] List types \u2014 [\"One\",\"Two\",\"Three\"] :: [String] Tuple types \u2014 (\"Yes\",True,\u2019a\u2019) :: (String,Bool,Char) Function types \u2014 not :: Bool -> Bool and add :: (Int,Int) -> Int Polymorphic types \u2014 length :: [a] -> Int and zip :: [a] -> [b] -> [(a, b)]","title":"Types"},{"location":"research/haskell/#classes","text":"They are collections of types that support certain overloaded operations called methods. Eq \u2014 (==) :: Eq a => a -> a -> Bool Ord \u2014 <, >, min, max with (<) :: Ord a => a -> a -> Bool Show \u2014 show :: a -> String Read \u2014 read :: String -> a Num \u2014 e.g., (+), (-), negate, abs, signum with (+) :: Num a => a -> a -> a Integral \u2014 div :: Int a => a -> a -> a and mod :: Int a => a -> a -> a , also Int and Integer types are instances of this class. Fractional \u2014 Float and Double are instances of this class. We also have methods such as / and recip .","title":"Classes"},{"location":"research/haskell/#functions","text":"Let us now introduce some really cool implementation techniques in haskell with respect to defining functions. Conditionals -- Let's see an example even :: a -> Bool even n = (n `mod` 2 == 0) -- Conditional using \"if else\" signum n = if n > 0 then 1 else if n < 0 then -1 else 0 -- Conditional using \"such that\" signum n | n > 0 = 1 | n < 0 = -1 | otherwise = 0 Pattern Matching -- Define functions using '_' len [] = 0 len (_:xs) = 1 + len xs initials :: String -> String -> String initials firstname lastname = [f] ++ \". \" ++ [l] ++ \".\" where (f:_) = firstname (l:_) = lastname -- Defining using '_' fundamentally test :: Int -> Int test 0 = 1 test 1 = 2 test _ = 0 Lambda Functions \\x -> x + 1 -- For example, add :: Int -> Int -> Int add x y = x + y -- and odds n = map f [0 .. n - 1] where f x = x * 2 + 1 -- can be written as add :: Int -> (Int -> Int) add = \\x -> (\\y -> x + y) -- and odds n = map (\\x -> x * 2 + 1) [0 .. n - 1]","title":"Functions"},{"location":"research/haskell/#currying-functions","text":"Let \\(x = f(a, b, c)\\) then, we shall have the following. \\[ f :: (\\text{Type}(a),\\ \\text{Type}(b),\\ \\text{Type}(c)) \\to \\text{Type}(x) \\] Upon currying, this gets translated to the below expression. \\[ f :: \\text{Type}(a) \\to (\\text{Type}(b) \\to (\\text{Type}(c) \\to (\\text{Type}(x)))) \\] \\[ \\text{or, } f :: \\text{Type}(a) \\to \\text{Type}(b) \\to \\text{Type}(c) \\to \\text{Type}(x) \\] This is because, \\(x = f(a, b, c)\\) becomes \\(h = g(a), i = h(b), x = i(c)\\) or if called in sequence \\(x = g(a)(b)(c)\\) . fst :: (a, b) -> a zip :: [a] -> [b] -> [(a, b)] id :: a -> a take :: Int -> [a] -> [a] head :: [a] -> a","title":"Currying Functions"},{"location":"research/haskell/#operator-sections","text":"-- Operator Sections w = 1 + 2 -- is same as x = (+) 1 2 -- is same as y = (1+) 2 -- or z = (+2) 1 This is very useful to construct certain functions.","title":"Operator Sections"},{"location":"research/haskell/#list-comprehension","text":"The idea is based on set construction from other sets. xy = [(x, y) | x <- [1..3], y <- [x..3]] -- The above is an example of dependent generators. -- Definitions of x and y serve as generators. -- Check out the usability of .. operator. concat :: [[a]] -> [a] concat xss = [x | xs <- xss, x <- xs] -- Guards are possible for example as bellow. evens = [x | x <- [1..10], even x]","title":"List Comprehension"},{"location":"research/haskell/#recursion","text":"Here we go.","title":"Recursion"},{"location":"research/haskell/#how-to-think-recursively","text":"Name the function Define its type Enumerate the cases Define the base cases List the \"ingredients\" Reason about the parameters Define the transition (non-trivial cases) Think about the result","title":"How to think recursively?"},{"location":"research/haskell/#recursion_1","text":"-- Example of a recursion zip :: [a] -> [b] -> [(a, b)] zip [] _ = [] zip _ [] = [] zip (x:xs) (y:ys) = (x, y) : zip xs ys -- Append Definition (++) :: [a] -> [a] -> [a] [] ++ ys = ys (x:xs) ++ ys = x : (xs ++ ys)","title":"Recursion"},{"location":"research/haskell/#memoization","text":"The following memoize function takes a function of type Int -> a and returns a memoized version of the same function. The trick is to turn a function into a value because, in Haskell, functions are not memoized but values are. import Data.Function (fix) memoize :: (Int -> a) -> (Int -> a) memoize f = (map f [0 ..] !!) fib :: (Int -> Integer) -> Int -> Integer fib f 0 = 0 fib f 1 = 1 fib f n = f (n - 1) + f (n - 2) fibMemo :: Int -> Integer fibMemo = fix (memoize . fib)","title":"Memoization"},{"location":"research/haskell/#higher-order-function","text":"A function is called a higher order if it takes a function as an argument or returns a function as a result. twice :: (a -> a) -> a -> a twice f x = f (f x) Common programming idioms can be encoded as functions within the language itself. Domain specific languages can be defined as collections of higher order functions. Algebraic properties of higher-order functions can be used to reason about numbers. -- Example map :: (a -> b) -> [a] -> [b] map f xs = [f x | x <- xs] -- Alternate map :: (a -> b) -> [a] -> [b] map f [] = [] map f (x:xs) = f x : map f xs -- Example filter :: (a -> Bool) -> [a] -> [a] filter f xs = [x | x <- xs, f x] -- Alternatively filter f [] = [] filter f (x:xs) | f x = x : filter f xs | otherwise = filter f xs","title":"Higher Order Function"},{"location":"research/haskell/#foldr","text":"A number of functions on lists can be defined using the following simple pattern of recursion. f [] = v f (x:xs) = x \u2295 f xs Here \\(f\\) maps the empty list to some value \\(v\\) and any non-empty list to some function \\(\u2295\\) applied to its head and \\(f\\) of its tail. The higher-order library function foldr (fold-right) encapsulates this simple pattern of recursion with the function \\(\u2295\\) and the value \\(v\\) as arguments. foldr :: (a -> b -> b) -> b -> [a] -> b foldr f v [] = v foldr f v (x:xs) = f x (foldr f v xs) -- Generic Examples sum = foldr (+) 0 product = foldr (*) 1 or = foldr (||) False and = foldr (&&) True -- Other examples length = foldr (\\ _ n -> 1 + n) 0 reverse = foldr (\\ x xs -> xs ++ [x]) [] (++ ys) = foldr (:) ys","title":"Foldr"},{"location":"research/haskell/#composition","text":"(.) :: (b -> c) -> (a -> b) -> (a -> c) f . g = \\x -> f (g x) odd :: Int -> Bool odd = not . even","title":"Composition"},{"location":"research/haskell/#other-nice-library-functions","text":"all :: (a -> Bool) -> [a] -> Bool all f xs = and [f x | x <- xs] any :: (a -> Bool) -> [a] -> Bool any f xs = or [f x | x <- xs] takeWhile :: (a -> Bool) -> [a] -> [a] takeWhile f [] = [] takeWhile f (x:xs) | f x = x : takeWhile f xs | otherwise = [] dropWhile :: (a -> Bool) -> [a] -> [a] dropWhile f [] = [] dropWhile f (x:xs) | f x = dropWhile f xs | otherwise = x:xs Moreover, curried functions are higher-order functions that returns a function.","title":"Other nice library functions"},{"location":"research/haskell/#where-and-let-clauses","text":"-- Let f = let x = 1; y = 2 in (x + y) -- Where f = x + y where x = 1; y = 1","title":"Where and Let Clauses"},{"location":"research/haskell/#type-and-class-declaration","text":"type Pos = (Int, Int) type Trans = Pos -> Pos type Tree = (Int, [Tree]) A completely new type can be defined by specifying its values in context-free formulation. data Bool = False | True data Answer = Yes | No | Unknown answers :: [Answer] answers = [Yes, No, Unknown] flip :: Answer -> Answer flip Yes = No flip No = Yes flip Unknown = Unknown Circle :: Float -> Shape Rect :: Float -> Float -> Shape data Shape = Circle Float | Rect Float Float square :: Float -> Shape square n = Rect n n area :: Shape -> Float area (Circle r) = pi * r^2 area (Rect x y) = x * y","title":"Type and Class Declaration"},{"location":"research/haskell/#maybe-nothing-just","text":"data Maybe a = Nothing | Just a safediv :: Int -> Int -> Maybe Int safediv _ 0 = Nothing safediv m n = Just (m `div` n) safehead :: [a] -> Maybe a safehead [] = Nothing safehead xs = Just (head xs) This is basically a cool syntactic encapsulation to prevent crashing. This is used to often deal with exceptions and create failsafe programs.","title":"Maybe, Nothing, Just"},{"location":"research/haskell/#recursive-types","text":"data Nat = Zero | Succ Nat -- we have Zero :: Nat -- and Succ :: Nat -> Nat data Expr = Val Int | Add Expr Expr | Mul Expr Expr eval :: Expr -> Int eval (Val n) = n eval (Add x y) = eval x + eval y eval (Mul x y) = eval x * eval y","title":"Recursive Types"},{"location":"research/haskell/#interactive-programming","text":"This is a problem because Haskell is designed to have no side effects and thus only to create batch programs. Interactive programs, on the other hand, necessarily require side effects.","title":"Interactive Programming"},{"location":"research/haskell/#solution","text":"We will use types to describe impure actions involving side effects ( IO a ). IO Char is the type of actions that return a character. IO () is the type of purely side effecting actions that return no result value. The standard library provides a number of actions including the following three primitives. -- reads character from the keyboard -- and echoes it o the screen -- returning the character as the result value getChar :: IO Char -- takes a character as a input and writes it to the screen -- and returns no result value putChar :: Char -> IO () -- simply return value without performing any interaction -- this is basically a pure to impure conversion return :: a -> IO a","title":"Solution"},{"location":"research/haskell/#sequencing","text":"act :: IO (Char, Char) act = do x <- getChar getChar y <- getChar return (x, y)","title":"Sequencing"},{"location":"research/haskell/#derived-primitives","text":"getLine :: IO String getLine = do x <- getChar if x == '\\n' then return [] else do xs <- getLine return (x:xs) putStr :: String -> IO () putStr [] = return () putStr (x:xs) = do putChar x putStr xs -- Just mentioned here so that -- one can try out problems with input import Control.Arrow ((>>>)) main :: IO () main = interact $ lines >>> head >>> read >>> solve >>> (++ \"\\n\") solve :: Int -> String -- Type 2 import Control.Arrow ((>>>)) main :: IO () main = interact $ words >>> map read >>> solve >>> show >>> (++ \"\\n\") solve :: [Integer] -> Integer","title":"Derived Primitives"},{"location":"research/haskell/#simple-io-operations","text":"These are in-built. putChar :: Char -> IO() putStr :: String -> IO () putStrLn :: String -> IO () print :: Show a => a -> IO () getChar :: IO Char getLine :: IO String getContents :: IO String interact :: (String -> String) -> IO () show :: Show a => a -> String read :: Read a => String -> a main = do putStrLn \"enter value for x: \" input1 <- getLine putStrLn \"enter value for y: \" input2 <- getLine let x = (read input1 :: Int) let y = (read input2 :: Int) print (x + y)","title":"Simple I/O Operations"},{"location":"research/haskell/#lazy-evaluation","text":"\\[ \\text{lazy evaluation} = \\text{outermost evaluation}\\ +\\ \\text{shared arguments} \\] Why is it important? No unnecessary evaluation Ensures termination when possible Supports programming with infinite structures Allows programs to be modular (separates control from data)","title":"Lazy Evaluation"},{"location":"research/haskell/#notes-on-functional-programming","text":"Haskell is very expressive language. It makes you think abstractly and forces to model and define the problem mathematically. Focus on what to compute (definitions) rather than how Power of abstraction and modularity Equational reasoning How powerful types are The main drawback is that it is hard to reason about efficiency.","title":"Notes on Functional Programming"},{"location":"research/linear-algebra/","text":"Linear Operators and Matrices \\[ A (\\Sigma a_i |v_i\\rangle) = \\Sigma a_iA|v_i\\rangle \\] Now, see a linear operator is just a matrix. Suppose \\(A: V \\to W\\) and \\(|v_1\\rangle, |v_2\\rangle, ..., |v_m\\rangle\\) are basis of \\(V\\) and \\(|w_1\\rangle, |w_2\\rangle, ..., |w_n\\rangle\\) is a basis of \\(W\\) then, \\[ A|v_j\\rangle = \\Sigma_i A_{ij}|w_i\\rangle \\] Inner Products Ok so imagine an operation \\((\\_ ,\\_): V\\times V \\to \\mathbb{C}\\) such that the following shit holds ok? \\((|v\\rangle, \\Sigma_i \\lambda_i|w_i\\rangle) = \\Sigma_i\\lambda_i(|v\\rangle, |w_i\\rangle)\\) \\((|v\\rangle, |w\\rangle) = (|w\\rangle, |v\\rangle)^*\\) \\((|v\\rangle, |v\\rangle) \\geq 0\\) and \\(= 0\\) iff \\(|v\\rangle\\) In finite dimensions, inner product space i.e., vector spaces equipped with inner prducts for all \\(|v\\rangle \\in\\) vector space \\(=\\) Hilbert Space Consider \\(|i\\rangle\\ \\&\\ |j\\rangle\\) to be orthonormal basis, we have \u2014 \\[ \\langle v|w \\rangle = (\\Sigma_i v_i|i\\rangle, \\Sigma_jw_j|j\\rangle) = \\Sigma_i \\Sigma_j v_i^*w_j\\delta_{ij} = \\Sigma_iv_i^*w_i = |v\\rangle^\\dagger |w\\rangle \\] Norm of a vector \\[ ||v|| = \\sqrt{\\langle v|v \\rangle} \\] We can say that \\(|v\\rangle\\) is normalized iff \\(||v|| = 1\\) . A set of \\(|a_i\\rangle\\) vectors is orthonormal if \\(\\langle a_i|a_j \\rangle = \\delta_{ij}\\) i.e., \\(\\forall\\ i \\neq j\\ \\langle a_i|a_j \\rangle = 0\\) and \\(\\langle a_i|a_j \\rangle = 1\\ \\forall\\ i=j\\) . Gram Schmidt: for orthonormal basis \\[ |v_{k+1}\\rangle = \\frac{|w_{k+1}\\rangle - \\Sigma_{i=1}^k \\langle v_i | w_{k+1}\\rangle |v_i\\rangle}{|||w_{k+1}\\rangle - \\Sigma_{i=1}^k \\langle v_i | w_{k+1}\\rangle |v_i\\rangle||},\\ |v_1\\rangle = |w_1\\rangle/|||w_1 \\rangle|| \\] Outer Product \\[ |w\\rangle \\langle v|(|v'\\rangle) = |w\\rangle |v\\rangle^\\dagger |v'\\rangle = |w\\rangle \\langle v|v' \\rangle = \\langle v|v' \\rangle |w\\rangle \\] From this notion we obtain the completeness relation, \\(\\Sigma_i |i\\rangle \\langle i| = I\\) . \\(A = I_wAI_v = \\Sigma_{ij} |w_j\\rangle\\langle w_j|A|v_i\\rangle\\langle v_i| = \\Sigma_{ij} \\langle w_j|A|v_i\\rangle|w_j\\rangle\\langle v_i|\\) Cauchy Schwarz: \\(\\langle v|v \\rangle \\langle w|w \\rangle \\geq \\langle v|w \\rangle \\langle w|v \\rangle = |\\langle v|w \\rangle|^2\\) Hilbert Space A Hilbert Space \\(\\mathcal{H}\\) is complete which means that every Cauchy sequence of vectors admits in the space itself. Under this hypothesis there exist Hilbert bases also known as complete orthonormal systems of vectors in \\(\\mathcal{H}\\) . For any orthonormal basis of \\(\\mathcal{H}\\) , we have the following. \\[ \\text{Orthonormality} \\equiv\\langle \\psi_i|\\psi_j\\rangle = \\delta_{ij}\\\\ \\text{Completeness} \\equiv\\sum_{i} |\\psi_i\\rangle\\langle\\psi_i| = I \\] Eigenvectors and Eigenvalues Under a given linear transformation \\(A\\) , \\(A|v\\rangle = \\lambda|v\\rangle\\) where \\(\\exists\\ |v\\rangle\\) s.t. they do not get shifted off their span. All such vectors are referred as eigenvectors and \\((A - \\lambda I)|v\\rangle = 0 \\implies det|A-\\lambda I| = 0\\) gives all possible eigenvalue. If all \\(\u03bb_i \u2265 0\\) , it is positive semidefinite and if they are \\(> 0\\) , it is positive definite. Eigenspace It is the space of all vectors with a given eigenvalue \\(\\lambda\\) . When an eigenspace is more than one dimensional, we call it degenerate. Adjoints and Hermitian Suppose \\(A: V \\to V\\) then \\(\\exists\\ A^\\dagger: V \\to V\\) such that \\(\\forall\\ \\vert v\\rangle,\\ \\vert w\\rangle \\in V\\) we have, \\[ (\\vert v\\rangle, A\\vert w\\rangle) = (A^\\dagger \\vert v\\rangle, \\vert w\\rangle) \\] This operator is called as the adjoint or Hermitian conjugate of the operator \\(A\\) . \\[ (\\vert v\\rangle, A\\vert w\\rangle) = \\langle v\\vert A\\vert w\\rangle = \\vec{v}^\\dagger A\\vec{w} = (A^\\dagger\\vec{v})^\\dagger\\vec{w} = (A^\\dagger\\vert v\\rangle, \\vert w\\rangle) \\] We have, \\((AB)^\\dagger = B^\\dagger A^\\dagger\\) \\(\\vert v\\rangle^\\dagger = \\langle v\\vert\\) Some defintions Normal matrices: \\(AA^\\dagger = A^\\dagger A\\) Hermitian matrices: \\(A^\\dagger = A\\) Unitary matrices: \\(AA^\\dagger = I\\) A normal matrix is Hermitian if and only if it has real eigenvalues. If \\(\\langle x| A|x\\rangle \\geq 0, \\forall\\ |x\\rangle\\) then \\(A\\) is positive semi-definite and has positive eigenvalues. Some properties If a Hermitian matrix has positive eigenvalues then it is positive semi-definite. If \\(M = AA^\\dagger\\) then it is both Hermitian and positive semi-definite. All positive semi-definite operators are Hermitian, by definition. Spectral Decomposition Definition: A linear operator is diagonalizable if and only if it is normal . Some notes and derivation regarding the above: \\(A\\vec{v} = \\lambda\\vec{v} = \\Sigma_i \\lambda_{ij}\\vec{q_i}\\) where \\(q_i\\) 's are linearly independent eigenvalues of \\(A\\) . \\(AQ = Q\\Lambda\\) where \\(Q = \\begin{bmatrix} q_1&q_2 &\\ldots&q_n \\end{bmatrix} \\implies A = Q{\\Lambda}Q^{-1}\\) \\[A = IAI = (P+Q)A(P+Q) = PAP + QAP + PAQ + QAQ\\] \\[\\implies A = \\lambda{P^2} + 0 + 0 + QAQ\\] \\[\\implies A = \\lambda{P^2} + QAQ\\] Matrices and Vectors In the following statements we are dealing with \\(\\{\\vert i\\rangle\\}\\) as a orthonormal basis set. \\[I = \\Sigma \\vert i\\rangle\\langle i\\vert\\] \\[\\vert \\psi \\rangle = \\Sigma \\sigma_i\\vert i\\rangle, \\text{ where } \\sigma_i = \\langle i\\vert \\psi\\rangle\\] Now, to represent a operator or linear transformation as matrix in orthonormal basis. \\[ A_{ij} = \\langle i\\vert A\\vert j\\rangle\\\\ A = \\Sigma_{i, j} \\langle i\\vert A\\vert j\\rangle \\vert i\\rangle \\langle j\\vert\\\\ \\text{tr}(A) = \\Sigma_i \\langle i\\vert A\\vert i\\rangle \\] Now diagonalization for any normal matrix. \\[ M = \\Sigma_i \\lambda_i \\vert i\\rangle \\langle i\\vert = \\Sigma_i \\lambda_i P_i,\\ \\text{where}\\ P_i^\\dagger = P_i\\\\ f(M) = \\Sigma_i f(\\lambda_i) \\vert i\\rangle \\langle i\\vert \\] where \\(\\lambda_i\\) are eigenvalues of \\(M\\) under a given orthonormal basis set \\(\\{\\vert i\\rangle\\}\\) for vector space \\(V\\) , each \\(\\vert i \\rangle\\) is an eigenvector of \\(M\\) with eigenvalue \\(\\lambda_i\\) . If \\(M\\) is Hermitian , all eigenvalues \\((\\lambda_i\\text{ s})\\) are real. Tensor Products \\(z\\vert{vw}\\rangle = (z\\vert{v}\\rangle) \\otimes (\\vert{w}\\rangle) =(\\vert{v}\\rangle) \\otimes (z\\vert{w}\\rangle)\\) \\((\\vert v_1 \\rangle + \\vert v_2 \\rangle) \\otimes \\vert w \\rangle = \\vert{v_1w}\\rangle + \\vert{v_2w}\\rangle\\) \\(\\vert v \\rangle \\otimes (\\vert w_1 \\rangle + \\vert w_2 \\rangle) = \\vert{vw_1}\\rangle + \\vert{vw_2}\\rangle\\) \\(\\vert \\psi \\rangle^{\\otimes^k} = \\vert \\psi \\rangle \\otimes \\ldots \\otimes \\vert \\psi \\rangle \\text{ k times}\\) \\((A \\otimes B)^\\dagger = A^\\dagger \\otimes B^\\dagger\\) Linear Product \\(A\\otimes{B}\\) forms the linear operator that acts on \\(V\\otimes W\\) vector space givern that \\(A\\) acts on \\(V\\) and \\(B\\) acts on \\(W\\) . \\[ (A\\otimes B)(\\Sigma_{i} a_i\\vert v_i\\rangle \\otimes \\vert w_i \\rangle) = \\Sigma_{i} a_iA\\vert v_i\\rangle \\otimes B\\vert w_i \\rangle \\] Inner Product \\[ (\\Sigma_{i} a_i\\vert v_i\\rangle \\otimes \\vert w_i \\rangle, \\Sigma_{i} a_j\\vert v'_j\\rangle \\otimes \\vert w'_j \\rangle) = \\Sigma_{ij} a_i^*b_j \\langle v_i\\vert v'_j\\rangle\\langle w_i\\vert w'_j\\rangle \\] Trace Properties of trace are given below as follows. \\(\\text{tr}(A) = \\Sigma_i A_{ii}\\) \\(\\text{tr}(A) = \\Sigma_i \\langle i\\vert A\\vert i\\rangle\\) for orthonormal basis \\(\\text{tr}(AB) = \\text{tr}(BA)\\) \\(\\text{tr}(zA+B) = z\\cdot \\text{tr}(A) + \\text{tr}(B)\\) The above properties yield certain implications as follows. \\(\\text{tr}(UAU^\\dagger) = tr(A)\\) \\(\\text{tr}(A\\vert \\psi\\rangle\\langle\\psi\\vert) = \\Sigma_i \\langle i\\vert A\\vert \\psi\\rangle\\langle\\psi\\vert i \\rangle\\) \\(\\text{tr}(A) = \\sum_i \\lambda_i\\) , \\(\\text{det}(A) = \\prod_i \\lambda_i\\) with algebraic multiplicities \\[ ||A|| = \\sqrt{tr (A^\\dagger A)} \\] Partial Trace Entanglement excludes the possibility of associating state vectors with individual subsystems. Therefore, we introduce density matrices and the corresponding idea of reduction preformed with partial trace. \\[ \\text{tr}(A \\otimes B) = \\text{ tr}(A) \\cdot \\text{tr}(B) \\] \\[ \\rho_{AB} : \\mathcal{H_A}\\otimes\\mathcal{H_B} \\xrightarrow{\\text{ tr}_B} \\rho_A : \\mathcal{H_A} \\] \\[ \\text{ tr}_B(AB) = A \\text{ tr}(B) \\] Hilbert-Schimdt Inner Product \\(L_v\\) forms the vector space of operators over the Hilbert space \\(V\\) . Then, we can show that \\(L_v\\) is also a Hilbert space with \\(\\text{tr}(A^\\dagger B)\\) as the inner product operator on \\(L_v \\times L_v\\) . Also, we have \\(div(L_v) = dim(V)^2\\) . Commutator and Anti-commutator \\[ [A, B] = AB - BA\\\\ \\{A, B\\} = AB + BA \\] Theorem of Simultaneous Diagonalization Suppose \\(A\\) and \\(B\\) are both Hermitian matrices, then \\([A, B] = 0\\) iff \\(\\exists\\) orthonormal basis such that both \\(A\\) and \\(B\\) are diagonal with respect to that basis. Polar Value Decomposition If \\(A\\) is any linear operator and \\(U\\) is a unitary then \\(J, K\\) are positive operators, such that \\[ A = UJ = KU, \\text{ where } J = \\sqrt{A^\\dagger A} \\text{ and } K = \\sqrt{AA^\\dagger} \\] Moreover, if \\(A^{-1}\\) exists, then \\(U\\) is unique. Singular Value Decomposition SVD in general is given as follows. \\[ {U\\Sigma V^{T}} \\] It generalizes the eigen decomposition of a square normal matrix with an orthonormal eigen basis to any \\(m\\times n\\) matrix. \\({\\Sigma}\\) is an \\({m\\times n}\\) rectangular diagonal matrix with non-negative real numbers on the diagonal (called singular values). Corollary: If \\(A\\) is a square matrix and \\(\\exists\\ U, V\\) unitaries then \\(D\\) is a diagonal matrix, such that \\[ A = UDV \\] where \\(D\\) has non-negative values. Corollary: If \\(A\\) has non-negative eigenvalues then, \\(A = U^\\dagger DU\\) is possible where \\(D\\) has non-negative values. If \\(A\\) is square both SVD and EVD exist but might not be same. If \\(A\\) is a square symmetric matrix both SVD and EVD exist and are equivalent. If \\(A\\) is non-square only SVD is possible. Rank of a matrix Rank \\(=\\) number of dimensions in column space. The row rank is the largest number of rows of \\(A\\) that constitute a linearly independent set. The column rank is the largest number of columns of \\(A\\) that constitute a linearly independent set. Moreover, column-rank \\(=\\) row-rank for \\(A \\in \\mathbb{R}^{m \\times n}\\) . \\[ rank(A \\in \\mathbb{R}^{m \\times n}) \\leq min(m, n) \\] Matrix is called full rank if equality holds. \\(rank(A^T) = rank(A)\\) \\(rank(AB) \u2264 min(rank(A), rank(B))\\) \\(rank(A + B) \u2264 rank(A) + rank(B)\\) Projection and Spaces \\[ Proj(y ; A) = argmin_{v \u2208R(A)} ||v \u2212 y||_2 = A(A^T A)^{\u22121} A^T y \\] \\(\\mathcal{N} (A) = \\{x \u2208 \\mathbb R^n : Ax = 0\\}\\) denotes all vectors in \\(\\mathbb R^n\\) that land at the origin after transformation. It is also called kernel. \\(\\mathcal R(A) = \\{v \u2208 \\mathbb R^m : v = Ax,~x \u2208 \\mathbb R^n \\}\\) denotes the space spanned by the transformed basis vectors in \\(\\mathbb R^n\\) . \\(\\mathcal R(A^T)\\) and \\(\\mathcal N (A)\\) are orthogonal spaces which together span \\(\\mathbb R^n\\) . Determinant \\(\\neq 0\\) implies that the matrix has an inverse. Quadratic Forms Reminder: we are in real \\(\\mathbb R\\) space. \\[ x^TAx = (x^TAx)^T = x^T(\\frac 1 2 A + \\frac 1 2 A^T)x \\] Moore-Penrose Pseudoinverse \\[ A^{\\text{left inv}} = (A^\\dagger A)^{-1}A^\\dagger \\] \\[ A^{\\text{right inv}} = A^\\dagger(AA^\\dagger)^{-1} \\] This is a pseudo inverse formalism with left and right inverses. \\[ A^{\\text{left inv}}A = I\\\\ AA^{\\text{right inv}} = I \\] Row Echelon Forms A matrix is in row echelon form if: All rows consisting of only zeroes are at the bottom. The leading coefficient (also called the pivot) of a nonzero row is always strictly to the right of the leading coefficient of the row above it. A matrix is in reduced row echelon form if: It is in row echelon form. The leading entry in each nonzero row is a 1 (called a leading 1). Each column containing a leading 1 has zeros in all its other entries. Spectral Decomposition Any normal operator \\(M\\) on a vector space \\(V\\) is diagonal with respect to some orthonormal basis for \\(V\\) . Conversely, any diagonalizable operator is normal. Proof \\[ M = (P+Q)M(P+Q) = PMP+QMP+PMQ+QMQ = PMP + QMQ \\] Now we have \\[ QM = QM(P+Q) = QMQ \\text{ and } QM^\\dagger = QM^\\dagger Q \\] \\[ QMQQM^\\dagger Q = QMM^\\dagger Q = QM^\\dagger MQ = QM^\\dagger QQMQ \\] Thus, if \\(M\\) is normal then \\(M = PMP + QMQ\\) where \\(PMP = \\lambda P^2 = \\lambda P\\) and thus is diagonalizable wrt orthonormal basis for \\(P\\) . Similarly, \\(QMQ\\) is also diagonalizable wrt some orthonormal basis for \\(Q\\) . Thus, \\(M\\) is diagonalizable for orthonormal basis of the entire vector space. \\[ M = \\Sigma \\lambda_i \\vert i\\rangle \\langle i \\vert = \\Sigma \\lambda_i P_i \\] Polar Value Decomposition Let \\(A\\) be a matrix on vector space \\(V\\) . Then there exists unitary \\(U\\) and positive operators \\(J\\) and \\(K\\) such that, \\(A= UJ=KU\\) where the unique positive operators shall satisfy the equations \\(J \\equiv \\sqrt{A^\\dagger A}\\) and \\(K \\equiv \\sqrt{AA^\\dagger}\\) . Moreover, if \\(A\\) is invertible then \\(U\\) is unique. Singular Value Decomposition Let \\(A\\) be a square matrix. Then there exist unitary matrices \\(U\\) and \\(V\\) , and a diagonal matrix \\(D\\) with non-negative entries such that \\(A = UDV\\) . The diagonal elements of \\(D\\) are called the singular values of \\(A\\) . Proof From polar value decomposition we have, \\(A = SJ = STDT^\\dagger = (ST)D(T^\\dagger) = UDV\\) .","title":"Linear Algebra"},{"location":"research/linear-algebra/#linear-operators-and-matrices","text":"\\[ A (\\Sigma a_i |v_i\\rangle) = \\Sigma a_iA|v_i\\rangle \\] Now, see a linear operator is just a matrix. Suppose \\(A: V \\to W\\) and \\(|v_1\\rangle, |v_2\\rangle, ..., |v_m\\rangle\\) are basis of \\(V\\) and \\(|w_1\\rangle, |w_2\\rangle, ..., |w_n\\rangle\\) is a basis of \\(W\\) then, \\[ A|v_j\\rangle = \\Sigma_i A_{ij}|w_i\\rangle \\]","title":"Linear Operators and Matrices"},{"location":"research/linear-algebra/#inner-products","text":"Ok so imagine an operation \\((\\_ ,\\_): V\\times V \\to \\mathbb{C}\\) such that the following shit holds ok? \\((|v\\rangle, \\Sigma_i \\lambda_i|w_i\\rangle) = \\Sigma_i\\lambda_i(|v\\rangle, |w_i\\rangle)\\) \\((|v\\rangle, |w\\rangle) = (|w\\rangle, |v\\rangle)^*\\) \\((|v\\rangle, |v\\rangle) \\geq 0\\) and \\(= 0\\) iff \\(|v\\rangle\\) In finite dimensions, inner product space i.e., vector spaces equipped with inner prducts for all \\(|v\\rangle \\in\\) vector space \\(=\\) Hilbert Space Consider \\(|i\\rangle\\ \\&\\ |j\\rangle\\) to be orthonormal basis, we have \u2014 \\[ \\langle v|w \\rangle = (\\Sigma_i v_i|i\\rangle, \\Sigma_jw_j|j\\rangle) = \\Sigma_i \\Sigma_j v_i^*w_j\\delta_{ij} = \\Sigma_iv_i^*w_i = |v\\rangle^\\dagger |w\\rangle \\]","title":"Inner Products"},{"location":"research/linear-algebra/#norm-of-a-vector","text":"\\[ ||v|| = \\sqrt{\\langle v|v \\rangle} \\] We can say that \\(|v\\rangle\\) is normalized iff \\(||v|| = 1\\) . A set of \\(|a_i\\rangle\\) vectors is orthonormal if \\(\\langle a_i|a_j \\rangle = \\delta_{ij}\\) i.e., \\(\\forall\\ i \\neq j\\ \\langle a_i|a_j \\rangle = 0\\) and \\(\\langle a_i|a_j \\rangle = 1\\ \\forall\\ i=j\\) .","title":"Norm of a vector"},{"location":"research/linear-algebra/#gram-schmidt-for-orthonormal-basis","text":"\\[ |v_{k+1}\\rangle = \\frac{|w_{k+1}\\rangle - \\Sigma_{i=1}^k \\langle v_i | w_{k+1}\\rangle |v_i\\rangle}{|||w_{k+1}\\rangle - \\Sigma_{i=1}^k \\langle v_i | w_{k+1}\\rangle |v_i\\rangle||},\\ |v_1\\rangle = |w_1\\rangle/|||w_1 \\rangle|| \\]","title":"Gram Schmidt: for orthonormal basis"},{"location":"research/linear-algebra/#outer-product","text":"\\[ |w\\rangle \\langle v|(|v'\\rangle) = |w\\rangle |v\\rangle^\\dagger |v'\\rangle = |w\\rangle \\langle v|v' \\rangle = \\langle v|v' \\rangle |w\\rangle \\] From this notion we obtain the completeness relation, \\(\\Sigma_i |i\\rangle \\langle i| = I\\) . \\(A = I_wAI_v = \\Sigma_{ij} |w_j\\rangle\\langle w_j|A|v_i\\rangle\\langle v_i| = \\Sigma_{ij} \\langle w_j|A|v_i\\rangle|w_j\\rangle\\langle v_i|\\) Cauchy Schwarz: \\(\\langle v|v \\rangle \\langle w|w \\rangle \\geq \\langle v|w \\rangle \\langle w|v \\rangle = |\\langle v|w \\rangle|^2\\)","title":"Outer Product"},{"location":"research/linear-algebra/#hilbert-space","text":"A Hilbert Space \\(\\mathcal{H}\\) is complete which means that every Cauchy sequence of vectors admits in the space itself. Under this hypothesis there exist Hilbert bases also known as complete orthonormal systems of vectors in \\(\\mathcal{H}\\) . For any orthonormal basis of \\(\\mathcal{H}\\) , we have the following. \\[ \\text{Orthonormality} \\equiv\\langle \\psi_i|\\psi_j\\rangle = \\delta_{ij}\\\\ \\text{Completeness} \\equiv\\sum_{i} |\\psi_i\\rangle\\langle\\psi_i| = I \\]","title":"Hilbert Space"},{"location":"research/linear-algebra/#eigenvectors-and-eigenvalues","text":"Under a given linear transformation \\(A\\) , \\(A|v\\rangle = \\lambda|v\\rangle\\) where \\(\\exists\\ |v\\rangle\\) s.t. they do not get shifted off their span. All such vectors are referred as eigenvectors and \\((A - \\lambda I)|v\\rangle = 0 \\implies det|A-\\lambda I| = 0\\) gives all possible eigenvalue. If all \\(\u03bb_i \u2265 0\\) , it is positive semidefinite and if they are \\(> 0\\) , it is positive definite.","title":"Eigenvectors and Eigenvalues"},{"location":"research/linear-algebra/#eigenspace","text":"It is the space of all vectors with a given eigenvalue \\(\\lambda\\) . When an eigenspace is more than one dimensional, we call it degenerate.","title":"Eigenspace"},{"location":"research/linear-algebra/#adjoints-and-hermitian","text":"Suppose \\(A: V \\to V\\) then \\(\\exists\\ A^\\dagger: V \\to V\\) such that \\(\\forall\\ \\vert v\\rangle,\\ \\vert w\\rangle \\in V\\) we have, \\[ (\\vert v\\rangle, A\\vert w\\rangle) = (A^\\dagger \\vert v\\rangle, \\vert w\\rangle) \\] This operator is called as the adjoint or Hermitian conjugate of the operator \\(A\\) . \\[ (\\vert v\\rangle, A\\vert w\\rangle) = \\langle v\\vert A\\vert w\\rangle = \\vec{v}^\\dagger A\\vec{w} = (A^\\dagger\\vec{v})^\\dagger\\vec{w} = (A^\\dagger\\vert v\\rangle, \\vert w\\rangle) \\] We have, \\((AB)^\\dagger = B^\\dagger A^\\dagger\\) \\(\\vert v\\rangle^\\dagger = \\langle v\\vert\\)","title":"Adjoints and Hermitian"},{"location":"research/linear-algebra/#some-defintions","text":"Normal matrices: \\(AA^\\dagger = A^\\dagger A\\) Hermitian matrices: \\(A^\\dagger = A\\) Unitary matrices: \\(AA^\\dagger = I\\) A normal matrix is Hermitian if and only if it has real eigenvalues. If \\(\\langle x| A|x\\rangle \\geq 0, \\forall\\ |x\\rangle\\) then \\(A\\) is positive semi-definite and has positive eigenvalues.","title":"Some defintions"},{"location":"research/linear-algebra/#some-properties","text":"If a Hermitian matrix has positive eigenvalues then it is positive semi-definite. If \\(M = AA^\\dagger\\) then it is both Hermitian and positive semi-definite. All positive semi-definite operators are Hermitian, by definition.","title":"Some properties"},{"location":"research/linear-algebra/#spectral-decomposition","text":"Definition: A linear operator is diagonalizable if and only if it is normal . Some notes and derivation regarding the above: \\(A\\vec{v} = \\lambda\\vec{v} = \\Sigma_i \\lambda_{ij}\\vec{q_i}\\) where \\(q_i\\) 's are linearly independent eigenvalues of \\(A\\) . \\(AQ = Q\\Lambda\\) where \\(Q = \\begin{bmatrix} q_1&q_2 &\\ldots&q_n \\end{bmatrix} \\implies A = Q{\\Lambda}Q^{-1}\\) \\[A = IAI = (P+Q)A(P+Q) = PAP + QAP + PAQ + QAQ\\] \\[\\implies A = \\lambda{P^2} + 0 + 0 + QAQ\\] \\[\\implies A = \\lambda{P^2} + QAQ\\]","title":"Spectral Decomposition"},{"location":"research/linear-algebra/#matrices-and-vectors","text":"In the following statements we are dealing with \\(\\{\\vert i\\rangle\\}\\) as a orthonormal basis set. \\[I = \\Sigma \\vert i\\rangle\\langle i\\vert\\] \\[\\vert \\psi \\rangle = \\Sigma \\sigma_i\\vert i\\rangle, \\text{ where } \\sigma_i = \\langle i\\vert \\psi\\rangle\\] Now, to represent a operator or linear transformation as matrix in orthonormal basis. \\[ A_{ij} = \\langle i\\vert A\\vert j\\rangle\\\\ A = \\Sigma_{i, j} \\langle i\\vert A\\vert j\\rangle \\vert i\\rangle \\langle j\\vert\\\\ \\text{tr}(A) = \\Sigma_i \\langle i\\vert A\\vert i\\rangle \\] Now diagonalization for any normal matrix. \\[ M = \\Sigma_i \\lambda_i \\vert i\\rangle \\langle i\\vert = \\Sigma_i \\lambda_i P_i,\\ \\text{where}\\ P_i^\\dagger = P_i\\\\ f(M) = \\Sigma_i f(\\lambda_i) \\vert i\\rangle \\langle i\\vert \\] where \\(\\lambda_i\\) are eigenvalues of \\(M\\) under a given orthonormal basis set \\(\\{\\vert i\\rangle\\}\\) for vector space \\(V\\) , each \\(\\vert i \\rangle\\) is an eigenvector of \\(M\\) with eigenvalue \\(\\lambda_i\\) . If \\(M\\) is Hermitian , all eigenvalues \\((\\lambda_i\\text{ s})\\) are real.","title":"Matrices and Vectors"},{"location":"research/linear-algebra/#tensor-products","text":"\\(z\\vert{vw}\\rangle = (z\\vert{v}\\rangle) \\otimes (\\vert{w}\\rangle) =(\\vert{v}\\rangle) \\otimes (z\\vert{w}\\rangle)\\) \\((\\vert v_1 \\rangle + \\vert v_2 \\rangle) \\otimes \\vert w \\rangle = \\vert{v_1w}\\rangle + \\vert{v_2w}\\rangle\\) \\(\\vert v \\rangle \\otimes (\\vert w_1 \\rangle + \\vert w_2 \\rangle) = \\vert{vw_1}\\rangle + \\vert{vw_2}\\rangle\\) \\(\\vert \\psi \\rangle^{\\otimes^k} = \\vert \\psi \\rangle \\otimes \\ldots \\otimes \\vert \\psi \\rangle \\text{ k times}\\) \\((A \\otimes B)^\\dagger = A^\\dagger \\otimes B^\\dagger\\)","title":"Tensor Products"},{"location":"research/linear-algebra/#linear-product","text":"\\(A\\otimes{B}\\) forms the linear operator that acts on \\(V\\otimes W\\) vector space givern that \\(A\\) acts on \\(V\\) and \\(B\\) acts on \\(W\\) . \\[ (A\\otimes B)(\\Sigma_{i} a_i\\vert v_i\\rangle \\otimes \\vert w_i \\rangle) = \\Sigma_{i} a_iA\\vert v_i\\rangle \\otimes B\\vert w_i \\rangle \\]","title":"Linear Product"},{"location":"research/linear-algebra/#inner-product","text":"\\[ (\\Sigma_{i} a_i\\vert v_i\\rangle \\otimes \\vert w_i \\rangle, \\Sigma_{i} a_j\\vert v'_j\\rangle \\otimes \\vert w'_j \\rangle) = \\Sigma_{ij} a_i^*b_j \\langle v_i\\vert v'_j\\rangle\\langle w_i\\vert w'_j\\rangle \\]","title":"Inner Product"},{"location":"research/linear-algebra/#trace","text":"Properties of trace are given below as follows. \\(\\text{tr}(A) = \\Sigma_i A_{ii}\\) \\(\\text{tr}(A) = \\Sigma_i \\langle i\\vert A\\vert i\\rangle\\) for orthonormal basis \\(\\text{tr}(AB) = \\text{tr}(BA)\\) \\(\\text{tr}(zA+B) = z\\cdot \\text{tr}(A) + \\text{tr}(B)\\) The above properties yield certain implications as follows. \\(\\text{tr}(UAU^\\dagger) = tr(A)\\) \\(\\text{tr}(A\\vert \\psi\\rangle\\langle\\psi\\vert) = \\Sigma_i \\langle i\\vert A\\vert \\psi\\rangle\\langle\\psi\\vert i \\rangle\\) \\(\\text{tr}(A) = \\sum_i \\lambda_i\\) , \\(\\text{det}(A) = \\prod_i \\lambda_i\\) with algebraic multiplicities \\[ ||A|| = \\sqrt{tr (A^\\dagger A)} \\]","title":"Trace"},{"location":"research/linear-algebra/#partial-trace","text":"Entanglement excludes the possibility of associating state vectors with individual subsystems. Therefore, we introduce density matrices and the corresponding idea of reduction preformed with partial trace. \\[ \\text{tr}(A \\otimes B) = \\text{ tr}(A) \\cdot \\text{tr}(B) \\] \\[ \\rho_{AB} : \\mathcal{H_A}\\otimes\\mathcal{H_B} \\xrightarrow{\\text{ tr}_B} \\rho_A : \\mathcal{H_A} \\] \\[ \\text{ tr}_B(AB) = A \\text{ tr}(B) \\]","title":"Partial Trace"},{"location":"research/linear-algebra/#hilbert-schimdt-inner-product","text":"\\(L_v\\) forms the vector space of operators over the Hilbert space \\(V\\) . Then, we can show that \\(L_v\\) is also a Hilbert space with \\(\\text{tr}(A^\\dagger B)\\) as the inner product operator on \\(L_v \\times L_v\\) . Also, we have \\(div(L_v) = dim(V)^2\\) .","title":"Hilbert-Schimdt Inner Product"},{"location":"research/linear-algebra/#commutator-and-anti-commutator","text":"\\[ [A, B] = AB - BA\\\\ \\{A, B\\} = AB + BA \\]","title":"Commutator and Anti-commutator"},{"location":"research/linear-algebra/#theorem-of-simultaneous-diagonalization","text":"Suppose \\(A\\) and \\(B\\) are both Hermitian matrices, then \\([A, B] = 0\\) iff \\(\\exists\\) orthonormal basis such that both \\(A\\) and \\(B\\) are diagonal with respect to that basis.","title":"Theorem of Simultaneous Diagonalization"},{"location":"research/linear-algebra/#polar-value-decomposition","text":"If \\(A\\) is any linear operator and \\(U\\) is a unitary then \\(J, K\\) are positive operators, such that \\[ A = UJ = KU, \\text{ where } J = \\sqrt{A^\\dagger A} \\text{ and } K = \\sqrt{AA^\\dagger} \\] Moreover, if \\(A^{-1}\\) exists, then \\(U\\) is unique.","title":"Polar Value Decomposition"},{"location":"research/linear-algebra/#singular-value-decomposition","text":"SVD in general is given as follows. \\[ {U\\Sigma V^{T}} \\] It generalizes the eigen decomposition of a square normal matrix with an orthonormal eigen basis to any \\(m\\times n\\) matrix. \\({\\Sigma}\\) is an \\({m\\times n}\\) rectangular diagonal matrix with non-negative real numbers on the diagonal (called singular values). Corollary: If \\(A\\) is a square matrix and \\(\\exists\\ U, V\\) unitaries then \\(D\\) is a diagonal matrix, such that \\[ A = UDV \\] where \\(D\\) has non-negative values. Corollary: If \\(A\\) has non-negative eigenvalues then, \\(A = U^\\dagger DU\\) is possible where \\(D\\) has non-negative values. If \\(A\\) is square both SVD and EVD exist but might not be same. If \\(A\\) is a square symmetric matrix both SVD and EVD exist and are equivalent. If \\(A\\) is non-square only SVD is possible.","title":"Singular Value Decomposition"},{"location":"research/linear-algebra/#rank-of-a-matrix","text":"Rank \\(=\\) number of dimensions in column space. The row rank is the largest number of rows of \\(A\\) that constitute a linearly independent set. The column rank is the largest number of columns of \\(A\\) that constitute a linearly independent set. Moreover, column-rank \\(=\\) row-rank for \\(A \\in \\mathbb{R}^{m \\times n}\\) . \\[ rank(A \\in \\mathbb{R}^{m \\times n}) \\leq min(m, n) \\] Matrix is called full rank if equality holds. \\(rank(A^T) = rank(A)\\) \\(rank(AB) \u2264 min(rank(A), rank(B))\\) \\(rank(A + B) \u2264 rank(A) + rank(B)\\)","title":"Rank of a matrix"},{"location":"research/linear-algebra/#projection-and-spaces","text":"\\[ Proj(y ; A) = argmin_{v \u2208R(A)} ||v \u2212 y||_2 = A(A^T A)^{\u22121} A^T y \\] \\(\\mathcal{N} (A) = \\{x \u2208 \\mathbb R^n : Ax = 0\\}\\) denotes all vectors in \\(\\mathbb R^n\\) that land at the origin after transformation. It is also called kernel. \\(\\mathcal R(A) = \\{v \u2208 \\mathbb R^m : v = Ax,~x \u2208 \\mathbb R^n \\}\\) denotes the space spanned by the transformed basis vectors in \\(\\mathbb R^n\\) . \\(\\mathcal R(A^T)\\) and \\(\\mathcal N (A)\\) are orthogonal spaces which together span \\(\\mathbb R^n\\) . Determinant \\(\\neq 0\\) implies that the matrix has an inverse.","title":"Projection and Spaces"},{"location":"research/linear-algebra/#quadratic-forms","text":"Reminder: we are in real \\(\\mathbb R\\) space. \\[ x^TAx = (x^TAx)^T = x^T(\\frac 1 2 A + \\frac 1 2 A^T)x \\]","title":"Quadratic Forms"},{"location":"research/linear-algebra/#moore-penrose-pseudoinverse","text":"\\[ A^{\\text{left inv}} = (A^\\dagger A)^{-1}A^\\dagger \\] \\[ A^{\\text{right inv}} = A^\\dagger(AA^\\dagger)^{-1} \\] This is a pseudo inverse formalism with left and right inverses. \\[ A^{\\text{left inv}}A = I\\\\ AA^{\\text{right inv}} = I \\]","title":"Moore-Penrose Pseudoinverse"},{"location":"research/linear-algebra/#row-echelon-forms","text":"A matrix is in row echelon form if: All rows consisting of only zeroes are at the bottom. The leading coefficient (also called the pivot) of a nonzero row is always strictly to the right of the leading coefficient of the row above it. A matrix is in reduced row echelon form if: It is in row echelon form. The leading entry in each nonzero row is a 1 (called a leading 1). Each column containing a leading 1 has zeros in all its other entries.","title":"Row Echelon Forms"},{"location":"research/linear-algebra/#spectral-decomposition_1","text":"Any normal operator \\(M\\) on a vector space \\(V\\) is diagonal with respect to some orthonormal basis for \\(V\\) . Conversely, any diagonalizable operator is normal.","title":"Spectral Decomposition"},{"location":"research/linear-algebra/#proof","text":"\\[ M = (P+Q)M(P+Q) = PMP+QMP+PMQ+QMQ = PMP + QMQ \\] Now we have \\[ QM = QM(P+Q) = QMQ \\text{ and } QM^\\dagger = QM^\\dagger Q \\] \\[ QMQQM^\\dagger Q = QMM^\\dagger Q = QM^\\dagger MQ = QM^\\dagger QQMQ \\] Thus, if \\(M\\) is normal then \\(M = PMP + QMQ\\) where \\(PMP = \\lambda P^2 = \\lambda P\\) and thus is diagonalizable wrt orthonormal basis for \\(P\\) . Similarly, \\(QMQ\\) is also diagonalizable wrt some orthonormal basis for \\(Q\\) . Thus, \\(M\\) is diagonalizable for orthonormal basis of the entire vector space. \\[ M = \\Sigma \\lambda_i \\vert i\\rangle \\langle i \\vert = \\Sigma \\lambda_i P_i \\]","title":"Proof"},{"location":"research/linear-algebra/#polar-value-decomposition_1","text":"Let \\(A\\) be a matrix on vector space \\(V\\) . Then there exists unitary \\(U\\) and positive operators \\(J\\) and \\(K\\) such that, \\(A= UJ=KU\\) where the unique positive operators shall satisfy the equations \\(J \\equiv \\sqrt{A^\\dagger A}\\) and \\(K \\equiv \\sqrt{AA^\\dagger}\\) . Moreover, if \\(A\\) is invertible then \\(U\\) is unique.","title":"Polar Value Decomposition"},{"location":"research/linear-algebra/#singular-value-decomposition_1","text":"Let \\(A\\) be a square matrix. Then there exist unitary matrices \\(U\\) and \\(V\\) , and a diagonal matrix \\(D\\) with non-negative entries such that \\(A = UDV\\) . The diagonal elements of \\(D\\) are called the singular values of \\(A\\) .","title":"Singular Value Decomposition"},{"location":"research/linear-algebra/#proof_1","text":"From polar value decomposition we have, \\(A = SJ = STDT^\\dagger = (ST)D(T^\\dagger) = UDV\\) .","title":"Proof"},{"location":"research/millennium/","text":"What gets you a million dollars? 1 down (by Perelman), 6 to go. The seven millennium problems. Hardest way (debatable) to generate a million dollars. Birch and Swinnerton-Dyer Conjecture : \"Imagine you've just been on a first date with someone. Now, based on how that date went, you're trying to predict how many more dates you two will have. Let's say there's a unique \"vibe meter\" that measures the chemistry between two people on a date.\" This conjecture relates the number of rational points on an elliptic curve to the behavior of the curve near the origin. The Birch and Swinnerton-Dyer conjecture relates the rank of the abelian group of points over a number field of an elliptic curve to the order of the zero of the associated-function at \\(s = 1\\) . Hodge Conjecture : \"Think of a jigsaw puzzle. Some pieces are unique and special. This problem asks if certain mathematical shapes can always be broken down into a combination of these special pieces.\" Given a projective complex manifold, every Hodge class on it is a linear combination with rational coefficients of the cohomology classes of complex subvarieties of it. Navier\u2013Stokes Existence and Smoothness : \"Fluids can be calm, but sometimes they throw tantrums and become turbulent. Can we predict when and how these tantrums occur?\" Letting \\( u_i \\) be the \\( i \\) th component of the velocity field (which is a vector field), \\( p \\) be the pressure field (which is a scalar), \\( \\rho \\) be the density of the fluid, \\( \\nu \\) be the kinematic viscosity, and \\( f_i \\) be the \\( i \\) th component of the force field, the equations are: \\[ \\frac{\\partial v_i}{\\partial t} + \\sum_{j=1}^{n} u_j \\frac{\\partial u_i}{\\partial x_j} = -\\frac{1}{\\rho} \\frac{\\partial p}{\\partial x_i} + \\nu \\sum_{j=1}^{n} \\frac{\\partial^2 v_i}{\\partial x_j^2} + f_i. \\] P vs NP Problem : \"Solving a riddle is hard, but checking an answer is easy. But what if computers could solve as easily as they check? This would change the world and not always in a good way!\" \\[ P \\stackrel{?}{=} NP \\] Riemann Hypothesis : \"There's a mysterious music note that every mathematician wants to hear. It's believed all these notes line up perfectly in a row. Proving this could unlock the secrets of prime numbers.\" States that all nontrivial (the trivial roots are when \\(s=-2, -4, -6, \u2026\\) ) zeros of the Riemann zeta function have real part \\({1}/{2}\\) . \\[ \\zeta (s)=\\sum _{n=1}^{\\infty }{\\frac {1}{n^{s}}}={\\frac {1}{1^{s}}}+{\\frac {1}{2^{s}}}+{\\frac {1}{3^{s}}}+\\cdots \\] \\[ \\sum _{n=1}^{\\infty }{\\frac {1}{n^{s}}}=\\prod _{p{\\text{ prime}}}{\\frac {1}{1-p^{-s}}} = {\\frac {1}{1-2^{-s}}}\\cdot {\\frac {1}{1-3^{-s}}}\\cdot {\\frac {1}{1-5^{-s}}}\\cdots {\\frac {1}{1-p^{-s}}}\\cdots \\] Yang\u2013Mills Existence and Mass Gap : \"Particles are the universe's aunties; some carry forces like they're passing on the latest gossip. Even though we thought some of them shouldn't weigh anything, turns out they've got some substance!\" \u201cIt's like trying to weigh a ghost. The equations say these particles should have no mass, but in real life, they seem to have weight. Did they sneak in some cosmic donuts?\u201d \\[ \\text{I HAVE NO FUCKING CLUE} :) \\]","title":"What gets you a million dollars?"},{"location":"research/millennium/#what-gets-you-a-million-dollars","text":"1 down (by Perelman), 6 to go. The seven millennium problems. Hardest way (debatable) to generate a million dollars. Birch and Swinnerton-Dyer Conjecture : \"Imagine you've just been on a first date with someone. Now, based on how that date went, you're trying to predict how many more dates you two will have. Let's say there's a unique \"vibe meter\" that measures the chemistry between two people on a date.\" This conjecture relates the number of rational points on an elliptic curve to the behavior of the curve near the origin. The Birch and Swinnerton-Dyer conjecture relates the rank of the abelian group of points over a number field of an elliptic curve to the order of the zero of the associated-function at \\(s = 1\\) . Hodge Conjecture : \"Think of a jigsaw puzzle. Some pieces are unique and special. This problem asks if certain mathematical shapes can always be broken down into a combination of these special pieces.\" Given a projective complex manifold, every Hodge class on it is a linear combination with rational coefficients of the cohomology classes of complex subvarieties of it. Navier\u2013Stokes Existence and Smoothness : \"Fluids can be calm, but sometimes they throw tantrums and become turbulent. Can we predict when and how these tantrums occur?\" Letting \\( u_i \\) be the \\( i \\) th component of the velocity field (which is a vector field), \\( p \\) be the pressure field (which is a scalar), \\( \\rho \\) be the density of the fluid, \\( \\nu \\) be the kinematic viscosity, and \\( f_i \\) be the \\( i \\) th component of the force field, the equations are: \\[ \\frac{\\partial v_i}{\\partial t} + \\sum_{j=1}^{n} u_j \\frac{\\partial u_i}{\\partial x_j} = -\\frac{1}{\\rho} \\frac{\\partial p}{\\partial x_i} + \\nu \\sum_{j=1}^{n} \\frac{\\partial^2 v_i}{\\partial x_j^2} + f_i. \\] P vs NP Problem : \"Solving a riddle is hard, but checking an answer is easy. But what if computers could solve as easily as they check? This would change the world and not always in a good way!\" \\[ P \\stackrel{?}{=} NP \\] Riemann Hypothesis : \"There's a mysterious music note that every mathematician wants to hear. It's believed all these notes line up perfectly in a row. Proving this could unlock the secrets of prime numbers.\" States that all nontrivial (the trivial roots are when \\(s=-2, -4, -6, \u2026\\) ) zeros of the Riemann zeta function have real part \\({1}/{2}\\) . \\[ \\zeta (s)=\\sum _{n=1}^{\\infty }{\\frac {1}{n^{s}}}={\\frac {1}{1^{s}}}+{\\frac {1}{2^{s}}}+{\\frac {1}{3^{s}}}+\\cdots \\] \\[ \\sum _{n=1}^{\\infty }{\\frac {1}{n^{s}}}=\\prod _{p{\\text{ prime}}}{\\frac {1}{1-p^{-s}}} = {\\frac {1}{1-2^{-s}}}\\cdot {\\frac {1}{1-3^{-s}}}\\cdot {\\frac {1}{1-5^{-s}}}\\cdots {\\frac {1}{1-p^{-s}}}\\cdots \\] Yang\u2013Mills Existence and Mass Gap : \"Particles are the universe's aunties; some carry forces like they're passing on the latest gossip. Even though we thought some of them shouldn't weigh anything, turns out they've got some substance!\" \u201cIt's like trying to weigh a ghost. The equations say these particles should have no mass, but in real life, they seem to have weight. Did they sneak in some cosmic donuts?\u201d \\[ \\text{I HAVE NO FUCKING CLUE} :) \\]","title":"What gets you a million dollars?"},{"location":"research/multi-variate/","text":"Differentials \\(df = f_xdx + f_ydy+f_zdz\\) \\(df \\neq \\Delta f\\) \\(\\Delta f = f_x\\Delta x + f_y\\Delta y + f_z\\Delta z\\) \\(df\\) is used to encode infinitesimal changes used to act as a placegolder value divide wrt time to get rate of change \\(\\rightarrow\\) CHAIN RULE Chain Rule with More Variables Let \\(w = f(x, y)\\) when \\(x(u, v), y(u, v)\\) then, \\[ dw = f_x dx + f_y dy = (f_xx_u+ f_yy_u)du + (f_xx_v+ f_yy_v)dv = \\frac{\\partial f}{\\partial u}du + \\frac{\\partial f}{\\partial v}dv \\] Gradient Vector \\[ \\frac{dw}{dt} = w_x \\frac{dx}{dt} + w_y \\frac{dy}{dt} + w_z \\frac{dz}{dt} = \\vec{\\nabla} w.\\frac{d\\vec{r}}{dt} \\] Note: \\(\\vec{\\nabla}w \\ \\perp \\text{ level surfaces}\\) (tangent to the level surface at any given point) Directional Derivatives \\[ \\frac{dw}{ds}|_{\\hat{u}} = \\vec{\\nabla}w \\cdot \\frac{d\\vec{r}}{ds} = \\vec{\\nabla}w \\cdot \\hat{u} \\] Implications Direction of \\(\\vec{\\nabla}w\\) is the direction of fastest increase of \\(w\\) Lagrange Multipliers Goal: minima/maximize a multi-variable function ( \\(min/max\\ \\ f(x, y, z)\\) ) where \\(x, y, z\\) are not independent and \\(\\exists\\) \\(g(x, y, z) = c\\) . These can be obtained on combining the given restraints with the following. \\[ \\vec{\\nabla}f = \\lambda \\vec{\\nabla}g \\] Basic idea: to find \\((x, y)\\) where the level curves of \\(f\\) and \\(g\\) are tangent to each other ( \\(\\vec{\\nabla}f \\parallel \\vec{\\nabla}g\\) ). Note: Take care that the point is indeed a maxima or minima as required and not just a saddle point (second derivative test won't be applicable so be vigilant). Functions Example Value First derivative Second derivative \\(f: \\mathbb R \\to \\mathbb R\\) \\(x^2\\) \\(\\mathbb R\\) \\(\\mathbb R\\) \\(\\mathbb R\\) \\(f: \\mathbb R^d \\to \\mathbb R\\) loss function \\(\\mathbb R\\) \\(\\mathbb R^d [\\text{gradient}]\\) \\(\\mathbb R^{d\\times d} [\\text{hessian}]\\) \\(f: \\mathbb R^d \\to \\mathbb R^p\\) neural net layer \\(\\mathbb R^p\\) \\(\\mathbb R^{d \\times p} [\\text{jacobian}]\\) \\(\\mathbb R^{d \\times p \\times p}\\) Gradient \\[ \\nabla_x f(x) = \\nabla_x f(x_1, \\cdots, x_d) = \\begin{bmatrix} \\frac{\\partial f(x)}{\\partial x_1}\\\\ \\vdots\\\\ \\frac{\\partial f(x)}{\\partial x_d} \\end{bmatrix} \\] \\[ \\nabla_Af(A) = \\begin{bmatrix} \\frac{\\partial f(A)}{\\partial A_{11}} & \\cdots\\\\ \\vdots & \\vdots\\\\ \\cdots & \\frac{\\partial f(A)}{\\partial A_{mn}} \\end{bmatrix} \\] Hessian We have \\(f: \\mathbb R^d \\to \\mathbb R^p\\) thus, \\(f(x_1, \\cdots, x_d) = \\begin{bmatrix} f_1(x_1, \\cdots, x_d)\\\\ \\vdots \\\\ f_p(x_1, \\cdots, x_d) \\end{bmatrix}\\) Note: Hessians are square-symmetric matrices. \\[ \\nabla_x^2 f(x) = \\begin{bmatrix} \\frac{\\partial^2f(x)}{\\partial x_1^2} & \\frac{\\partial^2f(x)}{\\partial x_1x_2} & \\cdots\\\\ \\vdots & \\ddots & \\vdots \\\\ \\vdots &\\cdots & \\frac{\\partial^2f(x)}{\\partial x_n^2} \\end{bmatrix} \\] Jacobian \\[ J = \\begin{bmatrix} \\dfrac{\\partial \\mathbf{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\mathbf{f}}{\\partial x_d}\\end{bmatrix}= \\begin{bmatrix} \\nabla^{\\mathrm T} f_1 \\\\ \\vdots \\\\ \\nabla^{\\mathrm T} f_p \\end{bmatrix}= \\begin{bmatrix} \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\dfrac{\\partial f_p}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_p}{\\partial x_d}\\end{bmatrix} \\] where \\(\\nabla^{\\mathrm T} f_i\\) is the transpose (row vector) of the gradient of the \\(i\\) component. Examples \\(\\nabla_xb^Tx = b\\) \\(\\nabla_x^2 b^Tx = 0\\) \\(\\nabla_xx^TAx = 2Ax\\) , if \\(A\\) is symmetric \\(\\nabla_x^2x^TAx = 2A\\) , if \\(A\\) is symmetric","title":"Multivariate Calculus"},{"location":"research/multi-variate/#differentials","text":"\\(df = f_xdx + f_ydy+f_zdz\\) \\(df \\neq \\Delta f\\) \\(\\Delta f = f_x\\Delta x + f_y\\Delta y + f_z\\Delta z\\) \\(df\\) is used to encode infinitesimal changes used to act as a placegolder value divide wrt time to get rate of change \\(\\rightarrow\\) CHAIN RULE","title":"Differentials"},{"location":"research/multi-variate/#chain-rule-with-more-variables","text":"Let \\(w = f(x, y)\\) when \\(x(u, v), y(u, v)\\) then, \\[ dw = f_x dx + f_y dy = (f_xx_u+ f_yy_u)du + (f_xx_v+ f_yy_v)dv = \\frac{\\partial f}{\\partial u}du + \\frac{\\partial f}{\\partial v}dv \\]","title":"Chain Rule with More Variables"},{"location":"research/multi-variate/#gradient-vector","text":"\\[ \\frac{dw}{dt} = w_x \\frac{dx}{dt} + w_y \\frac{dy}{dt} + w_z \\frac{dz}{dt} = \\vec{\\nabla} w.\\frac{d\\vec{r}}{dt} \\] Note: \\(\\vec{\\nabla}w \\ \\perp \\text{ level surfaces}\\) (tangent to the level surface at any given point)","title":"Gradient Vector"},{"location":"research/multi-variate/#directional-derivatives","text":"\\[ \\frac{dw}{ds}|_{\\hat{u}} = \\vec{\\nabla}w \\cdot \\frac{d\\vec{r}}{ds} = \\vec{\\nabla}w \\cdot \\hat{u} \\]","title":"Directional Derivatives"},{"location":"research/multi-variate/#implications","text":"Direction of \\(\\vec{\\nabla}w\\) is the direction of fastest increase of \\(w\\)","title":"Implications"},{"location":"research/multi-variate/#lagrange-multipliers","text":"Goal: minima/maximize a multi-variable function ( \\(min/max\\ \\ f(x, y, z)\\) ) where \\(x, y, z\\) are not independent and \\(\\exists\\) \\(g(x, y, z) = c\\) . These can be obtained on combining the given restraints with the following. \\[ \\vec{\\nabla}f = \\lambda \\vec{\\nabla}g \\] Basic idea: to find \\((x, y)\\) where the level curves of \\(f\\) and \\(g\\) are tangent to each other ( \\(\\vec{\\nabla}f \\parallel \\vec{\\nabla}g\\) ). Note: Take care that the point is indeed a maxima or minima as required and not just a saddle point (second derivative test won't be applicable so be vigilant). Functions Example Value First derivative Second derivative \\(f: \\mathbb R \\to \\mathbb R\\) \\(x^2\\) \\(\\mathbb R\\) \\(\\mathbb R\\) \\(\\mathbb R\\) \\(f: \\mathbb R^d \\to \\mathbb R\\) loss function \\(\\mathbb R\\) \\(\\mathbb R^d [\\text{gradient}]\\) \\(\\mathbb R^{d\\times d} [\\text{hessian}]\\) \\(f: \\mathbb R^d \\to \\mathbb R^p\\) neural net layer \\(\\mathbb R^p\\) \\(\\mathbb R^{d \\times p} [\\text{jacobian}]\\) \\(\\mathbb R^{d \\times p \\times p}\\)","title":"Lagrange Multipliers"},{"location":"research/multi-variate/#gradient","text":"\\[ \\nabla_x f(x) = \\nabla_x f(x_1, \\cdots, x_d) = \\begin{bmatrix} \\frac{\\partial f(x)}{\\partial x_1}\\\\ \\vdots\\\\ \\frac{\\partial f(x)}{\\partial x_d} \\end{bmatrix} \\] \\[ \\nabla_Af(A) = \\begin{bmatrix} \\frac{\\partial f(A)}{\\partial A_{11}} & \\cdots\\\\ \\vdots & \\vdots\\\\ \\cdots & \\frac{\\partial f(A)}{\\partial A_{mn}} \\end{bmatrix} \\]","title":"Gradient"},{"location":"research/multi-variate/#hessian","text":"We have \\(f: \\mathbb R^d \\to \\mathbb R^p\\) thus, \\(f(x_1, \\cdots, x_d) = \\begin{bmatrix} f_1(x_1, \\cdots, x_d)\\\\ \\vdots \\\\ f_p(x_1, \\cdots, x_d) \\end{bmatrix}\\) Note: Hessians are square-symmetric matrices. \\[ \\nabla_x^2 f(x) = \\begin{bmatrix} \\frac{\\partial^2f(x)}{\\partial x_1^2} & \\frac{\\partial^2f(x)}{\\partial x_1x_2} & \\cdots\\\\ \\vdots & \\ddots & \\vdots \\\\ \\vdots &\\cdots & \\frac{\\partial^2f(x)}{\\partial x_n^2} \\end{bmatrix} \\]","title":"Hessian"},{"location":"research/multi-variate/#jacobian","text":"\\[ J = \\begin{bmatrix} \\dfrac{\\partial \\mathbf{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\mathbf{f}}{\\partial x_d}\\end{bmatrix}= \\begin{bmatrix} \\nabla^{\\mathrm T} f_1 \\\\ \\vdots \\\\ \\nabla^{\\mathrm T} f_p \\end{bmatrix}= \\begin{bmatrix} \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\dfrac{\\partial f_p}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_p}{\\partial x_d}\\end{bmatrix} \\] where \\(\\nabla^{\\mathrm T} f_i\\) is the transpose (row vector) of the gradient of the \\(i\\) component.","title":"Jacobian"},{"location":"research/multi-variate/#examples","text":"\\(\\nabla_xb^Tx = b\\) \\(\\nabla_x^2 b^Tx = 0\\) \\(\\nabla_xx^TAx = 2Ax\\) , if \\(A\\) is symmetric \\(\\nabla_x^2x^TAx = 2A\\) , if \\(A\\) is symmetric","title":"Examples"},{"location":"research/music-theory/","text":"Music theory is the way of representing, discretizing, teaching and hence of ultimately effectively building music. By understanding music theory and how a composition works, you can: Learn to see limitations as freedom. Strive to create something unique, beautiful and most importantly worth creating. This strife will define greatness. Notes on a piano in comparision with those on sheet Musical Notes Notes : C, D, C# blah blah blah Same notes of different pitches belong to the same \u201c pitch class \u201d. Octave : ABCDEF Octave equivalence states that all notes from the same pitch class appeal similarly to the human ear, albeit at a different frequency or pitch. Tones and semitones are defined by number of steps taken after playing one note and then another. \\(B\\to C\\) is a semitone and so is \\(E\\to F\\) . Rest all adjacent letter transitions are tones. The above image denotes the C-major scale, the tonic of which is the C note. Scales Diatonic scale: a scale composed of 5 tones and 2 semitones. When we change the order in which the five tones and two semitones occur, the scale sounds different. We could say it has a different quality**. The seven classic diatonic modes are those defined by the 7 letter notes. A major D major Guitar and Piano (side by side) Scales at last Chords A triad is a three-note chord. The following are the C-major and A-minor traids respectively. Now, we have Major third + Perfect fifth = Major chord Minor third + Perfect fifth = Minor chord Major third + Diminished fifth = Diminished chord Primary Chords C, F and G are the primary chords (here these are majors since there is no small lettered m or dim mentioned to denote otherwise). They are called the C major tonic triad, the subdominant triad and the dominant triad respectively. Ukulele chords Rudiments of Duration Notation The above graphics show the symbols used to depict duration of both notes (sounds) and of rests (silence). They kinda look pretty beautiful as an amalgation. Key signatures","title":"Music Theory"},{"location":"research/music-theory/#musical-notes","text":"Notes : C, D, C# blah blah blah Same notes of different pitches belong to the same \u201c pitch class \u201d. Octave : ABCDEF Octave equivalence states that all notes from the same pitch class appeal similarly to the human ear, albeit at a different frequency or pitch. Tones and semitones are defined by number of steps taken after playing one note and then another. \\(B\\to C\\) is a semitone and so is \\(E\\to F\\) . Rest all adjacent letter transitions are tones. The above image denotes the C-major scale, the tonic of which is the C note.","title":"Musical Notes"},{"location":"research/music-theory/#scales","text":"Diatonic scale: a scale composed of 5 tones and 2 semitones. When we change the order in which the five tones and two semitones occur, the scale sounds different. We could say it has a different quality**. The seven classic diatonic modes are those defined by the 7 letter notes.","title":"Scales"},{"location":"research/music-theory/#a-major","text":"","title":"A major"},{"location":"research/music-theory/#d-major","text":"","title":"D major"},{"location":"research/music-theory/#guitar-and-piano-side-by-side","text":"","title":"Guitar and Piano (side by side)"},{"location":"research/music-theory/#scales-at-last","text":"","title":"Scales at last"},{"location":"research/music-theory/#chords","text":"A triad is a three-note chord. The following are the C-major and A-minor traids respectively. Now, we have Major third + Perfect fifth = Major chord Minor third + Perfect fifth = Minor chord Major third + Diminished fifth = Diminished chord","title":"Chords"},{"location":"research/music-theory/#primary-chords","text":"C, F and G are the primary chords (here these are majors since there is no small lettered m or dim mentioned to denote otherwise). They are called the C major tonic triad, the subdominant triad and the dominant triad respectively.","title":"Primary Chords"},{"location":"research/music-theory/#ukulele-chords","text":"","title":"Ukulele chords"},{"location":"research/music-theory/#rudiments-of-duration-notation","text":"The above graphics show the symbols used to depict duration of both notes (sounds) and of rests (silence). They kinda look pretty beautiful as an amalgation.","title":"Rudiments of Duration Notation"},{"location":"research/music-theory/#key-signatures","text":"","title":"Key signatures"},{"location":"research/poker/","text":"Basic Strategy When you have \\(\\leq 12\\text{BB}\\) in tournament or in general low stack, you should ideally try to shove it soon sometime and play more often. Odds \\(= \\text{reward}:\\text{risk}\\) Pot odds percentage \\(=\\) \\(\\frac{\\text{your bet}}{\\text{final pot value}} \\times 100\\% = \\frac{\\text{risk}}{\\text{risk + reward}} \\times 100\\%\\) Final pot value \\(=\\) pot value after you bet Call when your hand equity \\(>\\) pod odds percentage Equity = Outs \\(\\times\\) 4 (if both turn and river are left to be seen) Equity = (Outs \\(\\times\\) 4) \\(-\\) (Outs \\(-\\) 8) (if both turn and river are left to be seen) Equity = Outs \\(\\times\\) 2 (if only river is left to be seen) Don\u2019t tilt Be TAG Be unpredictable Pairs aren\u2019t worth much Shove only when strongest Look for straights or flushes of opponents Note: First in bluff is often something to use and watch out for. In Heads up, 70% of the time the flop misses both players. So, whoever bets first has a big advantage. Even mutiway, if no body takes interest, the first aggresor wins. People often C-Bet no matter the flop. Handy Outs Table Number of outs Drawing Hand 4 2 pair + needing a full house + inside straight draw 6 2 overcards needing to make a pair 8 Open ended straight draw 9 Flush draw 11 Flush draw plus a pair needing to improve to trips 12 Flush draw plus inside straight draw 15 Flush draw plus open ended straight draw 14 outs : makes you even money against a better hand (DO NOT FOLD). Ans you will always be getting better than even pot odds from any bet (can go ALL IN). Expressed odds : the odds of completing a hand based on the current pot size and the cost of calling a bet. Implied odds : take into account the potential future bets that may occur if you hit your hand. In general, implied odds tend to be more important in no limit hold'em than expressed odds, since there is no limit to how much players can bet. Range and Hand Decisions Premium Hand : AA, KK, QQ, AK Solid Hands : JJ, TT, 99, 88, 77, AQs, AJs Speculative Hands : Smaller pairs (22+), suited aces (A2s+), suited connectors (78s, 9Ts) Opening Big pairs or QQ+ Raise 3BB Re-raises 3x the previous bet Note JJ is mostly not considered Opening Small pairs or 22+ Weak game Play (max one raise) Strong game Don\u2019t play Warning Negative implied odds in super deep stack games Opening Medium pairs or 66+ Raise 3BB Re-raises > 4-bet FOLD Call 50% - 80% (depending on position) Opening AK Short stack Plays great Deep stack Drawing hand Against a pair Just < 50% (22 to QQ), but against AA 7% and against KK 30% Having suited cards only increases your chances of winning by 2% to 3%. Remember people have 1 in 17 chances of having pocket pairs, so if there are > 8 people on the table you have 50% chance of playing against a pocket pair. While choosing hands to open with, don\u2019t be results oriented since a good decision can look bad. AK is hard to play after flop. Best that you go all-in when playing short stack cash games. Describe a Hand Stack sizes Table image Recent activity Position Cards Thought process Action AQ \\(\\ll\\) AK. A2-AJ are highly overrated. Trouble Hands : KQ, KJ, KT, QJ, QT etc. are called pretty looking If someone represents a top 10% hand > 20% of the time, you know they are bluffing > 50% of the time. If you ever get 3-bet with the trouble hands, say bye bye. Suited Connectors : not good for short stack games but they can be played well as disguised hands (better played in multi-way scenarios) Don\u2019t play GTO poker 15%-20% of the time. Charts SPR SPR Size Hands to Commit Low 0 to 2 Over Pair, Top Pair, Bottom 2 Pair Medium 3 to 6 Top 2 Pair, Non-nutted flushes and strights High 7+ Sets and Nutted Hands Drawing Hand Flop not all-in & Turn equity Bet Sizing Pot Odds offered All-in case Flush + Open-ended SD 33% Slightly over pot + > 33% > 60% Flush + Gut-shot SD 25% 2/3 pot + > 28.5% > 45% Flush 19% 1/2 pot + > 25% > 33% Open-ended SD 17% 1/3 pot + > 25% 34% Over Cards + 2-Pair Draw 13% 1/4 pot + > 25% 26% Gut-shot straight draw 8% 1/4 pot + > 25% 16% Poker Terminology Ante : a stake put up by a player in poker before receiving cards. Effective stack : maximum amount of chips you can loose in a round Aggressive (Bets Often) Passive (Bets Infrequently) Tight (Calls Seldom) TAG TP \"Weak\" Loose (Calls Often) LAG Calling Machine Now, if you are not aggressive it is very highly likely that you will loose. Be aggressive no matter if you are tight or loose. Strategy for Pokerbaazi Don\u2019t bluff at higher stakes more than a very low freuquency. Playing style: play loose or tight depending on table try to be aggresive lure but do not slowplay Probability of getting beaten by a fullhouse or straight or flush is quite higher than normal. If u at all want to all in or overbet: make sure that you have the nuts don\u2019t try to catch bluff with something like 1 or 2 pair Play RIT if you feel prompted to do all in. Don\u2019t play cash games \\(\\ll 100\\ \\text{BB}\\) . Strategy for crushing Crash Games You need to exploit your opponent\u2019s weeknesses. Play few hands from early position, play more hands from late position. Do not slow play unless your opponent can easily catch up to good, but still second best hand.","title":"Poker"},{"location":"research/poker/#basic-strategy","text":"When you have \\(\\leq 12\\text{BB}\\) in tournament or in general low stack, you should ideally try to shove it soon sometime and play more often. Odds \\(= \\text{reward}:\\text{risk}\\) Pot odds percentage \\(=\\) \\(\\frac{\\text{your bet}}{\\text{final pot value}} \\times 100\\% = \\frac{\\text{risk}}{\\text{risk + reward}} \\times 100\\%\\) Final pot value \\(=\\) pot value after you bet Call when your hand equity \\(>\\) pod odds percentage Equity = Outs \\(\\times\\) 4 (if both turn and river are left to be seen) Equity = (Outs \\(\\times\\) 4) \\(-\\) (Outs \\(-\\) 8) (if both turn and river are left to be seen) Equity = Outs \\(\\times\\) 2 (if only river is left to be seen) Don\u2019t tilt Be TAG Be unpredictable Pairs aren\u2019t worth much Shove only when strongest Look for straights or flushes of opponents Note: First in bluff is often something to use and watch out for. In Heads up, 70% of the time the flop misses both players. So, whoever bets first has a big advantage. Even mutiway, if no body takes interest, the first aggresor wins. People often C-Bet no matter the flop.","title":"Basic Strategy"},{"location":"research/poker/#handy-outs-table","text":"Number of outs Drawing Hand 4 2 pair + needing a full house + inside straight draw 6 2 overcards needing to make a pair 8 Open ended straight draw 9 Flush draw 11 Flush draw plus a pair needing to improve to trips 12 Flush draw plus inside straight draw 15 Flush draw plus open ended straight draw 14 outs : makes you even money against a better hand (DO NOT FOLD). Ans you will always be getting better than even pot odds from any bet (can go ALL IN). Expressed odds : the odds of completing a hand based on the current pot size and the cost of calling a bet. Implied odds : take into account the potential future bets that may occur if you hit your hand. In general, implied odds tend to be more important in no limit hold'em than expressed odds, since there is no limit to how much players can bet.","title":"Handy Outs Table"},{"location":"research/poker/#range-and-hand-decisions","text":"Premium Hand : AA, KK, QQ, AK Solid Hands : JJ, TT, 99, 88, 77, AQs, AJs Speculative Hands : Smaller pairs (22+), suited aces (A2s+), suited connectors (78s, 9Ts) Opening Big pairs or QQ+ Raise 3BB Re-raises 3x the previous bet Note JJ is mostly not considered Opening Small pairs or 22+ Weak game Play (max one raise) Strong game Don\u2019t play Warning Negative implied odds in super deep stack games Opening Medium pairs or 66+ Raise 3BB Re-raises > 4-bet FOLD Call 50% - 80% (depending on position) Opening AK Short stack Plays great Deep stack Drawing hand Against a pair Just < 50% (22 to QQ), but against AA 7% and against KK 30% Having suited cards only increases your chances of winning by 2% to 3%. Remember people have 1 in 17 chances of having pocket pairs, so if there are > 8 people on the table you have 50% chance of playing against a pocket pair. While choosing hands to open with, don\u2019t be results oriented since a good decision can look bad. AK is hard to play after flop. Best that you go all-in when playing short stack cash games. Describe a Hand Stack sizes Table image Recent activity Position Cards Thought process Action AQ \\(\\ll\\) AK. A2-AJ are highly overrated. Trouble Hands : KQ, KJ, KT, QJ, QT etc. are called pretty looking If someone represents a top 10% hand > 20% of the time, you know they are bluffing > 50% of the time. If you ever get 3-bet with the trouble hands, say bye bye. Suited Connectors : not good for short stack games but they can be played well as disguised hands (better played in multi-way scenarios) Don\u2019t play GTO poker 15%-20% of the time.","title":"Range and Hand Decisions"},{"location":"research/poker/#charts","text":"SPR SPR Size Hands to Commit Low 0 to 2 Over Pair, Top Pair, Bottom 2 Pair Medium 3 to 6 Top 2 Pair, Non-nutted flushes and strights High 7+ Sets and Nutted Hands Drawing Hand Flop not all-in & Turn equity Bet Sizing Pot Odds offered All-in case Flush + Open-ended SD 33% Slightly over pot + > 33% > 60% Flush + Gut-shot SD 25% 2/3 pot + > 28.5% > 45% Flush 19% 1/2 pot + > 25% > 33% Open-ended SD 17% 1/3 pot + > 25% 34% Over Cards + 2-Pair Draw 13% 1/4 pot + > 25% 26% Gut-shot straight draw 8% 1/4 pot + > 25% 16%","title":"Charts"},{"location":"research/poker/#poker-terminology","text":"Ante : a stake put up by a player in poker before receiving cards. Effective stack : maximum amount of chips you can loose in a round Aggressive (Bets Often) Passive (Bets Infrequently) Tight (Calls Seldom) TAG TP \"Weak\" Loose (Calls Often) LAG Calling Machine Now, if you are not aggressive it is very highly likely that you will loose. Be aggressive no matter if you are tight or loose.","title":"Poker Terminology"},{"location":"research/poker/#strategy-for-pokerbaazi","text":"Don\u2019t bluff at higher stakes more than a very low freuquency. Playing style: play loose or tight depending on table try to be aggresive lure but do not slowplay Probability of getting beaten by a fullhouse or straight or flush is quite higher than normal. If u at all want to all in or overbet: make sure that you have the nuts don\u2019t try to catch bluff with something like 1 or 2 pair Play RIT if you feel prompted to do all in. Don\u2019t play cash games \\(\\ll 100\\ \\text{BB}\\) .","title":"Strategy for Pokerbaazi"},{"location":"research/poker/#strategy-for-crushing-crash-games","text":"You need to exploit your opponent\u2019s weeknesses. Play few hands from early position, play more hands from late position. Do not slow play unless your opponent can easily catch up to good, but still second best hand.","title":"Strategy for crushing Crash Games"},{"location":"research/probability/","text":"Axioms \\(P(A) \\geq 0\\) \\(P(A \\cup B) = P(A) + P(B)\\) , where both are disjoint sets \\(P(\\Omega) = 1\\) Properties \\[ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\] \\[ P(A) \\leq P(B), \\text{ if } A \\subseteq B \\] Conditional Probability \\[ P(A\\vert B) = \\frac{P(A \\cap B)}{P(B)} \\] Total Probability \\[ P(B) = P(A_1 \\cap B) + \\cdots + P (A_n \\cap B) \\] \\[ = \\sum P(A_i) P(B\\vert A_i) \\] Here, \\(A_i\\) \u2019s are independent. Independence If \\(A\\) and \\(B\\) are independent, we have the following. \\[ P(A \\vert B) = P(A) \\] \\[ P(A \\cap B) = P(A)P(B) \\] Conditional Independence \\[ P(A \\cap B \\vert C) = P(A \\vert C) P(B \\vert C) \\] Expectation and Variance \\[ E[g(X)] = \\sum_x g(x)p_X(x) \\] \\[ Var[X] = E[(X - E[X])^2] \\] \\[ Var[X] = E[X^2] - E[X]^2 \\] Linearity \\[ E[aX + b] = aE[X] + b \\] \\[ Var[aX + b] = a^2\\ Var[X] \\] \\[ E[aX + bY + c] = aE[X] + bE[Y] + c \\] Probability Mass Functions \\[ p_{X,Y}(x, y) = P(X=x, Y=y) \\] Marginal PMFs \\[ p_X(x) = \\sum_y p_{{X, Y}}(x, y) \\] \\[ E[g(X, Y)] = \\sum_x \\sum_y g(x, y)p_{X, Y} (x, y) \\] Conditionals \\[ p_{X|A}(x) = P(X = x | A), \\text{ such that } \\sum_x p_{X|A}(x) = 1 \\] \\[ p_X(x) = \\sum P(A_i) p_{x|A_i}(x) \\] \\[ p_{X, Y}(x, y) = p_Y(y)p_{X|Y}(x|y) \\] \\[ p_X = \\sum_y p_Y(y)p_{X|Y}(x|y) \\] \\[ E[g(X)|A] = \\sum_x g(x)p_{X|A}(x) \\] \\[ E[g(X)|Y=y] = \\sum_x g(x)p_{X|Y}(x|y) \\] \\[ E[X] = \\sum P(A_i)E[x|A_i] \\] Independence strikes back \\(p_{X|A}(x) = p_X(x)\\) \\(E[XY]=E[X]E[Y]\\) \\(Var[X + Y] = Var[X] + Var[Y]\\) \\(p_{X, Y}(x, y) = p_X(x) p_Y(y)\\) , \\(\\forall\\) \\(x \\in X\\) and \\(y \\in Y\\) Continuity and Lovely Curves \\[ P(X\\in B)= \\int_B f_X(x)dx, \\text{ where } f_X(x) \\geq 0 \\] \\[ E[g(X)] = \\int_{-\\infty}^{\\infty} g(x)f_X(x)dx \\] Cumulative Distributions \\[ F_X(x) = P(X \\leq x) \\] \\[ F_X(x) = \\int_{-\\infty}^x f_X(x)dx \\] Conditionals \\[ \\int_B f_{X|A}(x)dx = P(X \\in B|A) \\] \\[ f_X(x) = \\sum P(A_i) f_{x|A_i}(x) \\] \\[ f_{X, Y}(x, y) = f_Y(y)f_{X|Y}(x|y) \\] \\[ f_X = \\int_{-\\infty}^{\\infty} f_Y(y)f_{X|Y}(x|y) \\] Conditional Expectation \\[ E[g(X)|A] = \\int_{\\Omega} g(x)f_{X|A}(x) \\] \\[ E[g(X)|Y=y] = \\int_\\Omega g(x)f_{X|Y}(x|y) \\] \\[ E[X] = \\sum P(A_i)E[x|A_i] \\] Bayes' Theorem You can of course interchange \\(p\\) with \\(f\\) to account for continuous random variables. \\[ p_X(x)p_{Y|X}(y|x) = p_Y(y)p_{X|Y}(x|Y) \\] Gimme More \\[ F_Y(y) = P(g(X)\\leq y) = \\int_{x | g(x) \\leq y} f_X(x)dx \\] where \\(Y = g(X)\\) . Also, if \\(Y = aX+b\\) , then we have \\(f_Y(y) = \\frac{1}{|a|}f_X(\\frac{y - a}{b})\\) . Correlations \\[ Cov(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y] \\] \\[ Var[X + Y] = Var[X] + Var[Y] + 2 Cov[X, Y] \\] \\[ Cor(X, Y) = \\rho(X, Y) = \\frac{Cov(X, Y)}{\\sqrt{var(X)var(Y)}} \\] Law of iterated expectations \\[ E[E[X|Y]] = E[X] \\] Law of total variance \\[ Var[X] = E[Var[X|Y]] + Var[E[X|Y]] \\] Limits of the land If \\(X\\) takes only non-negative values then \\(P(X \\geq a) \\leq E[X]/a\\) . \\(P(|X - \\mu|\\geq c) \\leq \\sigma^2/c^2\\) for all \\(c > 0\\) . According to the idea of convergence \\(\\lim_{n\\to \\infty} P(|X_n - a| \\geq \\epsilon)= 0\\) . Central Limit Theorem Independent sampling of any distribution always yields a mean distribution which is normal. \\[ Z_n = \\frac{X_1 + \\cdots + X_n - n\\mu}{\\sigma\\sqrt n} \\] \\[ \\lim_{n \\to \\infty} P(Z_n \\leq z) = \\phi(z) = \\frac{1}{\\sqrt {2\\pi}} e^{-z^2/2} \\] Law of large numbers If you repeat an experiment independently a large number of times and average the result, what you obtain should be close to the expected value. \\[ P(\\lim \\frac{X_1 + \\cdots + X_n}{n} = \\mu) = 1 \\] Distributions The gaussian distribution shows up in nature a lot because there are many situations in which a lot of small effects sum up to the thing you actually measure. Poisson statistics describe situations where an event occurs randomly but has a constant probability of happening everytime. Discrete Random Distributions Continuous Random Distributions Poisson Distribution To predict the number of events occurring in the future! More formally, to predict the probability of a given number of events occurring in a fixed interval of time. The Poisson Distribution, on the other hand, doesn\u2019t require you to know \\(n\\) or \\(p\\) . We are assuming \\(n\\) is infinitely large and \\(p\\) is infinitesimal. The only parameter of the Poisson distribution is the rate \\(\\lambda\\) (the expected value of \\(x\\) ). In real life, only knowing the rate (i.e., during 2pm~4pm, I received 3 phone calls) is much more common than knowing both \\(n\\) & \\(p\\) . This gives the probability of observing \\(k\\) events in an interval. The average number of events in an interval is designated by \\(\\lambda\\) . \\[ P(X = k) = \\frac{\\lambda^k}{k!}e^{-\\lambda} \\] Even though the Poisson distribution models rare events, the rate \\(\\lambda\\) can be any number. It doesn\u2019t always have to be small. Assumptions: The average rate of events per unit time is constant . Events are independent . If the number of events per unit time follows a Poisson distribution, then the amount of time between events follows the exponential distribution. The Poisson distribution is discrete and the exponential distribution is continuous, yet the two distributions are closely related. Random Shit How do you test whether a data sample is normal or not? https://en.wikipedia.org/wiki/Jarque\u2013Bera_test Optimal theoretical size for a bet is given by the https://en.wikipedia.org/wiki/Kelly_criterion. Random variable X is distributed as N(a, b), and random variable Y is distributed as N(c, d). What is the distribution of (1) \\(X+Y\\) , (2) \\(X-Y\\) , (3) \\(X\\times Y\\) , (4) \\(X/Y\\) ? (1) \\(X+Y \\sim N(a+c, \\sqrt{(b^2 + d^2 + 2\\rho bd)})\\) (2) \\(X+Y \\sim N(a+c, \\sqrt{(b^2 + d^2 - 2\\rho bd)})\\) (3) \\(X\\times Y \\sim N(a\\times c, \\sqrt{(a^2 d + c^2 b + bd)})\\) The Monty Hall Problem","title":"Probability"},{"location":"research/probability/#axioms","text":"\\(P(A) \\geq 0\\) \\(P(A \\cup B) = P(A) + P(B)\\) , where both are disjoint sets \\(P(\\Omega) = 1\\)","title":"Axioms"},{"location":"research/probability/#properties","text":"\\[ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\] \\[ P(A) \\leq P(B), \\text{ if } A \\subseteq B \\]","title":"Properties"},{"location":"research/probability/#conditional-probability","text":"\\[ P(A\\vert B) = \\frac{P(A \\cap B)}{P(B)} \\]","title":"Conditional Probability"},{"location":"research/probability/#total-probability","text":"\\[ P(B) = P(A_1 \\cap B) + \\cdots + P (A_n \\cap B) \\] \\[ = \\sum P(A_i) P(B\\vert A_i) \\] Here, \\(A_i\\) \u2019s are independent.","title":"Total Probability"},{"location":"research/probability/#independence","text":"If \\(A\\) and \\(B\\) are independent, we have the following. \\[ P(A \\vert B) = P(A) \\] \\[ P(A \\cap B) = P(A)P(B) \\]","title":"Independence"},{"location":"research/probability/#conditional-independence","text":"\\[ P(A \\cap B \\vert C) = P(A \\vert C) P(B \\vert C) \\]","title":"Conditional Independence"},{"location":"research/probability/#expectation-and-variance","text":"\\[ E[g(X)] = \\sum_x g(x)p_X(x) \\] \\[ Var[X] = E[(X - E[X])^2] \\] \\[ Var[X] = E[X^2] - E[X]^2 \\]","title":"Expectation and Variance"},{"location":"research/probability/#linearity","text":"\\[ E[aX + b] = aE[X] + b \\] \\[ Var[aX + b] = a^2\\ Var[X] \\] \\[ E[aX + bY + c] = aE[X] + bE[Y] + c \\]","title":"Linearity"},{"location":"research/probability/#probability-mass-functions","text":"\\[ p_{X,Y}(x, y) = P(X=x, Y=y) \\]","title":"Probability Mass Functions"},{"location":"research/probability/#marginal-pmfs","text":"\\[ p_X(x) = \\sum_y p_{{X, Y}}(x, y) \\] \\[ E[g(X, Y)] = \\sum_x \\sum_y g(x, y)p_{X, Y} (x, y) \\]","title":"Marginal PMFs"},{"location":"research/probability/#conditionals","text":"\\[ p_{X|A}(x) = P(X = x | A), \\text{ such that } \\sum_x p_{X|A}(x) = 1 \\] \\[ p_X(x) = \\sum P(A_i) p_{x|A_i}(x) \\] \\[ p_{X, Y}(x, y) = p_Y(y)p_{X|Y}(x|y) \\] \\[ p_X = \\sum_y p_Y(y)p_{X|Y}(x|y) \\] \\[ E[g(X)|A] = \\sum_x g(x)p_{X|A}(x) \\] \\[ E[g(X)|Y=y] = \\sum_x g(x)p_{X|Y}(x|y) \\] \\[ E[X] = \\sum P(A_i)E[x|A_i] \\]","title":"Conditionals"},{"location":"research/probability/#independence-strikes-back","text":"\\(p_{X|A}(x) = p_X(x)\\) \\(E[XY]=E[X]E[Y]\\) \\(Var[X + Y] = Var[X] + Var[Y]\\) \\(p_{X, Y}(x, y) = p_X(x) p_Y(y)\\) , \\(\\forall\\) \\(x \\in X\\) and \\(y \\in Y\\)","title":"Independence strikes back"},{"location":"research/probability/#continuity-and-lovely-curves","text":"\\[ P(X\\in B)= \\int_B f_X(x)dx, \\text{ where } f_X(x) \\geq 0 \\] \\[ E[g(X)] = \\int_{-\\infty}^{\\infty} g(x)f_X(x)dx \\]","title":"Continuity and Lovely Curves"},{"location":"research/probability/#cumulative-distributions","text":"\\[ F_X(x) = P(X \\leq x) \\] \\[ F_X(x) = \\int_{-\\infty}^x f_X(x)dx \\]","title":"Cumulative Distributions"},{"location":"research/probability/#conditionals_1","text":"\\[ \\int_B f_{X|A}(x)dx = P(X \\in B|A) \\] \\[ f_X(x) = \\sum P(A_i) f_{x|A_i}(x) \\] \\[ f_{X, Y}(x, y) = f_Y(y)f_{X|Y}(x|y) \\] \\[ f_X = \\int_{-\\infty}^{\\infty} f_Y(y)f_{X|Y}(x|y) \\]","title":"Conditionals"},{"location":"research/probability/#conditional-expectation","text":"\\[ E[g(X)|A] = \\int_{\\Omega} g(x)f_{X|A}(x) \\] \\[ E[g(X)|Y=y] = \\int_\\Omega g(x)f_{X|Y}(x|y) \\] \\[ E[X] = \\sum P(A_i)E[x|A_i] \\]","title":"Conditional Expectation"},{"location":"research/probability/#bayes-theorem","text":"You can of course interchange \\(p\\) with \\(f\\) to account for continuous random variables. \\[ p_X(x)p_{Y|X}(y|x) = p_Y(y)p_{X|Y}(x|Y) \\]","title":"Bayes' Theorem"},{"location":"research/probability/#gimme-more","text":"\\[ F_Y(y) = P(g(X)\\leq y) = \\int_{x | g(x) \\leq y} f_X(x)dx \\] where \\(Y = g(X)\\) . Also, if \\(Y = aX+b\\) , then we have \\(f_Y(y) = \\frac{1}{|a|}f_X(\\frac{y - a}{b})\\) .","title":"Gimme More"},{"location":"research/probability/#correlations","text":"\\[ Cov(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y] \\] \\[ Var[X + Y] = Var[X] + Var[Y] + 2 Cov[X, Y] \\] \\[ Cor(X, Y) = \\rho(X, Y) = \\frac{Cov(X, Y)}{\\sqrt{var(X)var(Y)}} \\]","title":"Correlations"},{"location":"research/probability/#law-of-iterated-expectations","text":"\\[ E[E[X|Y]] = E[X] \\]","title":"Law of iterated expectations"},{"location":"research/probability/#law-of-total-variance","text":"\\[ Var[X] = E[Var[X|Y]] + Var[E[X|Y]] \\]","title":"Law of total variance"},{"location":"research/probability/#limits-of-the-land","text":"If \\(X\\) takes only non-negative values then \\(P(X \\geq a) \\leq E[X]/a\\) . \\(P(|X - \\mu|\\geq c) \\leq \\sigma^2/c^2\\) for all \\(c > 0\\) . According to the idea of convergence \\(\\lim_{n\\to \\infty} P(|X_n - a| \\geq \\epsilon)= 0\\) .","title":"Limits of the land"},{"location":"research/probability/#central-limit-theorem","text":"Independent sampling of any distribution always yields a mean distribution which is normal. \\[ Z_n = \\frac{X_1 + \\cdots + X_n - n\\mu}{\\sigma\\sqrt n} \\] \\[ \\lim_{n \\to \\infty} P(Z_n \\leq z) = \\phi(z) = \\frac{1}{\\sqrt {2\\pi}} e^{-z^2/2} \\]","title":"Central Limit Theorem"},{"location":"research/probability/#law-of-large-numbers","text":"If you repeat an experiment independently a large number of times and average the result, what you obtain should be close to the expected value. \\[ P(\\lim \\frac{X_1 + \\cdots + X_n}{n} = \\mu) = 1 \\]","title":"Law of large numbers"},{"location":"research/probability/#distributions","text":"The gaussian distribution shows up in nature a lot because there are many situations in which a lot of small effects sum up to the thing you actually measure. Poisson statistics describe situations where an event occurs randomly but has a constant probability of happening everytime.","title":"Distributions"},{"location":"research/probability/#discrete-random-distributions","text":"","title":"Discrete Random Distributions"},{"location":"research/probability/#continuous-random-distributions","text":"","title":"Continuous Random Distributions"},{"location":"research/probability/#poisson-distribution","text":"To predict the number of events occurring in the future! More formally, to predict the probability of a given number of events occurring in a fixed interval of time. The Poisson Distribution, on the other hand, doesn\u2019t require you to know \\(n\\) or \\(p\\) . We are assuming \\(n\\) is infinitely large and \\(p\\) is infinitesimal. The only parameter of the Poisson distribution is the rate \\(\\lambda\\) (the expected value of \\(x\\) ). In real life, only knowing the rate (i.e., during 2pm~4pm, I received 3 phone calls) is much more common than knowing both \\(n\\) & \\(p\\) . This gives the probability of observing \\(k\\) events in an interval. The average number of events in an interval is designated by \\(\\lambda\\) . \\[ P(X = k) = \\frac{\\lambda^k}{k!}e^{-\\lambda} \\] Even though the Poisson distribution models rare events, the rate \\(\\lambda\\) can be any number. It doesn\u2019t always have to be small. Assumptions: The average rate of events per unit time is constant . Events are independent . If the number of events per unit time follows a Poisson distribution, then the amount of time between events follows the exponential distribution. The Poisson distribution is discrete and the exponential distribution is continuous, yet the two distributions are closely related.","title":"Poisson Distribution"},{"location":"research/probability/#random-shit","text":"How do you test whether a data sample is normal or not? https://en.wikipedia.org/wiki/Jarque\u2013Bera_test Optimal theoretical size for a bet is given by the https://en.wikipedia.org/wiki/Kelly_criterion. Random variable X is distributed as N(a, b), and random variable Y is distributed as N(c, d). What is the distribution of (1) \\(X+Y\\) , (2) \\(X-Y\\) , (3) \\(X\\times Y\\) , (4) \\(X/Y\\) ? (1) \\(X+Y \\sim N(a+c, \\sqrt{(b^2 + d^2 + 2\\rho bd)})\\) (2) \\(X+Y \\sim N(a+c, \\sqrt{(b^2 + d^2 - 2\\rho bd)})\\) (3) \\(X\\times Y \\sim N(a\\times c, \\sqrt{(a^2 d + c^2 b + bd)})\\) The Monty Hall Problem","title":"Random Shit"},{"location":"research/prog-lang/","text":"So what are we actually studying? Logic and Proof Theory Type Theory Category Theory What's the target? Well there are two. The first one is to have enough understanding as to begin learning homotopy type theory. The second one is to be able to reason about programs and algorithms, get better at using or constructing features of languages and effectively work on theorem proving (classical and AI).","title":"Programming Language Theory"},{"location":"research/psdp/","text":"Definitions \\[ PSDP: \\min_{P\\in S^n_{\\geq}} ||F - PG|| \\] \\[ SP: \\min_{S\\in S^n} ||F - SG|| \\] Here, \\(F, G \\in R^{\\ n\\times m}\\) . Background \\(\\tilde{S}\\) denotes a minimizer for the SP problem. \\(\\tilde{P}\\) denotes a minimizer for the PSDP problem. \\(\\tilde{S}GG'+ GG'\\tilde{S} = FG' + GF' = Q\\) When \\(rank(G) = n\\) , a sufficient condition for \\(\\tilde{S}\\) to be in \\(S^n_{\\geq}\\) is that \\(Q \\in S^n_{\\geq}\\) . A sufficient condition for \\(\\tilde{S}\\) to be in \\(S^n_{\\geq}\\) is that \\(FG' \\in S^m_{\\geq}\\) . \\(rank(G) = n\\) is also necessary and sufficient for uniqueness of minimizers in case of PSDP. Previous Approches The objective function and the feasible set in case of PSDP are both convex. We can use convex optimization which directly exploits the convexity of \\(S^n_{\\geq}\\) . Solutions involve finite convergence towards global minimas bringing the approximate minimizer arbitrarily close to the unique global minimizer. Approach of this paper \\[ PSDP^*: \\min_{E \\in R^{n\\times n}} ||F - E'EG|| \\] \\[ \\tilde{P}_{\\text{global}} = \\tilde{E}_{\\text{local}}'\\tilde{E}_{\\text{local}} \\] Algorithm ideas: Steepest descent (doesn\u2019t converge, proven) Newton\u2019s algorithm (modified, superior than known convex programs of that time) Where do we obtain the data? Take stuff, perturb by errors to make sure that \\(\\tilde{S} \\neq \\tilde{P}\\) . Newton\u2019s method: \\[ x_{k + 1} = x_k - \\frac{f'(x_k)}{f''(x_k)} \\] Has quadratic convergence. Can find complex zeroes. If you have \\(n\\) variables, then \\(n^2\\) operations is needed to find \\(\\nabla^2 f\\) (Hessian) and \\(n^3\\) operations to find the inverse of the Hessian. Quasi Newtown method: \\[ GD:\\ \\ \\ \\ x_{k + 1} = x_k - \\eta \\cdot I\\cdot \\nabla f(x_k) \\] \\[ NM:\\ \\ \\ \\ x_{k + 1} = x_k - \\eta \\cdot \\frac{1}{\\nabla^{2}f(x_k)}\\cdot \\nabla f(x_k) \\] \\[ QNM:\\ \\ \\ \\ x_{k + 1} = x_k - \\eta \\cdot H_k\\cdot \\nabla f(x_k) \\] A good choice for \\(H\\) can be the following. This is the BFGS method and \\(H_k\\) holds curvature information. Key idea is to match curvature along trajectory. \\[ H = \\begin{bmatrix} 1& 1\\\\ \\end{bmatrix} \\]","title":"PSDP Problem"},{"location":"research/psdp/#definitions","text":"\\[ PSDP: \\min_{P\\in S^n_{\\geq}} ||F - PG|| \\] \\[ SP: \\min_{S\\in S^n} ||F - SG|| \\] Here, \\(F, G \\in R^{\\ n\\times m}\\) .","title":"Definitions"},{"location":"research/psdp/#background","text":"\\(\\tilde{S}\\) denotes a minimizer for the SP problem. \\(\\tilde{P}\\) denotes a minimizer for the PSDP problem. \\(\\tilde{S}GG'+ GG'\\tilde{S} = FG' + GF' = Q\\) When \\(rank(G) = n\\) , a sufficient condition for \\(\\tilde{S}\\) to be in \\(S^n_{\\geq}\\) is that \\(Q \\in S^n_{\\geq}\\) . A sufficient condition for \\(\\tilde{S}\\) to be in \\(S^n_{\\geq}\\) is that \\(FG' \\in S^m_{\\geq}\\) . \\(rank(G) = n\\) is also necessary and sufficient for uniqueness of minimizers in case of PSDP.","title":"Background"},{"location":"research/psdp/#previous-approches","text":"The objective function and the feasible set in case of PSDP are both convex. We can use convex optimization which directly exploits the convexity of \\(S^n_{\\geq}\\) . Solutions involve finite convergence towards global minimas bringing the approximate minimizer arbitrarily close to the unique global minimizer.","title":"Previous Approches"},{"location":"research/psdp/#approach-of-this-paper","text":"\\[ PSDP^*: \\min_{E \\in R^{n\\times n}} ||F - E'EG|| \\] \\[ \\tilde{P}_{\\text{global}} = \\tilde{E}_{\\text{local}}'\\tilde{E}_{\\text{local}} \\] Algorithm ideas: Steepest descent (doesn\u2019t converge, proven) Newton\u2019s algorithm (modified, superior than known convex programs of that time) Where do we obtain the data? Take stuff, perturb by errors to make sure that \\(\\tilde{S} \\neq \\tilde{P}\\) . Newton\u2019s method: \\[ x_{k + 1} = x_k - \\frac{f'(x_k)}{f''(x_k)} \\] Has quadratic convergence. Can find complex zeroes. If you have \\(n\\) variables, then \\(n^2\\) operations is needed to find \\(\\nabla^2 f\\) (Hessian) and \\(n^3\\) operations to find the inverse of the Hessian. Quasi Newtown method: \\[ GD:\\ \\ \\ \\ x_{k + 1} = x_k - \\eta \\cdot I\\cdot \\nabla f(x_k) \\] \\[ NM:\\ \\ \\ \\ x_{k + 1} = x_k - \\eta \\cdot \\frac{1}{\\nabla^{2}f(x_k)}\\cdot \\nabla f(x_k) \\] \\[ QNM:\\ \\ \\ \\ x_{k + 1} = x_k - \\eta \\cdot H_k\\cdot \\nabla f(x_k) \\] A good choice for \\(H\\) can be the following. This is the BFGS method and \\(H_k\\) holds curvature information. Key idea is to match curvature along trajectory. \\[ H = \\begin{bmatrix} 1& 1\\\\ \\end{bmatrix} \\]","title":"Approach of this paper"},{"location":"research/quantum-channels/","text":"Introduction Quantum Channels are generalizations of Quantum Operations. Since, in general our operations maybe noisy (inherently) and the system we are dealing with maybe open so Quantum Channels us the study of noisy open Quantum Operations. \\[ \\rho \\xrightarrow{U}\\rho' \\] The above only holds for closed quantum systems. Thus, in case of noisy open quantum systems we have the following. \\[ \\rho \\xrightarrow{\\mathcal E} \\rho' \\] Here, \\(\\mathcal E\\) is a combination of: unitaries \\((U)\\) adding systems \\((\\rho \\to \\rho \\otimes \\sigma)\\) subtracting systems or partial tracing \\((\\rho_{AB} \\to \\text{tr}_B \\rho_{AB} = \\rho_A)\\) Review of Density Matrices Moreover, to deal with noisy open systems we also need to review density matrices which are the general way of representing noisy quantum states. \\[ \\rho \\equiv \\text{noisy quantum state} \\] \\[ \\mathcal E \\equiv \\text{noisy open quantum channel} \\] Now since, \\(\\rho\\) is positive semi-definite thus, \\(\\frac{1 + ||\\vec{a}||}{2} \\geq 0 \\implies ||\\vec{a}|| \\leq 1\\) . This results in the Bloch Ball representation for general mixed state qubits, which serves as a generalization of the Bloch Sphere that serves as representation for pure state qubits. Pure state qubits have eigenvalues of \\(0\\) or \\(1\\) , \\(\\lambda(\\rho) = 0\\ \\text{or}\\ 1\\) . The eigenvalue of the maximally mixed state is \\({I}/{2}\\) where the state is actually given by the following. \\[ \\frac{1}{2}(|0 \\rangle\\langle 0| + |1 \\rangle\\langle 1|) \\equiv \\frac{1}{2}(|+ \\rangle\\langle +| + |- \\rangle\\langle -|) \\] Quantum Operations However, in both these above cases, we are still dealing with closed (albeit noisy ) for the random case. Generalized Quantum Operations We can refer to Generalized Quantum Operations as Quantum Channels ( noisy and open ) whose definitions we shall rediscover and formalize here. Now, it should be noted that there are three major equivalent formalisms for Quantum Channels: Operational (Steinspring) Mathemtically Simplified (Kraus) Axiomatic (TPCP maps) Introduction to Steinspring Representation For closed systems we have a probabilistic combinations of the above (noisy closed channels). \\[ |\\psi\\rangle \\to U|\\psi\\rangle \\] \\[ |\\psi\\rangle\\langle\\psi| \\to U |\\psi\\rangle\\langle\\psi|U^\\dagger \\] \\[ \\rho \\to U\\rho\\ U^\\dagger \\] This can be generalized for noisy open channels as combinations of unitaries \\((U)\\) , adding systems together \\((\\rho \\to \\rho \\otimes \\sigma)\\) and subtracting systems \\((\\rho_{AB} \\to \\text{tr}_B(\\rho_{AB}) = \\rho_A)\\) . But why is partial tracing allowed? Doesn't this \"delete\" information? The information has basically flown out of your lab to the heavens (but still exists in nature) and in most cases, you simply can't access it anymore, kinda like your ex-girlfriend. \\[ \\text{tr}(\\rho_{AB}) = \\sum_b (I_A \\otimes \\langle b|)\\ \\rho\\ (I_A \\otimes |b\\rangle) = (I_A \\otimes \\text{tr}_B)(\\rho_{AB}) \\] Isometry An operator \\(V \\in \\mathcal{L}(X),\\ V:{X \\to Y}\\) is called an isometry if and only if \\(||V\\vec{v}|| = ||\\vec{v}||\\) for all \\(\\vec{v} \\in {X}\\) . In other words, an isometry is a genralization of norm preserving operators. Now, properties of linear isometries: \\(\\langle v|v\\rangle = \\langle Vv|Vv\\rangle = \\langle v|V^\\dagger V|v\\rangle\\) Thus, \\(V^\\dagger V = I_{{X}}\\) . Linear isometries are not always unitary operators, though, as those require additionally that \\(X = Y\\) and \\(V V^\\dagger = I_X\\) . By the Mazur\u2013Ulam theorem, any isometry of normed vector spaces over \\(\\mathbb{R}\\) is affine. Affine transformations are those that preserve lines and parallelism (but not necessarily distances and angles). Steinspring Representation Isometries allow us to restate the requirements of a noisy open quantum channel. Now, it can be formed by combinations of isometries and partial trace. Moreover, the purpose of the partial trace is to trash the environment shit out of my lab. Thus, we have the following formalism. \\[ V:A\\to B\\otimes E \\] \\[ \\mathcal E : \\mathcal L(A) \\to \\mathcal L(B) \\] \\[ \\mathcal E(\\rho) = \\text{tr}_E(V\\rho\\ V^\\dagger) \\] Partial tracing can be delayed (deferred tracing), an idea similar to deferred measurements. Also, from a philosophical standpoint (Church of the Larger Hilbert Space) you can defer it forever. Now, the above circuit is equivalent to the below circuit. As a result, inductively, combinations of isometries and partial trace is equivalent to the \\(\\mathcal E\\) operator. Kraus Operator Picture We can fix a orthonormal bases \\(\\{|e\\rangle\\}\\) for \\(E\\) , and have \\(\\{V_e\\}\\) such that \\[ V = \\sum_e V_e \\otimes |e\\rangle \\] where \\(V_e \\in \\mathcal L(A), V_e: A \\to B\\) and furthermore \\[ I = V^\\dagger V = (\\sum_{e_1} V^\\dagger_{e_1} \\otimes \\langle e_1|)(\\sum_{e_2} V_{e_2} \\otimes |e_2\\rangle) \\] such that \\(V_e \\neq V_e^\\dagger\\) in general and thus, we have the Kraus operator condition (stated below). \\[ I = \\sum_e V_e^\\dagger V_e \\] Thus, we can have a equivalent and simplified formalism of the Steinspring Operator \\((\\mathcal E)\\) , where \\(\\{V_e\\}\\) are Kraus operators. \\[ \\mathcal E(\\rho) = \\text{tr}_E (V \\rho\\ V^\\dagger) = \\text{tr}_E (\\sum_{e_1, e_2} V_{e_1} \\rho\\ V_{e_2}^\\dagger \\otimes |e_1\\rangle\\langle e_2|) \\] \\[ \\implies \\mathcal E(\\rho) = \\sum_e V_e\\rho\\ V_e^\\dagger \\] Conversely, given \\(\\{V_e\\}\\) satisfying Kraus operator conditions, \\(\\mathcal E(\\rho) = \\sum_e {V_e \\rho\\ V_e^\\dagger}\\) is a quantum operator with \\(V = \\sum_e V_e \\otimes |e\\rangle\\) as the isometry involved in the transformation. Examples of Kraus Operators for Quantum Channels \\(\\mathcal E(\\rho) = U \\rho\\ U^\\dagger \\rightarrow\\) single Kraus operator \\(\\{U\\}\\) \\(\\mathcal E(\\rho) = \\sum_e p_e U_e\\ \\rho\\ U_e^\\dagger \\rightarrow\\) Kraus operators \\(\\{\\sqrt{p_e} U_e\\}\\) \\(\\text{tr}(\\rho_{AB}) = \\sum_b (I_A \\otimes \\langle b|)\\ \\rho\\ (I_A \\otimes |b\\rangle)\\) where \\(V_b = (I_A \\otimes \\langle b |_B)\\) thus we also have the following holding true, \\(\\sum_b V_b^\\dagger V_b = \\sum_b I_A \\otimes |b\\rangle\\langle b| = I_A \\otimes I_B = I\\) . Ultimate Refrigerator: \\(\\mathcal E (\\rho) = |0\\rangle\\langle 0|\\) , regardless of the initial state \\(\\rho\\) . \\[ V_0 = |0\\rangle\\langle 0|, V_1 = |0\\rangle\\langle 1| \\] \\[ V_0 \\rho\\ V_0^\\dagger + V_1 \\rho\\ V_1^\\dagger = |0\\rangle\\langle 0| \\] Depolarizing Channel: \\(\\mathcal E (\\rho) = (1 - p)\\rho + \\rho ({I}/{2})\\) \\(\\rightarrow\\) simple kind of white noise effect Amplitude Damping Channel: \\(\\mathcal E_\\gamma (\\rho) = K_0 \\rho\\ K_0^\\dagger + K_1 \\rho\\ K_1^\\dagger\\) where we have the following \\[ K_0 = \\begin{bmatrix} 1 & 0 \\\\ 0 & \\sqrt{1 - \\gamma} \\\\ \\end{bmatrix} \\text{ and, }\\ K_1 = \\begin{bmatrix} 0 & \\sqrt{\\gamma} \\\\ 0 & 0 \\\\ \\end{bmatrix} \\] Axiomatic Definitions By God, properties of \\(\\mathcal E\\) should be as follows: Linear (assume this or die, basically) and Hermitian preserving Trace preserving (TP) such that \\(\\text{tr}(\\mathcal E(\\rho)) = \\text{tr}(\\rho) = 1\\) Completely positive (CP) since \\(\\rho \\geq 0 \\implies (\\mathcal E \\otimes I)(\\rho) \\geq 0\\) But why completely positive? Well, because we are dealing with isometries of form \\(A \\to B \\otimes E\\) . Here, completely positive means that if \\(\\mathcal E\\) is applied on a subsystem, then the complete system and the subsystems must remain positive. Equivalence of TPCP maps and Kraus operators Equivalence Theorem: If \\(\\mathcal E(\\rho) = \\sum_e V_e\\rho\\ V_e^\\dagger\\) then \\(\\mathcal E\\) is TPCP and converse holds true as well. This implies that the map \\(N\u2192J(N)\\) is injective. In fact it is known as the Choi-Jamio\u0142kowski isomorphism \u2014 a correspondence between quantum channels and quantum states (described by density matrices). Equivalence of Operators: Given \\(2\\) operators \\(\\mathcal E\\) and \\(\\mathcal F\\) with operator elements \\(\\{\\mathcal E_i\\}\\) and \\(\\{\\mathcal F_i\\}\\) respectively, if \\(\\mathcal E = \\mathcal F\\) then \\[ \\mathcal E_i = \\sum_j u_{ij} \\mathcal F_j \\] Measurements as Quantum Operations Measurement can be thought of as a quantum operation where the input is any quantum state and the output is classical, \\(\\mathcal E(\\rho) = \\sum_x p_x|x\\rangle\\langle x|\\) (diagonal density matrix). The probabilities, \\(p_x\\) , should depend on the state. Further, \\(p_x\\) should be a linear function of the density matrix \\(p_x = \\text{tr}(M_x \\rho)\\) . From this we can work out the properties that \\(M_x\\) should obey: Normalization: \\(\\text{tr}(\\mathcal E(\\rho)) = \\text{tr}(p_x) = tr(M_x\\rho) = 1 \\implies \\sum M_x = I\\) Positive Semi-definite: \\((\\mathcal E \\otimes I)(\\rho) \\geq 0 \\implies p_x \\geq 0\\) thus \\(\\text{tr}(M_x \\rho) \\geq 0, \\forall\\ M_x\\) over \\(\\rho\\) thus, we have \\(M_x \\geq 0 \\implies M_x's\\) are positive semi-definite \\(\\implies M_x = E_x E_x^\\dagger\\) Thus, we have the following: \\[ \\sum_x M_x = I \\] \\[ M_x \\geq 0 \\] These conditions still leave room for noisy measurements, etc. We can also talk about non-demolition measurements, which do not discard the quantum systems aftermeasurements. Consider the following quantum channel. \\[ \\mathcal E(\\rho) =\\sum_e V_e \\rho V^\\dagger_e \\otimes |e\\rangle\\langle e| = \\sum_e \\frac{V_e \\rho V^\\dagger_e} {\\text{tr}(V_e\\rho V^\u2020_e)} \\otimes \\text{tr}(V_e \\rho V^\\dagger_e)|e\\rangle\\langle e| \\] This channel has the following interpretation: with probability as \\[ p_e = \\text{tr}(V_e \\rho V_e^\\dagger) = \\text{tr}(V_e V_e^\\dagger\\rho) = \\text{tr}(M_e\\rho) \\] the state of the system after the application of this channel is \\(\\rho_e = {(V_e \\rho V^\\dagger_e)}/{\\text{tr}(V_e\\rho V^\u2020_e)}\\) . This is similar to having a measurement that outputs the post measured state \\(\\rho_e\\) with probability \\(p_e\\) . Quantum Norms and Distance Metrics Motivation: Now that we have defined channels and states of information, how do you differentiate between two items of information? What does it mean to say that information is preserved by some process? Well for these questions it is necessary to develop distance measures. There is a certain arbitrariness in the way distance measures are defined, both classically and quantum mechanically, and the community of people studying quantum computation and quantum information has found it convenient to use a variety of distance measures over the years. Two of those measures, the trace distance and the fidelity, have particularly wide currency today. Norms Norm is a distance metric such that \\(|cv| = |c|.|v|\\) \\(|v+w| \\leq |v| + |w|\\) \\(|v| = 0\\) iff \\(v = 0\\) Examples of norms are as follows Manhattan ( \\(L_1\\) ) Euclidean ( \\(L_2\\) ) \\(L_p\\) norm \\(\\to |v|_{L_p} = (\\sum v_i^p)^{1/p}\\) Schatten \\(p\\) -norms \\(L_1\\) norm corresponds to probability distributions \\(L_2\\) norm corresponds to pure states \\(L_\\infty = \\max_i {|v_i|}\\) Similarly, schatten \\(p\\) -norm \\(:\\) \\(|M|_{S_p} = |\\sigma(M)|_{S_p}\\) where \\(\\sigma(M)\\) is a vector of singular values of a matrix, say \\(M\\) . \\[ S_1 = \\sum \\text{singular values} \\] \\[ S_2 = \\sqrt {\\sum \\text{singular values}^2} \\] \\[ S_\\infty = \\max_i |\\text{singular values}| \\] In case of density matrices (in general to positive semi-definite Hermitian operators), we have \\(\\text{singular values = eigen values}\\) thus, \\(\\sigma(M) = \\lambda(M)\\) . \\[ |X|_{S_p} = |\\sigma(X)|_{L_p} = (\\sum_i \\sigma_i^p)^{1/p} \\] If \\(X \\geq 0\\) , then \\(\\sigma(X) = \\lambda(X)\\) then, \\(|X|_{S_1} = tr X \\implies |\\rho|_{S_1} {= \\text{tr} (\\rho)}\\) . Thus, \\(\\rho\\) is a density matrix \\(\\iff \\rho \\geq 0\\) and \\(|\\rho|_{S_1} = 1\\) . Measurements Let us consider a simple system with \\(\\{M, I-M\\}\\) as measurement operators. Then, we have \\[ M \\geq 0, I - M \\geq 0 \\iff M \\leq I \\] \\[ \\iff 0 \\leq M \\leq I \\] \\[ \\iff |M|_{S_\\infty} \\leq 1 \\] But why is it that two objects with different norms can co-exist in the same operator framework? The answer lies in the notion of duality. Duality Given a norm \\(|.|\\) there exists a dual norm such that, \\(|x|_* = \\max_{|y| \\leq 1} |\\langle x,y\\rangle|\\) . \\(L_2\\) is dual to \\(L_2\\) \\(L_1\\) and \\(L_\\infty\\) are dual to each other \\(S_2\\) is dual to \\(S_2\\) \\(S_1\\) and \\(S_\\infty\\) are dual to each other Introduction to Trace Norm \\[ T(\\rho, \\sigma) = \\frac{1}{2}|\\rho - \\sigma|_{S_1} = \\frac{1}{2}\\ \\text{tr}\\ |\\rho - \\sigma| \\] Then, \\(\\forall M\\) where \\(M\\) denotes measurement, \\[ |\\text{tr}(M\\rho) - \\text{tr}(M\\sigma)| = |\\text{tr}(M (\\rho - \\sigma))| \\leq |M|_{S_\\infty} |\\rho -\\sigma|_{S_1} \\leq |\\rho -\\sigma|_{S_1} = 2T(\\rho, \\sigma) \\] In fact, we can tighten this inequality even further to obtain \\[ |\\text{tr}(M\\rho) - \\text{tr}(M\\sigma)| \\leq T(\\rho, \\sigma) \\] \\[ \\Big\\downarrow \\] \\[ T(\\rho, \\sigma) = \\max_M |\\text{tr}\\ M(\\rho-\\sigma)| \\] \\[ \\Big\\downarrow \\] \\[ T(\\mathcal E(\\rho) - \\mathcal E(\\sigma)) \\leq \\max_M |\\text{tr}\\ M(\\rho-\\sigma| = T(\\rho, \\sigma) \\] and finally we get the required inequality \\[ T(\\mathcal E(\\rho), \\mathcal E(\\sigma)) \\leq T(\\rho, \\sigma) \\] Thus, I can never increase the distance of two states, no matter what. So, basically when you wanna protect against noise all we are doing is slowing down the rate of noise (indistinguishable nonsense). Choi's Theorem Let \\(\\mathcal E :\\mathbb {C} ^{n\\times n}\\to \\mathbb {C} ^{m\\times m}\\) be a linear map. Then, the following are equivalent: \\(\\mathcal E\\) is \\(n\\) -positive (i.e. \\(\\mathcal E(A)\\in \\mathbb {C} ^{m\\times m}\\) is positive whenever \\(A\\in \\mathbb {C} ^{n\\times n}\\) is positive). The matrix \\(\\Gamma_\\mathcal E\\) , sometimes called the Choi matrix, is positive. Here the state \\(\\phi\\) is maximally entangled. \\[ \\Gamma_{\\mathcal E}=\\left(\\operatorname {id} _{n}\\otimes \\mathcal E \\right)\\left(\\vert \\phi \\rangle\\langle \\phi \\vert\\right) \\] \\(\\mathcal E\\) is completely positive. Solving a Lindblad Equation Any ideal open quantum system will undergo Markovian dynamics provided that its evolution satisfies a Master equation. \\[ \\frac{d\\rho}{dt} = \\mathcal L(\\rho) = -i[H, \\rho] + \\sum_k \\gamma_k (V_k\\rho V_k^\\dagger - \\frac{1}{2} \\{V_k^\\dagger V_k, \\rho\\}) \\] Let \\(\\rho = \\frac{1}{2}(I + \\vec r \\cdot \\vec \\sigma)\\) and differentiate \\(\\rho(t)\\) to obtain \\(\\dot \\rho(t)\\) in terms of \\(\\frac{d\\vec{r}}{dt}\\) . \\[ \\frac{d\\rho}{dt} = \\frac 1 2 (\\dot r_x\\sigma_x + \\dot r_y\\sigma_y + \\dot r_z\\sigma_z) \\] We equate \\(\\dot \\rho = \\mathcal L(\\rho)\\) using the above representation for \\(\\dot \\rho\\) and \\(\\rho\\) and solve the equation by solving the individual differential equations obtained. After we solve for \\(\\rho\\) , we obtain \\(\\Lambda_t\\) . \\[ \\Lambda_t: \\rho_0 \\to \\rho_t \\text{,~~~~~} \\rho(t) = \\Lambda_t(\\rho(0)) \\] Now, with Choi's theorem we check if \\(\\Lambda_t\\) is \\(CP\\) by checking whether \\(\\Gamma_\\Lambda\\) is positive semi-definite. \\[ \\Gamma_\\Lambda = C\\vert\\psi\\rangle\\langle\\psi\\vert \\] \\[ C = (I \\otimes \\Lambda_t) \\] Now, we find the eigenvalues and eigenvectors of \\(C\\) and represent it as \\(C = \\sum_i \\lambda_i\\vert v_i\\rangle\\) . Then, we can obtain Kraus operators \\(\\{K_i\\}_{\\forall i}\\) such as follows (check below). \\[ v_{i} = \\begin{bmatrix} -1/2\\\\1/2\\\\-1/2\\\\1/2 \\end{bmatrix} \\implies K_i = \\sqrt{\\lambda_i} \\begin{bmatrix} -1/2 & -1/2\\\\1/2 & 1/2 \\end{bmatrix} \\]","title":"Quantum Channels"},{"location":"research/quantum-channels/#introduction","text":"Quantum Channels are generalizations of Quantum Operations. Since, in general our operations maybe noisy (inherently) and the system we are dealing with maybe open so Quantum Channels us the study of noisy open Quantum Operations. \\[ \\rho \\xrightarrow{U}\\rho' \\] The above only holds for closed quantum systems. Thus, in case of noisy open quantum systems we have the following. \\[ \\rho \\xrightarrow{\\mathcal E} \\rho' \\] Here, \\(\\mathcal E\\) is a combination of: unitaries \\((U)\\) adding systems \\((\\rho \\to \\rho \\otimes \\sigma)\\) subtracting systems or partial tracing \\((\\rho_{AB} \\to \\text{tr}_B \\rho_{AB} = \\rho_A)\\)","title":"Introduction"},{"location":"research/quantum-channels/#review-of-density-matrices","text":"Moreover, to deal with noisy open systems we also need to review density matrices which are the general way of representing noisy quantum states. \\[ \\rho \\equiv \\text{noisy quantum state} \\] \\[ \\mathcal E \\equiv \\text{noisy open quantum channel} \\] Now since, \\(\\rho\\) is positive semi-definite thus, \\(\\frac{1 + ||\\vec{a}||}{2} \\geq 0 \\implies ||\\vec{a}|| \\leq 1\\) . This results in the Bloch Ball representation for general mixed state qubits, which serves as a generalization of the Bloch Sphere that serves as representation for pure state qubits. Pure state qubits have eigenvalues of \\(0\\) or \\(1\\) , \\(\\lambda(\\rho) = 0\\ \\text{or}\\ 1\\) . The eigenvalue of the maximally mixed state is \\({I}/{2}\\) where the state is actually given by the following. \\[ \\frac{1}{2}(|0 \\rangle\\langle 0| + |1 \\rangle\\langle 1|) \\equiv \\frac{1}{2}(|+ \\rangle\\langle +| + |- \\rangle\\langle -|) \\]","title":"Review of Density Matrices"},{"location":"research/quantum-channels/#quantum-operations","text":"However, in both these above cases, we are still dealing with closed (albeit noisy ) for the random case.","title":"Quantum Operations"},{"location":"research/quantum-channels/#generalized-quantum-operations","text":"We can refer to Generalized Quantum Operations as Quantum Channels ( noisy and open ) whose definitions we shall rediscover and formalize here. Now, it should be noted that there are three major equivalent formalisms for Quantum Channels: Operational (Steinspring) Mathemtically Simplified (Kraus) Axiomatic (TPCP maps)","title":"Generalized Quantum Operations"},{"location":"research/quantum-channels/#introduction-to-steinspring-representation","text":"For closed systems we have a probabilistic combinations of the above (noisy closed channels). \\[ |\\psi\\rangle \\to U|\\psi\\rangle \\] \\[ |\\psi\\rangle\\langle\\psi| \\to U |\\psi\\rangle\\langle\\psi|U^\\dagger \\] \\[ \\rho \\to U\\rho\\ U^\\dagger \\] This can be generalized for noisy open channels as combinations of unitaries \\((U)\\) , adding systems together \\((\\rho \\to \\rho \\otimes \\sigma)\\) and subtracting systems \\((\\rho_{AB} \\to \\text{tr}_B(\\rho_{AB}) = \\rho_A)\\) . But why is partial tracing allowed? Doesn't this \"delete\" information? The information has basically flown out of your lab to the heavens (but still exists in nature) and in most cases, you simply can't access it anymore, kinda like your ex-girlfriend. \\[ \\text{tr}(\\rho_{AB}) = \\sum_b (I_A \\otimes \\langle b|)\\ \\rho\\ (I_A \\otimes |b\\rangle) = (I_A \\otimes \\text{tr}_B)(\\rho_{AB}) \\]","title":"Introduction to Steinspring Representation"},{"location":"research/quantum-channels/#isometry","text":"An operator \\(V \\in \\mathcal{L}(X),\\ V:{X \\to Y}\\) is called an isometry if and only if \\(||V\\vec{v}|| = ||\\vec{v}||\\) for all \\(\\vec{v} \\in {X}\\) . In other words, an isometry is a genralization of norm preserving operators. Now, properties of linear isometries: \\(\\langle v|v\\rangle = \\langle Vv|Vv\\rangle = \\langle v|V^\\dagger V|v\\rangle\\) Thus, \\(V^\\dagger V = I_{{X}}\\) . Linear isometries are not always unitary operators, though, as those require additionally that \\(X = Y\\) and \\(V V^\\dagger = I_X\\) . By the Mazur\u2013Ulam theorem, any isometry of normed vector spaces over \\(\\mathbb{R}\\) is affine. Affine transformations are those that preserve lines and parallelism (but not necessarily distances and angles).","title":"Isometry"},{"location":"research/quantum-channels/#steinspring-representation","text":"Isometries allow us to restate the requirements of a noisy open quantum channel. Now, it can be formed by combinations of isometries and partial trace. Moreover, the purpose of the partial trace is to trash the environment shit out of my lab. Thus, we have the following formalism. \\[ V:A\\to B\\otimes E \\] \\[ \\mathcal E : \\mathcal L(A) \\to \\mathcal L(B) \\] \\[ \\mathcal E(\\rho) = \\text{tr}_E(V\\rho\\ V^\\dagger) \\] Partial tracing can be delayed (deferred tracing), an idea similar to deferred measurements. Also, from a philosophical standpoint (Church of the Larger Hilbert Space) you can defer it forever. Now, the above circuit is equivalent to the below circuit. As a result, inductively, combinations of isometries and partial trace is equivalent to the \\(\\mathcal E\\) operator.","title":"Steinspring Representation"},{"location":"research/quantum-channels/#kraus-operator-picture","text":"We can fix a orthonormal bases \\(\\{|e\\rangle\\}\\) for \\(E\\) , and have \\(\\{V_e\\}\\) such that \\[ V = \\sum_e V_e \\otimes |e\\rangle \\] where \\(V_e \\in \\mathcal L(A), V_e: A \\to B\\) and furthermore \\[ I = V^\\dagger V = (\\sum_{e_1} V^\\dagger_{e_1} \\otimes \\langle e_1|)(\\sum_{e_2} V_{e_2} \\otimes |e_2\\rangle) \\] such that \\(V_e \\neq V_e^\\dagger\\) in general and thus, we have the Kraus operator condition (stated below). \\[ I = \\sum_e V_e^\\dagger V_e \\] Thus, we can have a equivalent and simplified formalism of the Steinspring Operator \\((\\mathcal E)\\) , where \\(\\{V_e\\}\\) are Kraus operators. \\[ \\mathcal E(\\rho) = \\text{tr}_E (V \\rho\\ V^\\dagger) = \\text{tr}_E (\\sum_{e_1, e_2} V_{e_1} \\rho\\ V_{e_2}^\\dagger \\otimes |e_1\\rangle\\langle e_2|) \\] \\[ \\implies \\mathcal E(\\rho) = \\sum_e V_e\\rho\\ V_e^\\dagger \\] Conversely, given \\(\\{V_e\\}\\) satisfying Kraus operator conditions, \\(\\mathcal E(\\rho) = \\sum_e {V_e \\rho\\ V_e^\\dagger}\\) is a quantum operator with \\(V = \\sum_e V_e \\otimes |e\\rangle\\) as the isometry involved in the transformation.","title":"Kraus Operator Picture"},{"location":"research/quantum-channels/#examples-of-kraus-operators-for-quantum-channels","text":"\\(\\mathcal E(\\rho) = U \\rho\\ U^\\dagger \\rightarrow\\) single Kraus operator \\(\\{U\\}\\) \\(\\mathcal E(\\rho) = \\sum_e p_e U_e\\ \\rho\\ U_e^\\dagger \\rightarrow\\) Kraus operators \\(\\{\\sqrt{p_e} U_e\\}\\) \\(\\text{tr}(\\rho_{AB}) = \\sum_b (I_A \\otimes \\langle b|)\\ \\rho\\ (I_A \\otimes |b\\rangle)\\) where \\(V_b = (I_A \\otimes \\langle b |_B)\\) thus we also have the following holding true, \\(\\sum_b V_b^\\dagger V_b = \\sum_b I_A \\otimes |b\\rangle\\langle b| = I_A \\otimes I_B = I\\) . Ultimate Refrigerator: \\(\\mathcal E (\\rho) = |0\\rangle\\langle 0|\\) , regardless of the initial state \\(\\rho\\) . \\[ V_0 = |0\\rangle\\langle 0|, V_1 = |0\\rangle\\langle 1| \\] \\[ V_0 \\rho\\ V_0^\\dagger + V_1 \\rho\\ V_1^\\dagger = |0\\rangle\\langle 0| \\] Depolarizing Channel: \\(\\mathcal E (\\rho) = (1 - p)\\rho + \\rho ({I}/{2})\\) \\(\\rightarrow\\) simple kind of white noise effect Amplitude Damping Channel: \\(\\mathcal E_\\gamma (\\rho) = K_0 \\rho\\ K_0^\\dagger + K_1 \\rho\\ K_1^\\dagger\\) where we have the following \\[ K_0 = \\begin{bmatrix} 1 & 0 \\\\ 0 & \\sqrt{1 - \\gamma} \\\\ \\end{bmatrix} \\text{ and, }\\ K_1 = \\begin{bmatrix} 0 & \\sqrt{\\gamma} \\\\ 0 & 0 \\\\ \\end{bmatrix} \\]","title":"Examples of Kraus Operators for Quantum Channels"},{"location":"research/quantum-channels/#axiomatic-definitions","text":"By God, properties of \\(\\mathcal E\\) should be as follows: Linear (assume this or die, basically) and Hermitian preserving Trace preserving (TP) such that \\(\\text{tr}(\\mathcal E(\\rho)) = \\text{tr}(\\rho) = 1\\) Completely positive (CP) since \\(\\rho \\geq 0 \\implies (\\mathcal E \\otimes I)(\\rho) \\geq 0\\) But why completely positive? Well, because we are dealing with isometries of form \\(A \\to B \\otimes E\\) . Here, completely positive means that if \\(\\mathcal E\\) is applied on a subsystem, then the complete system and the subsystems must remain positive.","title":"Axiomatic Definitions"},{"location":"research/quantum-channels/#equivalence-of-tpcp-maps-and-kraus-operators","text":"Equivalence Theorem: If \\(\\mathcal E(\\rho) = \\sum_e V_e\\rho\\ V_e^\\dagger\\) then \\(\\mathcal E\\) is TPCP and converse holds true as well. This implies that the map \\(N\u2192J(N)\\) is injective. In fact it is known as the Choi-Jamio\u0142kowski isomorphism \u2014 a correspondence between quantum channels and quantum states (described by density matrices). Equivalence of Operators: Given \\(2\\) operators \\(\\mathcal E\\) and \\(\\mathcal F\\) with operator elements \\(\\{\\mathcal E_i\\}\\) and \\(\\{\\mathcal F_i\\}\\) respectively, if \\(\\mathcal E = \\mathcal F\\) then \\[ \\mathcal E_i = \\sum_j u_{ij} \\mathcal F_j \\]","title":"Equivalence of TPCP maps and Kraus operators"},{"location":"research/quantum-channels/#measurements-as-quantum-operations","text":"Measurement can be thought of as a quantum operation where the input is any quantum state and the output is classical, \\(\\mathcal E(\\rho) = \\sum_x p_x|x\\rangle\\langle x|\\) (diagonal density matrix). The probabilities, \\(p_x\\) , should depend on the state. Further, \\(p_x\\) should be a linear function of the density matrix \\(p_x = \\text{tr}(M_x \\rho)\\) . From this we can work out the properties that \\(M_x\\) should obey: Normalization: \\(\\text{tr}(\\mathcal E(\\rho)) = \\text{tr}(p_x) = tr(M_x\\rho) = 1 \\implies \\sum M_x = I\\) Positive Semi-definite: \\((\\mathcal E \\otimes I)(\\rho) \\geq 0 \\implies p_x \\geq 0\\) thus \\(\\text{tr}(M_x \\rho) \\geq 0, \\forall\\ M_x\\) over \\(\\rho\\) thus, we have \\(M_x \\geq 0 \\implies M_x's\\) are positive semi-definite \\(\\implies M_x = E_x E_x^\\dagger\\) Thus, we have the following: \\[ \\sum_x M_x = I \\] \\[ M_x \\geq 0 \\] These conditions still leave room for noisy measurements, etc. We can also talk about non-demolition measurements, which do not discard the quantum systems aftermeasurements. Consider the following quantum channel. \\[ \\mathcal E(\\rho) =\\sum_e V_e \\rho V^\\dagger_e \\otimes |e\\rangle\\langle e| = \\sum_e \\frac{V_e \\rho V^\\dagger_e} {\\text{tr}(V_e\\rho V^\u2020_e)} \\otimes \\text{tr}(V_e \\rho V^\\dagger_e)|e\\rangle\\langle e| \\] This channel has the following interpretation: with probability as \\[ p_e = \\text{tr}(V_e \\rho V_e^\\dagger) = \\text{tr}(V_e V_e^\\dagger\\rho) = \\text{tr}(M_e\\rho) \\] the state of the system after the application of this channel is \\(\\rho_e = {(V_e \\rho V^\\dagger_e)}/{\\text{tr}(V_e\\rho V^\u2020_e)}\\) . This is similar to having a measurement that outputs the post measured state \\(\\rho_e\\) with probability \\(p_e\\) .","title":"Measurements as Quantum Operations"},{"location":"research/quantum-channels/#quantum-norms-and-distance-metrics","text":"Motivation: Now that we have defined channels and states of information, how do you differentiate between two items of information? What does it mean to say that information is preserved by some process? Well for these questions it is necessary to develop distance measures. There is a certain arbitrariness in the way distance measures are defined, both classically and quantum mechanically, and the community of people studying quantum computation and quantum information has found it convenient to use a variety of distance measures over the years. Two of those measures, the trace distance and the fidelity, have particularly wide currency today.","title":"Quantum Norms and Distance Metrics"},{"location":"research/quantum-channels/#norms","text":"Norm is a distance metric such that \\(|cv| = |c|.|v|\\) \\(|v+w| \\leq |v| + |w|\\) \\(|v| = 0\\) iff \\(v = 0\\) Examples of norms are as follows Manhattan ( \\(L_1\\) ) Euclidean ( \\(L_2\\) ) \\(L_p\\) norm \\(\\to |v|_{L_p} = (\\sum v_i^p)^{1/p}\\)","title":"Norms"},{"location":"research/quantum-channels/#schatten-p-norms","text":"\\(L_1\\) norm corresponds to probability distributions \\(L_2\\) norm corresponds to pure states \\(L_\\infty = \\max_i {|v_i|}\\) Similarly, schatten \\(p\\) -norm \\(:\\) \\(|M|_{S_p} = |\\sigma(M)|_{S_p}\\) where \\(\\sigma(M)\\) is a vector of singular values of a matrix, say \\(M\\) . \\[ S_1 = \\sum \\text{singular values} \\] \\[ S_2 = \\sqrt {\\sum \\text{singular values}^2} \\] \\[ S_\\infty = \\max_i |\\text{singular values}| \\] In case of density matrices (in general to positive semi-definite Hermitian operators), we have \\(\\text{singular values = eigen values}\\) thus, \\(\\sigma(M) = \\lambda(M)\\) . \\[ |X|_{S_p} = |\\sigma(X)|_{L_p} = (\\sum_i \\sigma_i^p)^{1/p} \\] If \\(X \\geq 0\\) , then \\(\\sigma(X) = \\lambda(X)\\) then, \\(|X|_{S_1} = tr X \\implies |\\rho|_{S_1} {= \\text{tr} (\\rho)}\\) . Thus, \\(\\rho\\) is a density matrix \\(\\iff \\rho \\geq 0\\) and \\(|\\rho|_{S_1} = 1\\) .","title":"Schatten \\(p\\)-norms"},{"location":"research/quantum-channels/#measurements","text":"Let us consider a simple system with \\(\\{M, I-M\\}\\) as measurement operators. Then, we have \\[ M \\geq 0, I - M \\geq 0 \\iff M \\leq I \\] \\[ \\iff 0 \\leq M \\leq I \\] \\[ \\iff |M|_{S_\\infty} \\leq 1 \\] But why is it that two objects with different norms can co-exist in the same operator framework? The answer lies in the notion of duality.","title":"Measurements"},{"location":"research/quantum-channels/#duality","text":"Given a norm \\(|.|\\) there exists a dual norm such that, \\(|x|_* = \\max_{|y| \\leq 1} |\\langle x,y\\rangle|\\) . \\(L_2\\) is dual to \\(L_2\\) \\(L_1\\) and \\(L_\\infty\\) are dual to each other \\(S_2\\) is dual to \\(S_2\\) \\(S_1\\) and \\(S_\\infty\\) are dual to each other","title":"Duality"},{"location":"research/quantum-channels/#introduction-to-trace-norm","text":"\\[ T(\\rho, \\sigma) = \\frac{1}{2}|\\rho - \\sigma|_{S_1} = \\frac{1}{2}\\ \\text{tr}\\ |\\rho - \\sigma| \\] Then, \\(\\forall M\\) where \\(M\\) denotes measurement, \\[ |\\text{tr}(M\\rho) - \\text{tr}(M\\sigma)| = |\\text{tr}(M (\\rho - \\sigma))| \\leq |M|_{S_\\infty} |\\rho -\\sigma|_{S_1} \\leq |\\rho -\\sigma|_{S_1} = 2T(\\rho, \\sigma) \\] In fact, we can tighten this inequality even further to obtain \\[ |\\text{tr}(M\\rho) - \\text{tr}(M\\sigma)| \\leq T(\\rho, \\sigma) \\] \\[ \\Big\\downarrow \\] \\[ T(\\rho, \\sigma) = \\max_M |\\text{tr}\\ M(\\rho-\\sigma)| \\] \\[ \\Big\\downarrow \\] \\[ T(\\mathcal E(\\rho) - \\mathcal E(\\sigma)) \\leq \\max_M |\\text{tr}\\ M(\\rho-\\sigma| = T(\\rho, \\sigma) \\] and finally we get the required inequality \\[ T(\\mathcal E(\\rho), \\mathcal E(\\sigma)) \\leq T(\\rho, \\sigma) \\] Thus, I can never increase the distance of two states, no matter what. So, basically when you wanna protect against noise all we are doing is slowing down the rate of noise (indistinguishable nonsense).","title":"Introduction to Trace Norm"},{"location":"research/quantum-channels/#chois-theorem","text":"Let \\(\\mathcal E :\\mathbb {C} ^{n\\times n}\\to \\mathbb {C} ^{m\\times m}\\) be a linear map. Then, the following are equivalent: \\(\\mathcal E\\) is \\(n\\) -positive (i.e. \\(\\mathcal E(A)\\in \\mathbb {C} ^{m\\times m}\\) is positive whenever \\(A\\in \\mathbb {C} ^{n\\times n}\\) is positive). The matrix \\(\\Gamma_\\mathcal E\\) , sometimes called the Choi matrix, is positive. Here the state \\(\\phi\\) is maximally entangled. \\[ \\Gamma_{\\mathcal E}=\\left(\\operatorname {id} _{n}\\otimes \\mathcal E \\right)\\left(\\vert \\phi \\rangle\\langle \\phi \\vert\\right) \\] \\(\\mathcal E\\) is completely positive.","title":"Choi's Theorem"},{"location":"research/quantum-channels/#solving-a-lindblad-equation","text":"Any ideal open quantum system will undergo Markovian dynamics provided that its evolution satisfies a Master equation. \\[ \\frac{d\\rho}{dt} = \\mathcal L(\\rho) = -i[H, \\rho] + \\sum_k \\gamma_k (V_k\\rho V_k^\\dagger - \\frac{1}{2} \\{V_k^\\dagger V_k, \\rho\\}) \\] Let \\(\\rho = \\frac{1}{2}(I + \\vec r \\cdot \\vec \\sigma)\\) and differentiate \\(\\rho(t)\\) to obtain \\(\\dot \\rho(t)\\) in terms of \\(\\frac{d\\vec{r}}{dt}\\) . \\[ \\frac{d\\rho}{dt} = \\frac 1 2 (\\dot r_x\\sigma_x + \\dot r_y\\sigma_y + \\dot r_z\\sigma_z) \\] We equate \\(\\dot \\rho = \\mathcal L(\\rho)\\) using the above representation for \\(\\dot \\rho\\) and \\(\\rho\\) and solve the equation by solving the individual differential equations obtained. After we solve for \\(\\rho\\) , we obtain \\(\\Lambda_t\\) . \\[ \\Lambda_t: \\rho_0 \\to \\rho_t \\text{,~~~~~} \\rho(t) = \\Lambda_t(\\rho(0)) \\] Now, with Choi's theorem we check if \\(\\Lambda_t\\) is \\(CP\\) by checking whether \\(\\Gamma_\\Lambda\\) is positive semi-definite. \\[ \\Gamma_\\Lambda = C\\vert\\psi\\rangle\\langle\\psi\\vert \\] \\[ C = (I \\otimes \\Lambda_t) \\] Now, we find the eigenvalues and eigenvectors of \\(C\\) and represent it as \\(C = \\sum_i \\lambda_i\\vert v_i\\rangle\\) . Then, we can obtain Kraus operators \\(\\{K_i\\}_{\\forall i}\\) such as follows (check below). \\[ v_{i} = \\begin{bmatrix} -1/2\\\\1/2\\\\-1/2\\\\1/2 \\end{bmatrix} \\implies K_i = \\sqrt{\\lambda_i} \\begin{bmatrix} -1/2 & -1/2\\\\1/2 & 1/2 \\end{bmatrix} \\]","title":"Solving a Lindblad Equation"},{"location":"research/quantum-theory/","text":"On the Formulations of Quantum Mechanics Argument: Quantum Theory \\(=\\) Classical Probability Theory with \\(2\\) -norm and continuity axiom (complex numbers). Why \\(2\\) -norm and why not \\(p\\) -norm? Because if there are any linear transformations other than these trivial ones that preserve the \\(p\\) -norm, then either \\(p = 1\\) or \\(p=2\\) . If \\(p=1\\) we get classical probability theory, while if \\(p =2\\) we get quantum mechanics. Also all transformations must be norm preserving. So what kind of matrix or linear transformation preserves the \\(2\\) -norm? Unitary Matrices \\((UU^\\dagger = I)\\) . Why complex numbers and not real numbers? Axiom of continuity: There exists a continuous reversible transformation on a system between any two pure states of that system. Thus, we need our field associated with vector spaces to be algebraically closed. Hence, we need complex numbers. If you want every unitary operation to have a square root, then you have to go to the complex numbers. Why do transformations need to be linear? If quantum mechanics were nonlinear, then one could build a computer to solve NP-complete problems in polynomial time. (Abrams and Lloyd). History of Quantum Computing Models of Classical Computation The following serve as universal models of computation: Turing Machines \\(\\lambda\\) -calculus Circuits Super-Universal Circuits can describe computations which are beyond what a Turing machine can do. Apparently, we can solve the halting problem under some circuit model. But, what is that supposed to mean? Quantum Computation The idea of a QC model is to have information in superposition. If we want Turing Machines to be part of the quantum computational model then we will somehow have to make the read/write head to be in a superposition. But that is not possible (doubt). Hence we use a circuit where we keep the bits in a superposition and operate on them with gates. History of Quantum Mechanics and Computation Major questions in QM started arising with the EPR paradox in 1936 when faster than light travel was questioned. Here's the paradox: Consider \\(2\\) particles that are moving away. Their positions are \\(x\\) and \\(-x\\) and momenta are \\(p\\) and \\(-p\\) . Now we can't measure the exact position and momentum of one particle due to the uncertainty principle. But what if we measure the position of one particle and momentum of the other at the same time (Like a split second difference)? Assuming there is no interaction between the particles, we would in principle know the position and momentum of one particle at a time (cause we can know position or momentum of particle \\(1\\) if we measure that quantity on the second particle cause its just the negative of it). This would violate the uncertainty principle. Hence, there has to be some sort of interaction between the two particles that makes the wavefunction of particle \\(2\\) also collapse instantaneously when I measure position or momentum of particle \\(1\\) . This mean that the particles were somehow connected. This was the paradox. But Schrodinger said this is possible by entanglement. In 1964 Bell gave his Theorem and then it was later verified too which proved that Quantum Mechanics is cool with EPR pair. But there was another problem that how is faster than light communication possible? It was later proved that faster than light communication is not possible indeed as you cannot actually send 'information' with the entanglement coordination. Basically you cannot manipulate the particles to actually transmit any useful information. Non-cloning theorem: In physics, the no-cloning theorem states that it is impossible to create an independent and identical copy of an arbitrary unknown quantum state, a statement which has profound implications in the field of quantum computing among others. Quantum Theory from 5 reasonable Axioms State Mathematical object that can be used to deter-mine the probability associated with the out-comes of any measurement that may be per-formed on a system prepared by the given preparation. However, we do not need to measure all possible probability measurements to determine the state of a system. K (degrees of freedom) : minimum number of probability measurements required to determine the state of a system, i.e., number of real parameters re-quired to specify the state. N (dimension) : maximum number of states that can be reliably distinguished from one another in a single shot measurement. Axioms Probabilities Simplicity: \\(K = K(N)\\) Subspaces: If state of a system \\(A\\) \\(\\in M_{dim}\\) subspace \\(\\implies dim(A) = M\\) Composite systems: \\(N = N_aN_b \\text{ and } K = K_aK_b\\) Continuity: \\(\\exists\\) a continuous reversible transformation on a system between anytwo pure states of that system Landauer's Principle Landauer's Principle explains that when info is erased, it requires work. Each bit erased \\(\\implies \\Delta S = -K \\ln(2)\\) . Hence, heat associated is given by \\(Q = -KT \\ln(2)\\) . Hence to keep constant temp, that amount of work needs to be put in. This generalises to reversible processes requiring work. Postulates of Quantum Mechanics I shall state here the four major generic postulates of Quantum Mechanics stated in terms of both state-vector formalization and density-matrix formalization. State is a vector Isolated physical system is given by its state vector operating on a certain Hilbert space. Evolution of a closed quantum system is given by a unitary transformation. In its physical interpretation we have this postulate governed by the Schrodinger Equation, as stated. \\[ i{\\hbar}\\frac{d\\vert\\psi\\rangle}{dt} = H\\vert\\psi\\rangle \\] The Hamiltonian is a hermitian operator and has a spectral decomposition, \\(H = \\sum E\\vert E\\rangle\\langle E\\vert\\) . The state space of a composite physical system is the tensor product of the state spaces of the component systems. \\[ \\vert\\psi\\rangle = \\vert\\psi_1\\rangle\\otimes...\\otimes\\vert\\psi_n\\rangle \\] Quantum measurements are described by a collection \\(\\{M_m\\}\\) of measurement operators acting on the state space of the system. Probability that upon measurement the outcome is \\(m = p(m) = \\langle\\psi\\vert M_m^\\dagger M_m\\vert\\psi\\rangle\\) and the state of the system becomes as follows. \\[ \\vert\\psi\\rangle \\xrightarrow{\\text{on measuring}} \\frac{M_m\\vert\\psi\\rangle}{\\sqrt{p(m)}} = \\frac{M_m\\vert\\psi\\rangle}{||M_m\\vert\\psi\\rangle||} \\] Measurement operators also follow the completeness equation, \\(\\sum_m M_m^\\dagger M_m = I\\) . State is a density matrix Isolated physical system is given by its density matrix operating on a certain Hilbert space. Evolution of a closed quantum system is given by a unitary transformation as \\(\\rho \\xrightarrow{U} U\\rho U^\\dagger\\) . The state space of a composite physical system is the tensor product of the state spaces of the component systems. \\[ \\rho = \\rho_1\\otimes...\\otimes\\rho_n \\] Quantum measurements are described by a collection \\(\\{M_m\\}\\) of measurement operators acting on the state space of the system. Probability that upon measurement the outcome is \\(m = p(m) = tr(M_m^\\dagger M_m\\rho)\\) and the state of the system becomes as follows. \\[ \\rho \\xrightarrow{\\text{on measuring}} \\frac{M_m\\rho M_m^\\dagger}{p(m)} \\] Measurement operators also follow the completeness equation, \\(\\sum_m M_m^\\dagger M_m = I\\) . Measurement Postulate Quantum Measurements are descrivbed by a collection of measurement operators \\(\\{M_m\\}\\) with \\(p(m)= \\langle \\psi | M_m^\\dagger M_m | \\psi \\rangle,\\ \\sum{p(m)} = 1\\) . \\[ \\vert \\psi \\rangle \\xrightarrow{\\text{on measurement}} \\frac{M_m \\vert \\psi \\rangle}{\\sqrt{p(m)}} \\] If \\(M_i = |i\\rangle \\langle i|,\\ \\forall i\\) then we are measuring in a computational basis. Non-distinguishability of arbitrary states We cannot distinguish any two arbitrary non-orthogonal quantum states. Proof: Let \\(|\\psi_1\\rangle\\) and \\(|\\psi_2\\rangle\\) be two non-orthogonal states. Then \\(\\exists\\ E_1, E_2\\) such that \\(E_1 = \\sum_{j:f(j) = 1} M_j^\\dagger M_j\\) and similarly \\(E_2\\) then \\(\\langle \\psi_1 | E_1 | \\psi_1 \\rangle = 1\\) and \\(\\langle \\psi_2 | E_2 | \\psi_2 \\rangle = 1\\) . Thus, \\(\\sqrt{E_2}| \\psi_1\\rangle = 0\\) and let \\(\\psi_2 = a\\psi_1 + b \\phi\\) where \\(\\psi\\) and \\(\\phi\\) are orthogonal. \\[ \\langle \\psi_2 | E_2 \\psi_2 \\rangle = |\\beta|^2 \\langle \\psi_2 | \\phi \\psi_2 \\leq |\\beta|^2 < 1 \\] Hence, proved by contradiction. Projective Measurements We can use projective measurement formalism for any general measurement too. In case of projective measurements, \\(M = \\sum mP_m\\) and \\(p(m) = \\langle \\psi | P_m | \\psi \\rangle\\) . \\[ \\vert \\psi \\rangle \\rightarrow \\frac{P_m \\vert \\psi\\rangle}{\\sqrt {p(m)}} \\] \\[ E(M) = \\sum mp(m) = \\langle \\psi | M | \\psi \\rangle = \\langle M\\rangle \\] \\[ \\Delta(M) = \\sqrt{\\langle M^2\\rangle - \\langle M \\rangle^2} \\] Here, \\(E(M)\\) is expectation and \\(\\Delta(M)\\) is standard deviation or the square root of variance. POVM Measurements POVM Measurements are a formalism where only measurement statistics matters. \\[ \\{E_m\\} \\rightarrow \\sum{E_m} = I,\\ p(m) = \\langle \\psi | E_m |\\psi \\rangle \\] Here, each of \\(E_m\\) are hermitian. Global Phase doesn't matter We say \\(e^{i\\theta} |\\psi\\rangle \\equiv |\\psi\\rangle\\) but why? Because, \\(\\langle \\psi | M_m^{\\dagger}M_m | \\psi \\rangle = \\langle \\psi \\vert e^{-i\\theta}M_m^\\dagger M_m e^{i\\theta}\\vert \\psi \\rangle\\) . However, be aware that the global phase is quite different from the relative phase. Density Matrices We can represent a system as an ensemble of pure states \\(\\{p_i, \\psi_i\\}\\) . Now, if you have exact knowledge of the system then it is for sure in a pure state, i.e., \\(\\rho = |\\psi\\rangle\\langle\\psi|\\) . However, if we have classical uncertainty amongst the possible states. The system can be represented as a mixed state \\(\\rho = \\sum_{i} p_i\\psi_i\\) where the probabilities \\(p_i\\) are classical in nature. This formulation helps us a lot in dealing with quantum information, noisy systems and helps us represent measurements better, as well. Why? Because it provides a convenient means for describing quantum systems whose state is not completely known. Postulates in Density Matrices formulation Isolated physical system is given by its density matrix operating on a certain Hilbert space. Evolution of a closed quantum system is given by a unitary transformation as \\(\\rho \\xrightarrow{U} U\\rho U^\\dagger\\) . The state space of a composite physical system is the tensor product of the state spaces of the component systems. \\[ \\rho = \\rho_1\\otimes...\\otimes\\rho_n \\] Quantum measurements are described by a collection \\(\\{M_m\\}\\) of measurement operators acting on the state space of the system. Probability that upon measurement the outcome is \\(m = p(m) = tr(M_m^\\dagger M_m\\rho)\\) and the state of the system becomes as follows. \\[ \\rho \\xrightarrow{\\text{on measuring}} \\frac{M_m\\rho M_m^\\dagger}{p(m)} \\] Measurement operators also follow the completeness equation, \\(\\sum_m M_m^\\dagger M_m = I\\) . Properties Theorem 1: An operator \\(\\rho\\) is a density operator if and only if it is both positive semi-definite \\((\\rho = \\rho^\\dagger\\) and non-negative eigenvalues \\()\\) and \\(tr(\\rho) = 1\\) . Converse is easy to prove. If an operator is both positive and has trace as one, then it shall have a spectral decomposition of the form \\(\\sum_i\\lambda_i|i\\rangle\\langle i|\\) . For the direct proof, let us consider \\(tr(\\rho) = \\sum_i p_i tr(|\\psi_i\\rangle\\langle\\psi_i|) = 1\\) . Theorem 2: The sets \\(|\\tilde\\psi\\rangle\\) and \\(|\\tilde\\phi\\rangle\\) generate the same density matrix if and only if, \\[ |\\tilde\\psi_i\\rangle = \\sum_j u_{ij} |\\tilde\\phi_j\\rangle \\] where \\(u_{ij}\\) is a unitary matrix of complex numbers, with indices \\(i\\) and \\(j\\) , and we 'pad' whichever set of vectors \\(|\\tilde\\psi_i\\rangle\\) or \\(|\\tilde\\phi_j\\rangle\\) is smaller with additional vectors \\(0\\) so that the two sets have the same number of elements. As a consequence of the theorem, note that \\(\\rho = \\sum_i p_i |\\psi_i\\rangle\\langle\\psi_i| = \\sum_j q_j |\\phi_j\\rangle\\langle\\phi_j|\\) if and only if we have the following as true for some unitary matrix \\(u_{ij}\\) . \\[ \\sqrt p_i |\\psi_i\\rangle = \\sum_j u_{ij} \\sqrt q_j |\\phi_j\\rangle \\] Theorem 3: If \\(\\rho\\) is a density operator, then \\(\\rho\\) is a pure state if and only if \\(tr(\\rho^2) = 1\\) and mixed state if and only if \\(tr(\\rho^2) < 1\\) . Theorem 4: Observable \\(M\\) has expectation \\(\\sum_x \\langle \\psi_x|M|\\psi_x\\rangle = tr(M\\rho)\\) . Reduced Density Operator This is the single-most important application of density operator formulation is the existence of reduced density operator. It is defined as follows. \\[ \\rho_A = tr_B(\\rho_{AB}) \\] This allows us to talk about sub-systems of a composite system. \\[ tr_B(|a_1\\rangle\\langle a_2| \\otimes |b_1\\rangle\\langle b_2|) = |a_1\\rangle\\langle a_2| tr(|b_1\\rangle\\langle b_2|) \\] Schmidt Decomposition Suppose \\(|\\psi\\rangle\\) is a pure state of a composite system, \\(AB\\) . Then, there exists orthonormal states \\(|i_A\\rangle\\) for system \\(A\\) , and orthonormal states \\(|i_B\\rangle\\) of system \\(B\\) such that \\[ |\\psi\\rangle = \\sum_i \\lambda_i |i_A\\rangle |i_B\\rangle \\] where \\(\\lambda_i\\) are non-negative real numbers satisfying \\(\\sum_i \\lambda_i^2 = 1\\) known as Schmidt co-efficients. Purification Suppose we have a mixed state \\(\\rho_A\\) for a system \\(A\\) . Then, we can introduce another system \\(R\\) such that \\(AR\\) forms a pure state \\(|AR\\rangle\\) and \\(\\rho_A = tr_R(|AR\\rangle\\langle AR|)\\) . Given \\(\\rho_A = \\sum_i p_i |i_A\\rangle\\langle i_A|\\) , we shall have the following where \\(|i_R\\rangle\\) are orthonormal basis states. \\[ |AR\\rangle = \\sum_i \\sqrt{p_i}|i_A\\rangle|i_R\\rangle \\] Bloch Sphere and Rotations We can represent any \\(\\rho\\) (density matrix) as \\(\\frac 1 2 (I + \\vec{r}\\cdot\\vec{\\sigma})\\) Single Qubit Operations \\[ e^{iAx} = cos(x) + isin(x)A \\] Thus, \\(R_x(\\theta) = e^{i\\theta X/2}\\) , \\(R_y(\\theta) = e^{i\\theta Y/2}\\) and \\(R_z(\\theta) = e^{i\\theta Z/2}\\) . \\[ R_{\\hat{n}}(\\theta) = e^{i\\theta \\hat n\\cdot\\vec\\sigma/2} \\] In general, we have the above equation where \\(\\vec\\sigma = X\\hat i + Y\\hat j + Z\\hat k\\) . Some Algebra \\[ X^2 = Y^2 = Z^2 = -iXYZ = I \\] \\[ R_{\\hat n}(\\alpha) = R_z(\\phi)R_y(\\theta)R_z(\\alpha)R_y(-\\theta)R_z(-\\phi)\\\\ = R_z(\\phi)R_y(\\theta)R_z(\\alpha)R_y(\\theta)^\\dagger R_z(\\phi)^\\dagger \\] Theorems Any arbitrary single qubit unitary operator can be written in the form \\(U = e^{i\\alpha}R_{\\hat n}(\\theta)\\) . Suppose \\(U\\) is a unitary operation over a single qubit then \\(\\exists\\ \\alpha, \\beta, \\gamma, \\delta\\) such that \\(U = e^{i\\alpha}R_{\\hat n}(\\beta)R_{\\hat m}(\\gamma)R_{\\hat n}(\\delta)\\) . There exists unitaries \\(A, B, C\\) for any given unitary \\(U\\) such that \\(ABC = I\\) and \\(U = e^{i\\alpha} AXBXC\\) . Proof that Bloch Sphere Unitaries as Rotations A single qubit operator can be represented as \\(U = a_0I + a_1X + a_2Y +a_3Z\\) . Also, such a unitary can also be represented this way, \\[ U = \\begin{bmatrix} a & b\\\\ c & d \\end{bmatrix} \\] and thus, we obtain the following equivalences. \\[ a_0 = (a+d)/2,\\ a_1 = (b+c)/2, \\] \\[ a_2 = (c-b)/2i,\\ a_3 = (d-a)/2 \\] Also from \\(UU^\\dagger = I\\) we get, \\[ |a_0|^2 + |a_1|^2 + |a_2|^2 + |a_3|^2 = 1 \\] \\[ a^\u2217_0a_1+a^\u2217_1a_0+ia^\u2217_2a_3\u2212ia^\u2217_3a_2 = 0 \\] \\[ a^\u2217_0a_2\u2212ia^\u2217_1a_3+a^\u2217_2a_0+ia^\u2217_3a_1 = 0 \\] \\[ a^\u2217_0a_3+ia^\u2217_1a_2\u2212ia^\u2217_2a_1+a^\u2217_3a_0 = 0 \\] where we define \\(|a_0| = cos(\\theta/2)\\) then \\(|a_1|^2 + |a_2|^2 + |a_3|^2 = |sin(\\theta/2)|\\) . Then define, \\[ n_x = |a_1|/|sin(\\theta/2)| \\] \\[ n_y = |a_2|/|sin(\\theta/2)| \\] \\[ n_z = |a_3|/|sin(\\theta/2)| \\] and further we get \\(n_x^2 + n_y^2 + n_z^2 = 1\\) . Now, we define \\(exp(i\u03b1) =a_0/cos(\u03b8/2)\\) and denote the phase of \\(a_1,a_2,a_3\\) as \\(\u03b1_1,\u03b1_2,\u03b1_3\\) respectively. By putting these in the other constraints we get, \\(\u03b1_1=\u03b1_2=\u03b1_3=\u03b1\u2212\u03c0/2\\) . \\[ a_0 = e^{i\\alpha}cos(\\theta/2), \\] \\[ a_1 = -ie^{i\\alpha}sin(\\theta/2)n_x, \\] \\[ a_2 = -ie^{i\\alpha}sin(\\theta/2)n_y, \\] \\[ a_3 = -ie^{i\\alpha}sin(\\theta/2)n_z \\] \\[ U= e^{i\u03b1}(\\cos(\\frac{\u03b8}{2})I \u2212 i\\sin(\\frac{\\theta}{2})(n_xX+n_yY+n_zZ))= e^{i\u03b1}R_{\\hat n}(\u03b8) \\] CHSH Inequality In a classical experiment, we have the following setup where Alice can choose to measure either Q or R and Bob chooses either S or T. The measurements are performed imultaneously and far off from each other. \\[ E(QS) + E(RS) + E(RT) - E(QT) = E(QS + RS + RT-QT) \\] \\[ \\implies E(QS) + E(RS) + E(RT) - E(QT) = \\sum_{q,r,s,t}p(q, r, s, t)(qs + rs + rt - qt) \\] \\[ \\implies E(QS) + E(RS) + E(RT) - E(QT) \\leq \\sum_{q,r,s,t}p\\times 2 = 2 \\] and thereby we obtain the inequality \\(E(QS) + E(RS) + E(RT) - E(QT) \\leq 2\\) . This is one of the set of Bell inequalities, the first of which was found by John Bell. This one in particular is named CHSH inequality. Quantum Anomaly In the quantum case, let us consider the measurements to be based on the following observables over the EPR pair \\(|\\psi\\rangle = \\frac{|01\\rangle - |10\\rangle}{\\sqrt 2}\\) . \\[ Q = Z_1, R= X_1, \\] \\[ S = \\frac{-Z_2-X_2}{\\sqrt 2}, \\] \\[ T = \\frac{Z_2 - X_2}{\\sqrt 2} \\] Then, we have the following result. \\[ E(QS) + E(RS) + E(RT) - E(QT) = \\frac{1}{\\sqrt 2} + \\frac{1}{\\sqrt 2} + \\frac{1}{\\sqrt 2} - \\frac{1}{\\sqrt 2} = 2\\sqrt 2 > 2 \\] Thus, in other words, CHSH inequality doesn't hold. Interpretation The fact that CHSH doesn't hold in the quantum scenario implies that two of the major assumptions about nature is wrong in case of the classical experiment. The assumptions are: Realism: Q, R, S, T are physical quantities which have defininte values irrespective of observation. Locality: Alice's measurement doesn't influence that of Bob's. Thus, the result of CHSH being false when accounted for the quantum mechanical properties of nature (we can perform the experiment in a lab with particles) suggests that nature cannot be locally real and neither can any true mathematical representation of it be locally real.","title":"Quantum Theory"},{"location":"research/quantum-theory/#on-the-formulations-of-quantum-mechanics","text":"Argument: Quantum Theory \\(=\\) Classical Probability Theory with \\(2\\) -norm and continuity axiom (complex numbers). Why \\(2\\) -norm and why not \\(p\\) -norm? Because if there are any linear transformations other than these trivial ones that preserve the \\(p\\) -norm, then either \\(p = 1\\) or \\(p=2\\) . If \\(p=1\\) we get classical probability theory, while if \\(p =2\\) we get quantum mechanics. Also all transformations must be norm preserving. So what kind of matrix or linear transformation preserves the \\(2\\) -norm? Unitary Matrices \\((UU^\\dagger = I)\\) . Why complex numbers and not real numbers? Axiom of continuity: There exists a continuous reversible transformation on a system between any two pure states of that system. Thus, we need our field associated with vector spaces to be algebraically closed. Hence, we need complex numbers. If you want every unitary operation to have a square root, then you have to go to the complex numbers. Why do transformations need to be linear? If quantum mechanics were nonlinear, then one could build a computer to solve NP-complete problems in polynomial time. (Abrams and Lloyd).","title":"On the Formulations of Quantum Mechanics"},{"location":"research/quantum-theory/#history-of-quantum-computing","text":"","title":"History of Quantum Computing"},{"location":"research/quantum-theory/#models-of-classical-computation","text":"The following serve as universal models of computation: Turing Machines \\(\\lambda\\) -calculus Circuits","title":"Models of Classical Computation"},{"location":"research/quantum-theory/#super-universal","text":"Circuits can describe computations which are beyond what a Turing machine can do. Apparently, we can solve the halting problem under some circuit model. But, what is that supposed to mean?","title":"Super-Universal"},{"location":"research/quantum-theory/#quantum-computation","text":"The idea of a QC model is to have information in superposition. If we want Turing Machines to be part of the quantum computational model then we will somehow have to make the read/write head to be in a superposition. But that is not possible (doubt). Hence we use a circuit where we keep the bits in a superposition and operate on them with gates.","title":"Quantum Computation"},{"location":"research/quantum-theory/#history-of-quantum-mechanics-and-computation","text":"Major questions in QM started arising with the EPR paradox in 1936 when faster than light travel was questioned. Here's the paradox: Consider \\(2\\) particles that are moving away. Their positions are \\(x\\) and \\(-x\\) and momenta are \\(p\\) and \\(-p\\) . Now we can't measure the exact position and momentum of one particle due to the uncertainty principle. But what if we measure the position of one particle and momentum of the other at the same time (Like a split second difference)? Assuming there is no interaction between the particles, we would in principle know the position and momentum of one particle at a time (cause we can know position or momentum of particle \\(1\\) if we measure that quantity on the second particle cause its just the negative of it). This would violate the uncertainty principle. Hence, there has to be some sort of interaction between the two particles that makes the wavefunction of particle \\(2\\) also collapse instantaneously when I measure position or momentum of particle \\(1\\) . This mean that the particles were somehow connected. This was the paradox. But Schrodinger said this is possible by entanglement. In 1964 Bell gave his Theorem and then it was later verified too which proved that Quantum Mechanics is cool with EPR pair. But there was another problem that how is faster than light communication possible? It was later proved that faster than light communication is not possible indeed as you cannot actually send 'information' with the entanglement coordination. Basically you cannot manipulate the particles to actually transmit any useful information. Non-cloning theorem: In physics, the no-cloning theorem states that it is impossible to create an independent and identical copy of an arbitrary unknown quantum state, a statement which has profound implications in the field of quantum computing among others.","title":"History of Quantum Mechanics and Computation"},{"location":"research/quantum-theory/#quantum-theory-from-5-reasonable-axioms","text":"","title":"Quantum Theory from 5 reasonable Axioms"},{"location":"research/quantum-theory/#state","text":"Mathematical object that can be used to deter-mine the probability associated with the out-comes of any measurement that may be per-formed on a system prepared by the given preparation. However, we do not need to measure all possible probability measurements to determine the state of a system. K (degrees of freedom) : minimum number of probability measurements required to determine the state of a system, i.e., number of real parameters re-quired to specify the state. N (dimension) : maximum number of states that can be reliably distinguished from one another in a single shot measurement.","title":"State"},{"location":"research/quantum-theory/#axioms","text":"Probabilities Simplicity: \\(K = K(N)\\) Subspaces: If state of a system \\(A\\) \\(\\in M_{dim}\\) subspace \\(\\implies dim(A) = M\\) Composite systems: \\(N = N_aN_b \\text{ and } K = K_aK_b\\) Continuity: \\(\\exists\\) a continuous reversible transformation on a system between anytwo pure states of that system","title":"Axioms"},{"location":"research/quantum-theory/#landauers-principle","text":"Landauer's Principle explains that when info is erased, it requires work. Each bit erased \\(\\implies \\Delta S = -K \\ln(2)\\) . Hence, heat associated is given by \\(Q = -KT \\ln(2)\\) . Hence to keep constant temp, that amount of work needs to be put in. This generalises to reversible processes requiring work.","title":"Landauer's Principle"},{"location":"research/quantum-theory/#postulates-of-quantum-mechanics","text":"I shall state here the four major generic postulates of Quantum Mechanics stated in terms of both state-vector formalization and density-matrix formalization.","title":"Postulates of Quantum Mechanics"},{"location":"research/quantum-theory/#state-is-a-vector","text":"Isolated physical system is given by its state vector operating on a certain Hilbert space. Evolution of a closed quantum system is given by a unitary transformation. In its physical interpretation we have this postulate governed by the Schrodinger Equation, as stated. \\[ i{\\hbar}\\frac{d\\vert\\psi\\rangle}{dt} = H\\vert\\psi\\rangle \\] The Hamiltonian is a hermitian operator and has a spectral decomposition, \\(H = \\sum E\\vert E\\rangle\\langle E\\vert\\) . The state space of a composite physical system is the tensor product of the state spaces of the component systems. \\[ \\vert\\psi\\rangle = \\vert\\psi_1\\rangle\\otimes...\\otimes\\vert\\psi_n\\rangle \\] Quantum measurements are described by a collection \\(\\{M_m\\}\\) of measurement operators acting on the state space of the system. Probability that upon measurement the outcome is \\(m = p(m) = \\langle\\psi\\vert M_m^\\dagger M_m\\vert\\psi\\rangle\\) and the state of the system becomes as follows. \\[ \\vert\\psi\\rangle \\xrightarrow{\\text{on measuring}} \\frac{M_m\\vert\\psi\\rangle}{\\sqrt{p(m)}} = \\frac{M_m\\vert\\psi\\rangle}{||M_m\\vert\\psi\\rangle||} \\] Measurement operators also follow the completeness equation, \\(\\sum_m M_m^\\dagger M_m = I\\) .","title":"State is a vector"},{"location":"research/quantum-theory/#state-is-a-density-matrix","text":"Isolated physical system is given by its density matrix operating on a certain Hilbert space. Evolution of a closed quantum system is given by a unitary transformation as \\(\\rho \\xrightarrow{U} U\\rho U^\\dagger\\) . The state space of a composite physical system is the tensor product of the state spaces of the component systems. \\[ \\rho = \\rho_1\\otimes...\\otimes\\rho_n \\] Quantum measurements are described by a collection \\(\\{M_m\\}\\) of measurement operators acting on the state space of the system. Probability that upon measurement the outcome is \\(m = p(m) = tr(M_m^\\dagger M_m\\rho)\\) and the state of the system becomes as follows. \\[ \\rho \\xrightarrow{\\text{on measuring}} \\frac{M_m\\rho M_m^\\dagger}{p(m)} \\] Measurement operators also follow the completeness equation, \\(\\sum_m M_m^\\dagger M_m = I\\) .","title":"State is a density matrix"},{"location":"research/quantum-theory/#measurement","text":"","title":"Measurement"},{"location":"research/quantum-theory/#postulate","text":"Quantum Measurements are descrivbed by a collection of measurement operators \\(\\{M_m\\}\\) with \\(p(m)= \\langle \\psi | M_m^\\dagger M_m | \\psi \\rangle,\\ \\sum{p(m)} = 1\\) . \\[ \\vert \\psi \\rangle \\xrightarrow{\\text{on measurement}} \\frac{M_m \\vert \\psi \\rangle}{\\sqrt{p(m)}} \\] If \\(M_i = |i\\rangle \\langle i|,\\ \\forall i\\) then we are measuring in a computational basis.","title":"Postulate"},{"location":"research/quantum-theory/#non-distinguishability-of-arbitrary-states","text":"We cannot distinguish any two arbitrary non-orthogonal quantum states. Proof: Let \\(|\\psi_1\\rangle\\) and \\(|\\psi_2\\rangle\\) be two non-orthogonal states. Then \\(\\exists\\ E_1, E_2\\) such that \\(E_1 = \\sum_{j:f(j) = 1} M_j^\\dagger M_j\\) and similarly \\(E_2\\) then \\(\\langle \\psi_1 | E_1 | \\psi_1 \\rangle = 1\\) and \\(\\langle \\psi_2 | E_2 | \\psi_2 \\rangle = 1\\) . Thus, \\(\\sqrt{E_2}| \\psi_1\\rangle = 0\\) and let \\(\\psi_2 = a\\psi_1 + b \\phi\\) where \\(\\psi\\) and \\(\\phi\\) are orthogonal. \\[ \\langle \\psi_2 | E_2 \\psi_2 \\rangle = |\\beta|^2 \\langle \\psi_2 | \\phi \\psi_2 \\leq |\\beta|^2 < 1 \\] Hence, proved by contradiction.","title":"Non-distinguishability of arbitrary states"},{"location":"research/quantum-theory/#projective-measurements","text":"We can use projective measurement formalism for any general measurement too. In case of projective measurements, \\(M = \\sum mP_m\\) and \\(p(m) = \\langle \\psi | P_m | \\psi \\rangle\\) . \\[ \\vert \\psi \\rangle \\rightarrow \\frac{P_m \\vert \\psi\\rangle}{\\sqrt {p(m)}} \\] \\[ E(M) = \\sum mp(m) = \\langle \\psi | M | \\psi \\rangle = \\langle M\\rangle \\] \\[ \\Delta(M) = \\sqrt{\\langle M^2\\rangle - \\langle M \\rangle^2} \\] Here, \\(E(M)\\) is expectation and \\(\\Delta(M)\\) is standard deviation or the square root of variance.","title":"Projective Measurements"},{"location":"research/quantum-theory/#povm-measurements","text":"POVM Measurements are a formalism where only measurement statistics matters. \\[ \\{E_m\\} \\rightarrow \\sum{E_m} = I,\\ p(m) = \\langle \\psi | E_m |\\psi \\rangle \\] Here, each of \\(E_m\\) are hermitian.","title":"POVM Measurements"},{"location":"research/quantum-theory/#global-phase-doesnt-matter","text":"We say \\(e^{i\\theta} |\\psi\\rangle \\equiv |\\psi\\rangle\\) but why? Because, \\(\\langle \\psi | M_m^{\\dagger}M_m | \\psi \\rangle = \\langle \\psi \\vert e^{-i\\theta}M_m^\\dagger M_m e^{i\\theta}\\vert \\psi \\rangle\\) . However, be aware that the global phase is quite different from the relative phase.","title":"Global Phase doesn't matter"},{"location":"research/quantum-theory/#density-matrices","text":"We can represent a system as an ensemble of pure states \\(\\{p_i, \\psi_i\\}\\) . Now, if you have exact knowledge of the system then it is for sure in a pure state, i.e., \\(\\rho = |\\psi\\rangle\\langle\\psi|\\) . However, if we have classical uncertainty amongst the possible states. The system can be represented as a mixed state \\(\\rho = \\sum_{i} p_i\\psi_i\\) where the probabilities \\(p_i\\) are classical in nature. This formulation helps us a lot in dealing with quantum information, noisy systems and helps us represent measurements better, as well. Why? Because it provides a convenient means for describing quantum systems whose state is not completely known.","title":"Density Matrices"},{"location":"research/quantum-theory/#postulates-in-density-matrices-formulation","text":"Isolated physical system is given by its density matrix operating on a certain Hilbert space. Evolution of a closed quantum system is given by a unitary transformation as \\(\\rho \\xrightarrow{U} U\\rho U^\\dagger\\) . The state space of a composite physical system is the tensor product of the state spaces of the component systems. \\[ \\rho = \\rho_1\\otimes...\\otimes\\rho_n \\] Quantum measurements are described by a collection \\(\\{M_m\\}\\) of measurement operators acting on the state space of the system. Probability that upon measurement the outcome is \\(m = p(m) = tr(M_m^\\dagger M_m\\rho)\\) and the state of the system becomes as follows. \\[ \\rho \\xrightarrow{\\text{on measuring}} \\frac{M_m\\rho M_m^\\dagger}{p(m)} \\] Measurement operators also follow the completeness equation, \\(\\sum_m M_m^\\dagger M_m = I\\) .","title":"Postulates in Density Matrices formulation"},{"location":"research/quantum-theory/#properties","text":"Theorem 1: An operator \\(\\rho\\) is a density operator if and only if it is both positive semi-definite \\((\\rho = \\rho^\\dagger\\) and non-negative eigenvalues \\()\\) and \\(tr(\\rho) = 1\\) . Converse is easy to prove. If an operator is both positive and has trace as one, then it shall have a spectral decomposition of the form \\(\\sum_i\\lambda_i|i\\rangle\\langle i|\\) . For the direct proof, let us consider \\(tr(\\rho) = \\sum_i p_i tr(|\\psi_i\\rangle\\langle\\psi_i|) = 1\\) . Theorem 2: The sets \\(|\\tilde\\psi\\rangle\\) and \\(|\\tilde\\phi\\rangle\\) generate the same density matrix if and only if, \\[ |\\tilde\\psi_i\\rangle = \\sum_j u_{ij} |\\tilde\\phi_j\\rangle \\] where \\(u_{ij}\\) is a unitary matrix of complex numbers, with indices \\(i\\) and \\(j\\) , and we 'pad' whichever set of vectors \\(|\\tilde\\psi_i\\rangle\\) or \\(|\\tilde\\phi_j\\rangle\\) is smaller with additional vectors \\(0\\) so that the two sets have the same number of elements. As a consequence of the theorem, note that \\(\\rho = \\sum_i p_i |\\psi_i\\rangle\\langle\\psi_i| = \\sum_j q_j |\\phi_j\\rangle\\langle\\phi_j|\\) if and only if we have the following as true for some unitary matrix \\(u_{ij}\\) . \\[ \\sqrt p_i |\\psi_i\\rangle = \\sum_j u_{ij} \\sqrt q_j |\\phi_j\\rangle \\] Theorem 3: If \\(\\rho\\) is a density operator, then \\(\\rho\\) is a pure state if and only if \\(tr(\\rho^2) = 1\\) and mixed state if and only if \\(tr(\\rho^2) < 1\\) . Theorem 4: Observable \\(M\\) has expectation \\(\\sum_x \\langle \\psi_x|M|\\psi_x\\rangle = tr(M\\rho)\\) .","title":"Properties"},{"location":"research/quantum-theory/#reduced-density-operator","text":"This is the single-most important application of density operator formulation is the existence of reduced density operator. It is defined as follows. \\[ \\rho_A = tr_B(\\rho_{AB}) \\] This allows us to talk about sub-systems of a composite system. \\[ tr_B(|a_1\\rangle\\langle a_2| \\otimes |b_1\\rangle\\langle b_2|) = |a_1\\rangle\\langle a_2| tr(|b_1\\rangle\\langle b_2|) \\]","title":"Reduced Density Operator"},{"location":"research/quantum-theory/#schmidt-decomposition","text":"Suppose \\(|\\psi\\rangle\\) is a pure state of a composite system, \\(AB\\) . Then, there exists orthonormal states \\(|i_A\\rangle\\) for system \\(A\\) , and orthonormal states \\(|i_B\\rangle\\) of system \\(B\\) such that \\[ |\\psi\\rangle = \\sum_i \\lambda_i |i_A\\rangle |i_B\\rangle \\] where \\(\\lambda_i\\) are non-negative real numbers satisfying \\(\\sum_i \\lambda_i^2 = 1\\) known as Schmidt co-efficients.","title":"Schmidt Decomposition"},{"location":"research/quantum-theory/#purification","text":"Suppose we have a mixed state \\(\\rho_A\\) for a system \\(A\\) . Then, we can introduce another system \\(R\\) such that \\(AR\\) forms a pure state \\(|AR\\rangle\\) and \\(\\rho_A = tr_R(|AR\\rangle\\langle AR|)\\) . Given \\(\\rho_A = \\sum_i p_i |i_A\\rangle\\langle i_A|\\) , we shall have the following where \\(|i_R\\rangle\\) are orthonormal basis states. \\[ |AR\\rangle = \\sum_i \\sqrt{p_i}|i_A\\rangle|i_R\\rangle \\]","title":"Purification"},{"location":"research/quantum-theory/#bloch-sphere-and-rotations","text":"We can represent any \\(\\rho\\) (density matrix) as \\(\\frac 1 2 (I + \\vec{r}\\cdot\\vec{\\sigma})\\)","title":"Bloch Sphere and Rotations"},{"location":"research/quantum-theory/#single-qubit-operations","text":"\\[ e^{iAx} = cos(x) + isin(x)A \\] Thus, \\(R_x(\\theta) = e^{i\\theta X/2}\\) , \\(R_y(\\theta) = e^{i\\theta Y/2}\\) and \\(R_z(\\theta) = e^{i\\theta Z/2}\\) . \\[ R_{\\hat{n}}(\\theta) = e^{i\\theta \\hat n\\cdot\\vec\\sigma/2} \\] In general, we have the above equation where \\(\\vec\\sigma = X\\hat i + Y\\hat j + Z\\hat k\\) .","title":"Single Qubit Operations"},{"location":"research/quantum-theory/#some-algebra","text":"\\[ X^2 = Y^2 = Z^2 = -iXYZ = I \\] \\[ R_{\\hat n}(\\alpha) = R_z(\\phi)R_y(\\theta)R_z(\\alpha)R_y(-\\theta)R_z(-\\phi)\\\\ = R_z(\\phi)R_y(\\theta)R_z(\\alpha)R_y(\\theta)^\\dagger R_z(\\phi)^\\dagger \\]","title":"Some Algebra"},{"location":"research/quantum-theory/#theorems","text":"Any arbitrary single qubit unitary operator can be written in the form \\(U = e^{i\\alpha}R_{\\hat n}(\\theta)\\) . Suppose \\(U\\) is a unitary operation over a single qubit then \\(\\exists\\ \\alpha, \\beta, \\gamma, \\delta\\) such that \\(U = e^{i\\alpha}R_{\\hat n}(\\beta)R_{\\hat m}(\\gamma)R_{\\hat n}(\\delta)\\) . There exists unitaries \\(A, B, C\\) for any given unitary \\(U\\) such that \\(ABC = I\\) and \\(U = e^{i\\alpha} AXBXC\\) .","title":"Theorems"},{"location":"research/quantum-theory/#proof-that-bloch-sphere-unitaries-as-rotations","text":"A single qubit operator can be represented as \\(U = a_0I + a_1X + a_2Y +a_3Z\\) . Also, such a unitary can also be represented this way, \\[ U = \\begin{bmatrix} a & b\\\\ c & d \\end{bmatrix} \\] and thus, we obtain the following equivalences. \\[ a_0 = (a+d)/2,\\ a_1 = (b+c)/2, \\] \\[ a_2 = (c-b)/2i,\\ a_3 = (d-a)/2 \\] Also from \\(UU^\\dagger = I\\) we get, \\[ |a_0|^2 + |a_1|^2 + |a_2|^2 + |a_3|^2 = 1 \\] \\[ a^\u2217_0a_1+a^\u2217_1a_0+ia^\u2217_2a_3\u2212ia^\u2217_3a_2 = 0 \\] \\[ a^\u2217_0a_2\u2212ia^\u2217_1a_3+a^\u2217_2a_0+ia^\u2217_3a_1 = 0 \\] \\[ a^\u2217_0a_3+ia^\u2217_1a_2\u2212ia^\u2217_2a_1+a^\u2217_3a_0 = 0 \\] where we define \\(|a_0| = cos(\\theta/2)\\) then \\(|a_1|^2 + |a_2|^2 + |a_3|^2 = |sin(\\theta/2)|\\) . Then define, \\[ n_x = |a_1|/|sin(\\theta/2)| \\] \\[ n_y = |a_2|/|sin(\\theta/2)| \\] \\[ n_z = |a_3|/|sin(\\theta/2)| \\] and further we get \\(n_x^2 + n_y^2 + n_z^2 = 1\\) . Now, we define \\(exp(i\u03b1) =a_0/cos(\u03b8/2)\\) and denote the phase of \\(a_1,a_2,a_3\\) as \\(\u03b1_1,\u03b1_2,\u03b1_3\\) respectively. By putting these in the other constraints we get, \\(\u03b1_1=\u03b1_2=\u03b1_3=\u03b1\u2212\u03c0/2\\) . \\[ a_0 = e^{i\\alpha}cos(\\theta/2), \\] \\[ a_1 = -ie^{i\\alpha}sin(\\theta/2)n_x, \\] \\[ a_2 = -ie^{i\\alpha}sin(\\theta/2)n_y, \\] \\[ a_3 = -ie^{i\\alpha}sin(\\theta/2)n_z \\] \\[ U= e^{i\u03b1}(\\cos(\\frac{\u03b8}{2})I \u2212 i\\sin(\\frac{\\theta}{2})(n_xX+n_yY+n_zZ))= e^{i\u03b1}R_{\\hat n}(\u03b8) \\]","title":"Proof that Bloch Sphere Unitaries as Rotations"},{"location":"research/quantum-theory/#chsh-inequality","text":"In a classical experiment, we have the following setup where Alice can choose to measure either Q or R and Bob chooses either S or T. The measurements are performed imultaneously and far off from each other. \\[ E(QS) + E(RS) + E(RT) - E(QT) = E(QS + RS + RT-QT) \\] \\[ \\implies E(QS) + E(RS) + E(RT) - E(QT) = \\sum_{q,r,s,t}p(q, r, s, t)(qs + rs + rt - qt) \\] \\[ \\implies E(QS) + E(RS) + E(RT) - E(QT) \\leq \\sum_{q,r,s,t}p\\times 2 = 2 \\] and thereby we obtain the inequality \\(E(QS) + E(RS) + E(RT) - E(QT) \\leq 2\\) . This is one of the set of Bell inequalities, the first of which was found by John Bell. This one in particular is named CHSH inequality.","title":"CHSH Inequality"},{"location":"research/quantum-theory/#quantum-anomaly","text":"In the quantum case, let us consider the measurements to be based on the following observables over the EPR pair \\(|\\psi\\rangle = \\frac{|01\\rangle - |10\\rangle}{\\sqrt 2}\\) . \\[ Q = Z_1, R= X_1, \\] \\[ S = \\frac{-Z_2-X_2}{\\sqrt 2}, \\] \\[ T = \\frac{Z_2 - X_2}{\\sqrt 2} \\] Then, we have the following result. \\[ E(QS) + E(RS) + E(RT) - E(QT) = \\frac{1}{\\sqrt 2} + \\frac{1}{\\sqrt 2} + \\frac{1}{\\sqrt 2} - \\frac{1}{\\sqrt 2} = 2\\sqrt 2 > 2 \\] Thus, in other words, CHSH inequality doesn't hold.","title":"Quantum Anomaly"},{"location":"research/quantum-theory/#interpretation","text":"The fact that CHSH doesn't hold in the quantum scenario implies that two of the major assumptions about nature is wrong in case of the classical experiment. The assumptions are: Realism: Q, R, S, T are physical quantities which have defininte values irrespective of observation. Locality: Alice's measurement doesn't influence that of Bob's. Thus, the result of CHSH being false when accounted for the quantum mechanical properties of nature (we can perform the experiment in a lab with particles) suggests that nature cannot be locally real and neither can any true mathematical representation of it be locally real.","title":"Interpretation"},{"location":"src/kotobaro-bhebechhinu/","text":"Countless times have I\u200c thought To let my zealous feelings rest at your doorstep Without caring for where it will lead In regards to my heart and self, To try to express how futile finding bliss is without you, And to let you know how much I have loved you in solitude. How can I\u200c bring it up to myself to tell you, Of my bare infatuations when to me you are but divine. How I did intend to disguise myself at a distance, And let my yearnings be unrequited and unknown. Concealed shall be my love, Concealed shall be my tears uncared for. But now that you come and ask If I ever thought of you in love or otherwise, How can I even express the extent of my love, Where unsaid all pain and me joy lies. This is a translation of '\u0995\u09a4\u09ac\u09be\u09b0 \u09ad\u09c7\u09ac\u09c7\u099b\u09bf\u09a8\u09c1 (Kotobaro Bhebechhinu)' written by Rabindranath Tagore in 1885. He based the song's music on an old rendition (around 1770s) of Ben Jonson's poem 'To Celia' . The rendition is called 'Drink to me only with thine eyes' (first line of the poem).","title":"Kotobaro Bhebechhinu"},{"location":"src/on-composition/","text":"On Composition To compose great writing, you must bleed words. But to bleed you must pick the sword up and fight. A lot. The desire preceeds the need. And when the words are stringed altogether in the bloodbath of thoughts and love, know that its still just words. And its still just for you. The composed art lies within, to be revealed once you wash the red away and only the deep stains which must remain, remain. With the revelation, also immerses away your burden of ownership. As it should. The art was by you, now its for all.","title":"On Composition"},{"location":"src/on-composition/#on-composition","text":"To compose great writing, you must bleed words. But to bleed you must pick the sword up and fight. A lot. The desire preceeds the need. And when the words are stringed altogether in the bloodbath of thoughts and love, know that its still just words. And its still just for you. The composed art lies within, to be revealed once you wash the red away and only the deep stains which must remain, remain. With the revelation, also immerses away your burden of ownership. As it should. The art was by you, now its for all.","title":"On Composition"},{"location":"src/on-problem-solving/","text":"A Treatise on Thinking and Problem Solving I have wasted a lot of time meta-problem solving. Or more specifically, wondering about how to think about approaching a problem. Most of these venturing was during my preparation for ICPC (and eventually World Finals). It wasn't all a waste though. Somethings stayed and changed my way of working. Firstly, the differences in approaching problems based on culture (read: particular field) and character (read: seeker or craftspeople). Secondly, the similarities across the board which could be generalized. Cliche, much? Do mathematicians, physicists and computer scientists approach a given problem differently? Is mathematical thinking different from algorithmic thinking? I love the word algorithmic rather than computational because the latter presents an ambiguous scope of considering the purely mechanistic working of a computer. Are there any generalizable heuristics that can be used or applied to any formal problem? In this essay, I will try my best to put forward several ideas I have come across and as they have been filtered and modified by my own point of view. References to interesting reads are present at the end. Mathematical Thinking and Algorithmic Thinking Mathematical thinking and algorithmic thinking have a lot in common. It shall be easier to first define what we consider mathematical thinking to be and what it means to be a mathematician. Mathematical Thinking Mathematical thinking involves the skills of modeling a given problem formally and rigorously arguing about the formal statement. Here, the point of rigor is not to destroy all intuition; instead, it should be used to eliminate bad intuition while clarifying and elevating good intuition. Even though rigor is merely the instrument of demonstration, just as intuition and curiosity are the instruments of invention, there is no denying that it alone can provide us with certainty. Moreover, in mathematics, there are two cultures involved. They differ based on the following two ideologies. The point of understanding mathematics is to become better able to solve problems. The point of solving problems is to understand mathematics better. I subscribe more towards the second culture, probably because I have a greater passion for theory building. It feels quite undeniable to me that there is something truly romantic about extracting underlying structures and generalizing them to create a much bigger, expressive, and beautiful framework. Differences There are two major differences between mathematical and algorithmic thinking. These involve two basic notions inherent to computer science and are totally absent (to a large extent in mathematics). notion of complexity and economy of operation (complexity theories) dynamic notion of the state of any process (data structures) These notions are inherently associated with the idea of implementability, which can be used to put forward the following picture \u2014 is computer science implementable mathematics? And if it were so, then computer science is rooted in physics as much as we believe it is rooted in mathematics. After all, it is the nature of our universe and existence which determines what is implementable and what is not. On Heuristics Proofs are Programs : Often, we find that the approaches we make (both mathematical and psychological) while trying to prove some statement or construct an algorithm are similar. This becomes clearer when we start reading more about the correspondence between proofs, programs, and algebraic structures. Such a trinity allows us to say with certainty that proofs and algorithms are complementary and analogous to a great extent. Furthermore, thinking about proofs and programs in this manner often gives us a clearer perspective while approaching any given problem. Abstraction and Modularity : The set of ideas I hope to entail here also go by several other names, like, wishful thinking and thinking from first principles. However, the main idea is following a modular approach while constructing an algorithm or proving a theorem. You can start off by dividing you problem in layers of chunks and while solving for a specific chunk, consider everything else as a blackbox. You can further hope to simplify each chunk or a set of chunks by reducing it to another problem or wishing off several constraints and trying your hand at that.When dealing with implementation jobs, then this methodology is also referred to as the top-down approach, albeit with some more caveats sprinkled all-over. On a similar note, it is worth remembering that while constructing any mathematical proof, you ought to try stringing lemmas together, along with speculation that has been cleaned and sharpened with rigor. Questions and Perspective : When solving a problem it is vital to keep changing your perspective. You need to look at it and attack it differently (using different tools with different tricks). For example, try contradictions, then maybe induction, then maybe something else and so on. While looking at a problem through different lenses ideally you would want to find the natural world for the problem, express it cohomologically and often the cohomology of that world may solve your problem, like a ripe avocado bursts in your hand. (Grothendieck) Likewise, never hesitate to keep asking questions, no matter how dumb they might seem. Don't take anything you don't really understand for granted! Where's the fun in that? Also, you need to realize how crucial partial progress is. Failures are often crucial advances. In fact, people should write about their failures and speculations while writing a paper. Because well, if we don't at all have any insight into what made someone come up with an argument or construction, then what is the fricking point at all? Formalism and Writing : Formalism is arguably the most important and recurring step towards solving any problem. This is something I learned while studying functional programming. Often, the act of describing a problem and stating it formally reveals what you need to solve it. And in case you are dealing with an algorithm or a computer system, formalism through programming is both the crux and beauty of it. Similarly, on a broader note, writing too is hugely essential. Ideally, it should be the primary mechanism for doing research and not just for reporting it. Solving problems depend on the ability to reason and implement your reasonings well. The ability to reason well is dependent on your knowledge (nodes), understanding (representation of the known nodes in a graph) and extrapolation (randomness is all you need). Reasoning is generally of three types: deductive: proofs, formalism inductive: abstractive, bayesian abductive: randomness, speculative questioning analogical: drawing parallels In case of competitive programming, for example, reasoning involves making observations and applying techniques. Observation: understanding the problem and come up with non-trivial properties. Technique: appling known algorithms or data structure to the problem. Implementation: writing the solution fast, bug-free and naturally understandable. In the end, all you need is courage, the strength to back it up and to love the pursuit of the same. Conclusion Heuristics, as mentioned above, serve as more of a psychological strategy than a tangible cut out path for solving problems. Almost all major problems that lurk in the horizon of human understanding are mainly solved by experience (of your own field and others), curiosity, hard work and peeking into randomness. Furthermore, we don't really understand how a human being thinks. We do have some understanding that our linguistic capabilities serve as an operating system for our minds supporting abstraction and other cognitive abilities. But, there is so much that we don't understand. However, do not let this lack of understanding reflect any kind of unimportance on the subject - for if we don't even understand how we think, how could we possibly make machines that can truly think like and for us? References The Two Cultures of Mathematics by Gowers On Proof and Progress in Mathematics by Thurston Poincar\u00e9 on Intuition in Mathematics Algorithmic Thinking and Mathematical Thinking by Knuth There\u2019s more to mathematics than rigor and proofs by Tao Ask yourself dumb questions by Tao Solving mathematical problems by Tao How to write a great research paper Seekers and Craftspeople by Lee Smolin Birds and Frogs by Freeman Dyson Some blog on codeforces","title":"On Problem Solving"},{"location":"src/on-problem-solving/#a-treatise-on-thinking-and-problem-solving","text":"I have wasted a lot of time meta-problem solving. Or more specifically, wondering about how to think about approaching a problem. Most of these venturing was during my preparation for ICPC (and eventually World Finals). It wasn't all a waste though. Somethings stayed and changed my way of working. Firstly, the differences in approaching problems based on culture (read: particular field) and character (read: seeker or craftspeople). Secondly, the similarities across the board which could be generalized. Cliche, much? Do mathematicians, physicists and computer scientists approach a given problem differently? Is mathematical thinking different from algorithmic thinking? I love the word algorithmic rather than computational because the latter presents an ambiguous scope of considering the purely mechanistic working of a computer. Are there any generalizable heuristics that can be used or applied to any formal problem? In this essay, I will try my best to put forward several ideas I have come across and as they have been filtered and modified by my own point of view. References to interesting reads are present at the end.","title":"A Treatise on Thinking and Problem Solving"},{"location":"src/on-problem-solving/#mathematical-thinking-and-algorithmic-thinking","text":"Mathematical thinking and algorithmic thinking have a lot in common. It shall be easier to first define what we consider mathematical thinking to be and what it means to be a mathematician.","title":"Mathematical Thinking and Algorithmic Thinking"},{"location":"src/on-problem-solving/#mathematical-thinking","text":"Mathematical thinking involves the skills of modeling a given problem formally and rigorously arguing about the formal statement. Here, the point of rigor is not to destroy all intuition; instead, it should be used to eliminate bad intuition while clarifying and elevating good intuition. Even though rigor is merely the instrument of demonstration, just as intuition and curiosity are the instruments of invention, there is no denying that it alone can provide us with certainty. Moreover, in mathematics, there are two cultures involved. They differ based on the following two ideologies. The point of understanding mathematics is to become better able to solve problems. The point of solving problems is to understand mathematics better. I subscribe more towards the second culture, probably because I have a greater passion for theory building. It feels quite undeniable to me that there is something truly romantic about extracting underlying structures and generalizing them to create a much bigger, expressive, and beautiful framework.","title":"Mathematical Thinking"},{"location":"src/on-problem-solving/#differences","text":"There are two major differences between mathematical and algorithmic thinking. These involve two basic notions inherent to computer science and are totally absent (to a large extent in mathematics). notion of complexity and economy of operation (complexity theories) dynamic notion of the state of any process (data structures) These notions are inherently associated with the idea of implementability, which can be used to put forward the following picture \u2014 is computer science implementable mathematics? And if it were so, then computer science is rooted in physics as much as we believe it is rooted in mathematics. After all, it is the nature of our universe and existence which determines what is implementable and what is not.","title":"Differences"},{"location":"src/on-problem-solving/#on-heuristics","text":"Proofs are Programs : Often, we find that the approaches we make (both mathematical and psychological) while trying to prove some statement or construct an algorithm are similar. This becomes clearer when we start reading more about the correspondence between proofs, programs, and algebraic structures. Such a trinity allows us to say with certainty that proofs and algorithms are complementary and analogous to a great extent. Furthermore, thinking about proofs and programs in this manner often gives us a clearer perspective while approaching any given problem. Abstraction and Modularity : The set of ideas I hope to entail here also go by several other names, like, wishful thinking and thinking from first principles. However, the main idea is following a modular approach while constructing an algorithm or proving a theorem. You can start off by dividing you problem in layers of chunks and while solving for a specific chunk, consider everything else as a blackbox. You can further hope to simplify each chunk or a set of chunks by reducing it to another problem or wishing off several constraints and trying your hand at that.When dealing with implementation jobs, then this methodology is also referred to as the top-down approach, albeit with some more caveats sprinkled all-over. On a similar note, it is worth remembering that while constructing any mathematical proof, you ought to try stringing lemmas together, along with speculation that has been cleaned and sharpened with rigor. Questions and Perspective : When solving a problem it is vital to keep changing your perspective. You need to look at it and attack it differently (using different tools with different tricks). For example, try contradictions, then maybe induction, then maybe something else and so on. While looking at a problem through different lenses ideally you would want to find the natural world for the problem, express it cohomologically and often the cohomology of that world may solve your problem, like a ripe avocado bursts in your hand. (Grothendieck) Likewise, never hesitate to keep asking questions, no matter how dumb they might seem. Don't take anything you don't really understand for granted! Where's the fun in that? Also, you need to realize how crucial partial progress is. Failures are often crucial advances. In fact, people should write about their failures and speculations while writing a paper. Because well, if we don't at all have any insight into what made someone come up with an argument or construction, then what is the fricking point at all? Formalism and Writing : Formalism is arguably the most important and recurring step towards solving any problem. This is something I learned while studying functional programming. Often, the act of describing a problem and stating it formally reveals what you need to solve it. And in case you are dealing with an algorithm or a computer system, formalism through programming is both the crux and beauty of it. Similarly, on a broader note, writing too is hugely essential. Ideally, it should be the primary mechanism for doing research and not just for reporting it. Solving problems depend on the ability to reason and implement your reasonings well. The ability to reason well is dependent on your knowledge (nodes), understanding (representation of the known nodes in a graph) and extrapolation (randomness is all you need). Reasoning is generally of three types: deductive: proofs, formalism inductive: abstractive, bayesian abductive: randomness, speculative questioning analogical: drawing parallels In case of competitive programming, for example, reasoning involves making observations and applying techniques. Observation: understanding the problem and come up with non-trivial properties. Technique: appling known algorithms or data structure to the problem. Implementation: writing the solution fast, bug-free and naturally understandable. In the end, all you need is courage, the strength to back it up and to love the pursuit of the same.","title":"On Heuristics"},{"location":"src/on-problem-solving/#conclusion","text":"Heuristics, as mentioned above, serve as more of a psychological strategy than a tangible cut out path for solving problems. Almost all major problems that lurk in the horizon of human understanding are mainly solved by experience (of your own field and others), curiosity, hard work and peeking into randomness. Furthermore, we don't really understand how a human being thinks. We do have some understanding that our linguistic capabilities serve as an operating system for our minds supporting abstraction and other cognitive abilities. But, there is so much that we don't understand. However, do not let this lack of understanding reflect any kind of unimportance on the subject - for if we don't even understand how we think, how could we possibly make machines that can truly think like and for us?","title":"Conclusion"},{"location":"src/on-problem-solving/#references","text":"The Two Cultures of Mathematics by Gowers On Proof and Progress in Mathematics by Thurston Poincar\u00e9 on Intuition in Mathematics Algorithmic Thinking and Mathematical Thinking by Knuth There\u2019s more to mathematics than rigor and proofs by Tao Ask yourself dumb questions by Tao Solving mathematical problems by Tao How to write a great research paper Seekers and Craftspeople by Lee Smolin Birds and Frogs by Freeman Dyson Some blog on codeforces","title":"References"},{"location":"src/reality-of-fiction/","text":"Literary fiction and society what is literature? is fiction needed to build meaning out of unbothered reality? This writing is in progress?? society is built on meaning? literary fiction as a medium for philosophical thinking? What is good literature? literature = story + style + philosophy + good style = good writing = good music = good composition (refer: Write Music by Provost) just the right amount of stickyness in the composition kisses, caresses, bites and punches you while either playing or fighting your awe, emotions and values rich enough to let you experience and feel another character and reality provides an exploratory platform for philosophical thinking, shaping or destroying or questioning ideologies entertains as well as helps you find 'new meanings' and question or ponder over old ones This sentence has five words. Here are five more words. Five word sentences are fine. But several together become monotonous. Listen to what is happening. The writing is getting boring. The sound of it drones. It's like a stuck record. The ear demands some variety. Now listen. I vary the sentence length, and I create music. Music. The writing sings. It has a pleasant rhythm, a lilt, a harmony. I use short sentences. And I use sentences of medium length. And sometimes when I am certain the reader is rested, I will engage him with a sentence of considerable length, a sentence that burns with energy and builds with all the impetus of a crescendo, the roll of the drums, the crash of the cymbals\u2014 sounds that say listen to this, it is important.\" So write with a combination of short, medium, and long sentences. Create a sound that pleases the reader's ear. Don't just write words. Write music. what separates good from great? \"great literature\" is transformative? outlasts? universal theme? influential on downstream work? innovative in some fashion? References What is literature? What is literature for? Write Music by Gary Provost Philosophy through fiction A Novelist\u2019s Tips for Writing Philosophical Fiction Plot as argument, argument as plot Philosophy and the Literary Medium: The Existentialist Predicament by Kleppner What Makes a Great Book? with Sir Jonathan Bate, Roosevelt Montas, Catherine Zuckert, & Michael Fink Why read the classics?","title":"Reality of Fiction"},{"location":"src/reality-of-fiction/#literary-fiction-and-society","text":"what is literature? is fiction needed to build meaning out of unbothered reality? This writing is in progress?? society is built on meaning? literary fiction as a medium for philosophical thinking?","title":"Literary fiction and society"},{"location":"src/reality-of-fiction/#what-is-good-literature","text":"literature = story + style + philosophy + good style = good writing = good music = good composition (refer: Write Music by Provost) just the right amount of stickyness in the composition kisses, caresses, bites and punches you while either playing or fighting your awe, emotions and values rich enough to let you experience and feel another character and reality provides an exploratory platform for philosophical thinking, shaping or destroying or questioning ideologies entertains as well as helps you find 'new meanings' and question or ponder over old ones This sentence has five words. Here are five more words. Five word sentences are fine. But several together become monotonous. Listen to what is happening. The writing is getting boring. The sound of it drones. It's like a stuck record. The ear demands some variety. Now listen. I vary the sentence length, and I create music. Music. The writing sings. It has a pleasant rhythm, a lilt, a harmony. I use short sentences. And I use sentences of medium length. And sometimes when I am certain the reader is rested, I will engage him with a sentence of considerable length, a sentence that burns with energy and builds with all the impetus of a crescendo, the roll of the drums, the crash of the cymbals\u2014 sounds that say listen to this, it is important.\" So write with a combination of short, medium, and long sentences. Create a sound that pleases the reader's ear. Don't just write words. Write music. what separates good from great? \"great literature\" is transformative? outlasts? universal theme? influential on downstream work? innovative in some fashion?","title":"What is good literature?"},{"location":"src/reality-of-fiction/#references","text":"What is literature? What is literature for? Write Music by Gary Provost Philosophy through fiction A Novelist\u2019s Tips for Writing Philosophical Fiction Plot as argument, argument as plot Philosophy and the Literary Medium: The Existentialist Predicament by Kleppner What Makes a Great Book? with Sir Jonathan Bate, Roosevelt Montas, Catherine Zuckert, & Michael Fink Why read the classics?","title":"References"},{"location":"src/sanskrit/","text":"Mahamrityunjay mantra (origin: Shivapuran) \u0950 \u0924\u094d\u0930\u094d\u092f\u092e\u094d\u092c\u0915\u0902 \u092f\u091c\u093e\u092e\u0939\u0947 \u0938\u0941\u0917\u0928\u094d\u0927\u093f\u0902 \u092a\u0941\u0937\u094d\u091f\u093f\u0935\u0930\u094d\u0927\u0928\u092e\u094d\u0964 \u0909\u0930\u094d\u0935\u093e\u0930\u0941\u0915\u092e\u093f\u0935 \u092c\u0928\u094d\u0927\u0928\u093e\u0928\u094d \u092e\u0943\u0924\u094d\u092f\u094b\u0930\u094d\u092e\u0941\u0915\u094d\u0937\u0940\u092f \u092e\u093e\u093d\u092e\u0943\u0924\u093e\u0924\u094d\u0965 On shiva (origin: Yajurveda) \u0915\u0930\u094d\u092a\u0942\u0930\u0917\u094c\u0930\u0902 \u0915\u0930\u0941\u0923\u093e\u0935\u0924\u093e\u0930\u0902 \u0938\u0902\u0938\u093e\u0930\u0938\u093e\u0930\u0902 \u092d\u0941\u091c\u0917\u0947\u0928\u094d\u0926\u094d\u0930\u0939\u093e\u0930\u092e\u094d\u0964 \u0938\u0926\u093e \u0935\u0938\u0928\u094d\u0924\u0902 \u0939\u0943\u0926\u092f\u093e\u0930\u0935\u093f\u0928\u094d\u0926\u0947 \u092d\u0935\u0902 \u092d\u0935\u093e\u0928\u0940\u0938\u0939\u093f\u0924\u0902 \u0928\u092e\u093e\u092e\u093f\u0965 On work as an end in itself (origin: Bhagavad Gita) \u0915\u0930\u094d\u092e\u0923\u094d\u092f\u0947\u0935\u093e\u0927\u093f\u0915\u093e\u0930\u0938\u094d\u0924\u0947 \u092e\u093e \u092b\u0932\u0947\u0937\u0941 \u0915\u0926\u093e\u091a\u0928\u0964 \u092e\u093e \u0915\u0930\u094d\u092e\u092b\u0932\u0939\u0947\u0924\u0941\u0930\u094d\u092d\u0942\u0930\u094d\u092e\u093e \u0924\u0947 \u0938\u0919\u094d\u0917\u094b\u093d\u0938\u094d\u0924\u094d\u0935\u0915\u0930\u094d\u092e\u0923\u093f\u0965 Practicing Sanskrit Something of my own writing directly influenced from the kedarnath song. The sanskritisation is completely mine along with several semantic changes. \u0930\u0941\u0926\u094d\u0930 \u0936\u093f\u0935 \u0938\u094b\u092e\u0947\u0936\u094d\u0935\u0930\u092e\u094d; \u092d\u0926\u094d\u0930 \u0928\u091f \u092f\u0941\u0917\u0928\u094d\u0927\u0930\u092e\u094d; \u0924\u094d\u0930\u094d\u092f\u092e\u094d\u092c\u0915\u0947\u0913 \u092d\u094b\u0932\u0947\u0928\u093e\u0925; \u092e\u0939\u093e\u0926\u0947\u0935\u094b \u0928\u092e\u094b-\u0928\u092e\u092e\u094d\u0964 \u0936\u0942\u0928\u094d\u092f\u093e\u0927\u093f\u0915 \u0938\u0942\u0915\u094d\u0937\u094d\u092e-\u0924\u094d\u092f \u0906\u0915\u093e\u0936\u094b\u092a\u092e\u093e\u0928 \u0935\u093f\u0938\u094d\u0924\u093e\u0930; \u0915\u093f \u0938\u0943\u0937\u094d\u091f\u093f-\u092e\u0930\u094d\u092e-\u0938\u094d\u0925\u093f\u0924\u093f-\u091a, \u0924\u0941\u092e\u093f\u0935 \u0906\u0926\u093f-\u0905\u0928\u094d\u0924 \u091a; \u0938\u0902\u0938\u093e\u0930\u0947 \u092c\u0939\u0941\u0935-\u0905\u0927\u0930\u094d\u092e\u0903, \u092a\u0930 \u0924\u0924\u094d\u0924\u094d\u0935 \u090f\u0915\u092e\u0947\u0935\u092e\u093e; \u0905\u0938\u0924\u094d-\u0915\u093e\u092e-\u0932\u094b\u092d \u091c\u0917\u0924\u094d\u092f\u0916\u0902\u0921\u0938\u0924\u094d\u092f \u0936\u0902\u0915\u0930\u0964 \u0930\u0941\u0926\u094d\u0930 \u0936\u093f\u0935 \u0938\u094b\u092e\u0947\u0936\u094d\u0935\u0930\u092e\u094d; \u092d\u0926\u094d\u0930 \u0928\u091f \u092f\u0941\u0917\u0928\u094d\u0927\u0930\u092e\u094d; \u0924\u094d\u0930\u094d\u092f\u092e\u094d\u092c\u0915\u090f \u0928\u0940\u0932\u0915\u0902\u0920; \u092e\u0939\u093e\u0926\u0947\u0935\u094b \u0928\u092e\u094b-\u0928\u092e\u092e\u094d\u0964","title":"Random Sanskrit"},{"location":"src/sanskrit/#mahamrityunjay-mantra-origin-shivapuran","text":"\u0950 \u0924\u094d\u0930\u094d\u092f\u092e\u094d\u092c\u0915\u0902 \u092f\u091c\u093e\u092e\u0939\u0947 \u0938\u0941\u0917\u0928\u094d\u0927\u093f\u0902 \u092a\u0941\u0937\u094d\u091f\u093f\u0935\u0930\u094d\u0927\u0928\u092e\u094d\u0964 \u0909\u0930\u094d\u0935\u093e\u0930\u0941\u0915\u092e\u093f\u0935 \u092c\u0928\u094d\u0927\u0928\u093e\u0928\u094d \u092e\u0943\u0924\u094d\u092f\u094b\u0930\u094d\u092e\u0941\u0915\u094d\u0937\u0940\u092f \u092e\u093e\u093d\u092e\u0943\u0924\u093e\u0924\u094d\u0965","title":"Mahamrityunjay mantra (origin: Shivapuran)"},{"location":"src/sanskrit/#on-shiva-origin-yajurveda","text":"\u0915\u0930\u094d\u092a\u0942\u0930\u0917\u094c\u0930\u0902 \u0915\u0930\u0941\u0923\u093e\u0935\u0924\u093e\u0930\u0902 \u0938\u0902\u0938\u093e\u0930\u0938\u093e\u0930\u0902 \u092d\u0941\u091c\u0917\u0947\u0928\u094d\u0926\u094d\u0930\u0939\u093e\u0930\u092e\u094d\u0964 \u0938\u0926\u093e \u0935\u0938\u0928\u094d\u0924\u0902 \u0939\u0943\u0926\u092f\u093e\u0930\u0935\u093f\u0928\u094d\u0926\u0947 \u092d\u0935\u0902 \u092d\u0935\u093e\u0928\u0940\u0938\u0939\u093f\u0924\u0902 \u0928\u092e\u093e\u092e\u093f\u0965","title":"On shiva (origin: Yajurveda)"},{"location":"src/sanskrit/#on-work-as-an-end-in-itself-origin-bhagavad-gita","text":"\u0915\u0930\u094d\u092e\u0923\u094d\u092f\u0947\u0935\u093e\u0927\u093f\u0915\u093e\u0930\u0938\u094d\u0924\u0947 \u092e\u093e \u092b\u0932\u0947\u0937\u0941 \u0915\u0926\u093e\u091a\u0928\u0964 \u092e\u093e \u0915\u0930\u094d\u092e\u092b\u0932\u0939\u0947\u0924\u0941\u0930\u094d\u092d\u0942\u0930\u094d\u092e\u093e \u0924\u0947 \u0938\u0919\u094d\u0917\u094b\u093d\u0938\u094d\u0924\u094d\u0935\u0915\u0930\u094d\u092e\u0923\u093f\u0965","title":"On work as an end in itself (origin: Bhagavad Gita)"},{"location":"src/sanskrit/#practicing-sanskrit","text":"Something of my own writing directly influenced from the kedarnath song. The sanskritisation is completely mine along with several semantic changes. \u0930\u0941\u0926\u094d\u0930 \u0936\u093f\u0935 \u0938\u094b\u092e\u0947\u0936\u094d\u0935\u0930\u092e\u094d; \u092d\u0926\u094d\u0930 \u0928\u091f \u092f\u0941\u0917\u0928\u094d\u0927\u0930\u092e\u094d; \u0924\u094d\u0930\u094d\u092f\u092e\u094d\u092c\u0915\u0947\u0913 \u092d\u094b\u0932\u0947\u0928\u093e\u0925; \u092e\u0939\u093e\u0926\u0947\u0935\u094b \u0928\u092e\u094b-\u0928\u092e\u092e\u094d\u0964 \u0936\u0942\u0928\u094d\u092f\u093e\u0927\u093f\u0915 \u0938\u0942\u0915\u094d\u0937\u094d\u092e-\u0924\u094d\u092f \u0906\u0915\u093e\u0936\u094b\u092a\u092e\u093e\u0928 \u0935\u093f\u0938\u094d\u0924\u093e\u0930; \u0915\u093f \u0938\u0943\u0937\u094d\u091f\u093f-\u092e\u0930\u094d\u092e-\u0938\u094d\u0925\u093f\u0924\u093f-\u091a, \u0924\u0941\u092e\u093f\u0935 \u0906\u0926\u093f-\u0905\u0928\u094d\u0924 \u091a; \u0938\u0902\u0938\u093e\u0930\u0947 \u092c\u0939\u0941\u0935-\u0905\u0927\u0930\u094d\u092e\u0903, \u092a\u0930 \u0924\u0924\u094d\u0924\u094d\u0935 \u090f\u0915\u092e\u0947\u0935\u092e\u093e; \u0905\u0938\u0924\u094d-\u0915\u093e\u092e-\u0932\u094b\u092d \u091c\u0917\u0924\u094d\u092f\u0916\u0902\u0921\u0938\u0924\u094d\u092f \u0936\u0902\u0915\u0930\u0964 \u0930\u0941\u0926\u094d\u0930 \u0936\u093f\u0935 \u0938\u094b\u092e\u0947\u0936\u094d\u0935\u0930\u092e\u094d; \u092d\u0926\u094d\u0930 \u0928\u091f \u092f\u0941\u0917\u0928\u094d\u0927\u0930\u092e\u094d; \u0924\u094d\u0930\u094d\u092f\u092e\u094d\u092c\u0915\u090f \u0928\u0940\u0932\u0915\u0902\u0920; \u092e\u0939\u093e\u0926\u0947\u0935\u094b \u0928\u092e\u094b-\u0928\u092e\u092e\u094d\u0964","title":"Practicing Sanskrit"},{"location":"src/search-for-meaning/","text":"Search for Meaning It is just that there is no overarching epistemic meaning to life, universe and existence. No one is epistemically richer, none a life more purposeful. I find that to have a good life, however, instrumental meaning is necessary. And such is instantiated with fuzzyness, reasonability and the intent of pragmatic action. Reasonability allows for more freedom than rationality as it is commonly understood. It allows for absurdity, beauty and constrained rationality. But with freedom comes perils \u2013 the perils of being lost or not seeking, the perils of inaction or oblivion. It is also worth noting the importance of pragmatic action which I believe is interdependent on environment, emperical reality and shared non-deconstructive fiction. This is also a right time to juxtapose the concept of Dostoevsky\u2019s Idiot and the Idiot\u2019s awareness of its own limitations. There is much we do not know. So why try to conform and fit everything into a block without respecting abstractions? Why hold on to absolutisms? Aren\u2019t the notions of self and divinity always plural or plurally singular? Why not find beauty through the destruction of the boring in unknowability? Why hate paradoxes? Why not be arrogant and humble? A lot of daemons arise from self-referencing (paradox?) of the self and subversion of the holistic experience we call life. More daemons arise from debauchery of strength, discipline and happiness. It is beautiful though that we are aware of such problems and that in the sea of paradoxes and uncomputable abstractive jumps there does lie great pools of comprehensive consistency. References: Fuzzy logic, Intuitionistic logic Godel incompleteness, Computation Rationality and its disputed bases of definition Category theory Samkhya, Uttara mimamsa, Buddhism Metaphysics, Instrumentalism, Pragmatism Logical reasoning and rational irrationality Writing allows for inifnite working tape. I also take liberty in broadening the notion of what it means to be fuzzy from propositional logic and embracing incompleteness of the same. Maybe faith helps. Absurd? Maybe. Maybe not.","title":"Search for Meaning"},{"location":"src/search-for-meaning/#search-for-meaning","text":"It is just that there is no overarching epistemic meaning to life, universe and existence. No one is epistemically richer, none a life more purposeful. I find that to have a good life, however, instrumental meaning is necessary. And such is instantiated with fuzzyness, reasonability and the intent of pragmatic action. Reasonability allows for more freedom than rationality as it is commonly understood. It allows for absurdity, beauty and constrained rationality. But with freedom comes perils \u2013 the perils of being lost or not seeking, the perils of inaction or oblivion. It is also worth noting the importance of pragmatic action which I believe is interdependent on environment, emperical reality and shared non-deconstructive fiction. This is also a right time to juxtapose the concept of Dostoevsky\u2019s Idiot and the Idiot\u2019s awareness of its own limitations. There is much we do not know. So why try to conform and fit everything into a block without respecting abstractions? Why hold on to absolutisms? Aren\u2019t the notions of self and divinity always plural or plurally singular? Why not find beauty through the destruction of the boring in unknowability? Why hate paradoxes? Why not be arrogant and humble? A lot of daemons arise from self-referencing (paradox?) of the self and subversion of the holistic experience we call life. More daemons arise from debauchery of strength, discipline and happiness. It is beautiful though that we are aware of such problems and that in the sea of paradoxes and uncomputable abstractive jumps there does lie great pools of comprehensive consistency. References: Fuzzy logic, Intuitionistic logic Godel incompleteness, Computation Rationality and its disputed bases of definition Category theory Samkhya, Uttara mimamsa, Buddhism Metaphysics, Instrumentalism, Pragmatism Logical reasoning and rational irrationality Writing allows for inifnite working tape. I also take liberty in broadening the notion of what it means to be fuzzy from propositional logic and embracing incompleteness of the same. Maybe faith helps. Absurd? Maybe. Maybe not.","title":"Search for Meaning"}]}